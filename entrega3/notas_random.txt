

Proceso mental y motivaciones.
como la web semántica permite representar información desde múltiples fuentes, se me ocurrió hacer eso mismo con los tps.

tp1 exporatría en una ontología básica creada en protegé para experimentar la creación de ontologías simples cmo explicó casco en los videos.
en este caso voy a leer el archivo json merge que ya está estructurado.
esto me permite tener una idea de como es exportar datos estructurados de una base en formatos de la web semántica.
los tomo como si fueran "mis" datos generados en un sistema. (salas cines funciones, etc)

tp2 modificar los scrapers para manejar tripletas directamente, (o generar alguno simplificado ya que no tendría que manejar json-ld si no mas bien tripletas del grafo)
analizar si es que me conviene usar la ontología creada anteriormente o extender alguna, usar alguna combinación de las mismas.
la idea es experimentar un caso real sobre la web semántica donde se obtienen datos de múltiples ontologías.

en las etapas anteriores tendría en owl los datos para ser consumidos por la última etapa del sismtea.
armar algun merger para ver como me conviene tratar los datos. usar distintos grafos nombrados para almacenar los datos de distintas fuentes?.
o hacer merge directo (unión) de los grafos completos.


---
definición de la ontología.

como representar la duración de las películas en owl, ya que los tipos permitidos xsd:dateTime xsd:dateTimestamp
si o si llevan la fecha de acuerdo a la definición.
un posible datatype podría haber sido xsd:decimal y expresarla en minutos. (es la que tomé en la definición de twss_simple.ttl)
pero quería la posibilidad de expresarla en hh:mm:ss
veo que existe una ontología que es una recomendación para representar el tiempo.
https://www.w3.org/TR/owl-time/
la definición esta en : http://www.w3.org/2006/time#

----
Ya tengo los datos de tp1 en ontología y sus individuals.
ahora sigo con tp2 y la modificación del scraper.

idea, como comentó casco en el foro, esta bueno pensar en tripletas en vez de en clases.
asi que voy a hacer la prueba de cargar los jsons-ld que responden los sitios a tripletas y manejarlos internamente en el scraper como tripletas.
para todas las operaciones posteriores realizarlas en el grafo. (los merges, etc)
para esto voy a modificar el scraper que había realizado en el tp2. en vez de procesar los datos almacenados desde un archivo.

segunda idea, para evitar que haya problemas con internet y poder procesar mejor los datos, voy a encarar la solución de la siguiente forma:
1 - armar algun script que me descarge la info json-ld de los sitios en cuestión y las deje en archivos para sus posteriores análisis.
2 - armar un parser que procese la info en json-ld y la escriba en owl. (no debería ser complejo)

el punto 1 super simple. (utils/download_jsons.py)
el punto 2.
genero un parser para ver si puedo parsear la info json-ld a tripletas.
uso rdflib para testear.


primer problema encontrado:


  File "/home/pablo/.local/lib/python3.9/site-packages/rdflib_jsonld/context.py", line 65, in subcontext
    ctx.load(source)
  File "/home/pablo/.local/lib/python3.9/site-packages/rdflib_jsonld/context.py", line 200, in load
    self._prep_sources(base, source, sources)
  File "/home/pablo/.local/lib/python3.9/site-packages/rdflib_jsonld/context.py", line 213, in _prep_sources
    source = source_to_json(source_url)
  File "/home/pablo/.local/lib/python3.9/site-packages/rdflib_jsonld/util.py", line 28, in source_to_json
    return json.load(StringIO(stream.read().decode('utf-8')))
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte


cuando leo detenidamente el fuente de rdflib_jsonld está tratando de cargar contextos, no es relacionado a los datos de entrada del json que obtengo.
por lo que veo es un bug de schema.org

https://github.com/RDFLib/rdflib-jsonld/issues/84

efectivamente trata de descargar el context de schema.org ya que el json-ld de origen lo lista como contexto.
viendo el tráfico de red generado cuando ejecuto el script. veo que se comunica con schema.org

    192.168.10.5.45910 > 172.217.172.78.80: Flags [P.], cksum 0x913d (correct), seq 1:213, ack 1, win 502, options [nop,nop,TS val 1128910949 ecr 556658059], length 212: HTTP, length: 212
	GET / HTTP/1.1
	Accept-Encoding: identity
	Host: schema.org
	User-Agent: rdflib-5.0.0 (http://rdflib.net/; eikeon@eikeon.com)
	Accept: application/ld+json, application/json;q=0.9, */*;q=0.1
	Connection: close
	
19:05:23.272368 IP (tos 0x0, ttl 58, id 34438, offset 0, flags [none], proto TCP (6), length 52)
    172.217.172.78.80 > 192.168.10.5.45910: Flags [.], cksum 0x7e6a (correct), ack 213, win 261, options [nop,nop,TS val 556658075 ecr 1128910949], length 0
19:05:23.272417 IP (tos 0x0, ttl 58, id 34439, offset 0, flags [none], proto TCP (6), length 52)
    172.217.172.78.80 > 192.168.10.5.45910: Flags [.], cksum 0x7e6a (correct), ack 213, win 261, options [nop,nop,TS val 556658075 ecr 1128910949], length 0
19:05:23.445763 IP (tos 0x0, ttl 58, id 34553, offset 0, flags [none], proto TCP (6), length 303)
    172.217.172.78.80 > 192.168.10.5.45910: Flags [P.], cksum 0xcb52 (correct), seq 1:252, ack 213, win 261, options [nop,nop,TS val 556658250 ecr 1128910949], length 251: HTTP, length: 251
	HTTP/1.1 301 Moved Permanently
	Location: https://schema.org/
	X-Cloud-Trace-Context: 5dd336392bd7d9264f23ac7362a8d2e3;o=1
	Date: Sat, 08 May 2021 22:05:23 GMT
	Content-Type: text/html
	Server: Google Frontend
	Content-Length: 0
	Connection: close
	


curl -v -H "accept: application/ld+json" "http://schema.org"
https://github.com/schemaorg/schemaorg/issues/2578


el problema es generado por la cabecera link de shcema.org
es parte de la especificación de json-ld 1.1 para manejar la negociación de contenido y de donde descargar la info, pero parece que las librerías todavia no lo han actualizado.
asi que modifico HORRIBLEMENTE la librería como dicen en los issues de github para que carge del archivo local el contexto de schema.org


rdflib_jsonld/context.py

    def _prep_sources(self, base, inputs, sources, referenced_contexts=None,
            in_source_url=None):
        referenced_contexts = referenced_contexts or set()
        for source in inputs:
            if isinstance(source, str):
                source_url = urljoin(base, source)

                ''' TODO: horrible hack: sacado: https://github.com/RDFLib/rdflib-jsonld/issues/84 '''
                if "/schema.org" in source_url:
                    source_url = "src/jsonldcontext.jsonld"

                if source_url in referenced_contexts:
                    raise errors.RECURSIVE_CONTEXT_INCLUSION
                referenced_contexts.add(source_url)
                source = source_to_json(source_url)
                if CONTEXT not in source:
                    raise errors.INVALID_REMOTE_CONTEXT
            else:
                source_url = in_source_url

            if isinstance(source, dict):
                if CONTEXT in source:
                    source = source[CONTEXT]
                    source = source if isinstance(source, list) else [source]
            if isinstance(source, list):
                self._prep_sources(base, source, sources, referenced_contexts, source_url)
            else:
                sources.append((source_url, source))


-----

otra alternativa de procesarlo podría haber sido como comentó Leonardo en el post del foro.
cambiar el context de los json-ld para la url donde schema.org sirve los archivos 
https://schema.org/docs/jsonldcontext.jsonld

era mucho mas facil que andar debuggeando la libreria!!.

-----

ejemplo de ejecución del script para ver si convierte ok en grafo a los archivos de json.

python3 src/utils/parse_jsons.py data/tp2/https___www.ecartelera.com_peliculas_wonder-woman-1984.json
python3 src/utils/parse_jsons.py data/tp2/https___www.imdb.com_title_tt7126948_.json 
python3 src/utils/parse_jsons.py data/tp2/https___www.metacritic.com_movie_wonder-woman-1984.json 
python3 src/utils/parse_jsons.py data/tp2/https___www.rottentomatoes.com_m_wonder_woman_1984.json 

asi que funciona bien en pasar los json-ld a turttle e interpretar el grafo.
lo único es que algunas iris al ser relativas y el parser del grafo no sabe cual es la url real las transofrma a iris del sistema de archivos.
ej:


[] a schema:Movie ;
    schema:actor [ a schema:Person ;
            schema:name "Pedro Pascal" ;
            schema:url <file:///name/nm0050959/> ],
        [ a schema:Person ;
            schema:name "Kristen Wiig" ;
            schema:url <file:///name/nm1325419/> ],


también noto que varios sitios representan la info con blank nodes y colecciones.

[] a schema:Movie ;
    schema:actor [ a schema:Person ;
            schema:name "Jamaal Burcher" ;
            schema:url <https://www.metacritic.com/person/jamaal-burcher> ],
        [ a schema:Person ;
            schema:name "Chris Silcox" ;
            schema:url <https://www.metacritic.com/person/chris-silcox> ],

en json-ld se ve bien el formato despues de serializar el grafo de tripletas.

[
  {
    "@id": "https://www.ecartelera.com/peliculas/wonder-woman-1984/",
    "@type": [
      "http://schema.org/Movie"
    ],
    "http://schema.org/actor": [
      {
        "@id": "_:N6c6979e8b5d44455853b74273b109464"
      },
      {
        "@id": "_:N88b28347cddf43578bcff381133b40cc"
      },

en ese caso las personas son blank nodes y usa referencias para ubicarlas dentro del json-ld

por ahora solo notarlo, para tenerlo en cuenta en la solución final.
ahora si entendiendo un poco el proceso armo un script que se conecte al sitio, descargue el json-ld y lo transforme a turtle.


----

ejecución inicial de una prueba del script.

python3 src/tp2_2_owl.py https://www.ecartelera.com/peliculas/wonder-woman-1984

hace merge de la info de tp1 y lo que lee del sitio en json-ld.
y lo escribe en un archivo merged.ttl

portegé no muestra cosas coherentes. muestra las clases pero todo lo demas como anotaciones.
no muestra correctamente las propiedades, etc.
tiene que estar faltando las clases que usa protegé para analizar y interpretar los datos en su interface.

----

para los individuals protegé le asigna la clase owl:NamedIndividual
y para las clases owl:Class

###  http://www.semanticweb.org/pablo/ontologies/2021/4/untitled-ontology-18#Pepe
:Pepe rdf:type owl:Class .

###  http://www.semanticweb.org/pablo/ontologies/2021/4/untitled-ontology-18#pepeInstance
:pepeInstance rdf:type owl:NamedIndividual ,
                       :Pepe .


ya hice el cambio a los scripts para que le agreguen eso a los datos del tp1.

---

ahora el tema del procesamiento de los datos que vienen de los sitios con el context schema.org
lograr que protegé los lea correctamente del archivo escribo en turtle por el script
primero le voy a agregar esas clases. (NamedIndividual y Class)

sigue sin funcionar. no me muestra la instancia de schema:Movie con id: 
https://www.ecartelera.com/peliculas/wonder-woman-1984/

identifique cual es el problema.
protegé no lo toma como individual si tiene un / final en el iri.
claramente no es acorde con la sintaxis de las iris.

sintaxis de las iris: https://datatracker.ietf.org/doc/html/rfc3987
   
con un ejemplo como :

@prefix owl: <http://www.w3.org/2002/07/owl#> .

<http://schema.org/Movie> a owl:Class .

<https://www.ecartelera.com/peliculas/wonder-woman-1984> a <http://schema.org/Movie> .

me lo tomó ok el protegé

-----

bien entonces ahora tengo 2 opciones.
1 - extender mi ontología para abarcar las clases de schema.
2 - implementar una ontología especifica de schema y asociarla con la mía mediante owl, por ejemplo usando owl:sameAs

voy a probar con la opción 2 para ver como es la ontología de schema.org a ver si le puedo hacer un merge con la mía.
y replicar solo las propieades que usan los sitios actuales que estamos analizando en los tps


problema 1 definiendo schema.org
mainEntityOfPage --> dominio: Thing  range: CreativeWork, URL

veo los datos:

        [ a schema:Person ;
            schema:mainEntityOfPage <https://www.ecartelera.com/personas/robin-wright/> ;

la cual no es una iri válida, o sea que la debería tomar como una url, un texto. pero como está definida ahi sería una iri de un recurso.
probé y veo que si le saco la / final el sitio si redirige a la url con /, asi que sería correcto sacarle las /

con respecto a:
schema:countryOfOrigin "EE.UU." ;
no es una iri, si no un texto. asi que por ahora lo voy a modelar como un string.

duration:
schema:duration "PT2H31M" ;
tambien la modelo como string y como numero para abarcar mi modelo anterior.

url esta definido como hacia una iri    
schema:image [ a schema:ImageObject ;
    schema:url <https://img.ecartelera.com/carteles/13100/13170/009_p.jpg> ] ;

pero me parece mejor definirlo como string.

----

schema.org modela todo como clases. Text, etc.
pero en la práctica los datos los sitios los exportan como string.


--------

Problemón!!!! todos los jsons-ld consumidos de los sitios me generan blank nodes para los datos de las películas. usan colecciones para manejar a los actores, etc.
y no me los interpreta bien protegé.
así que tengo que convertilos a nodes normales con iris.

defini un pefijo para mi namespace de datos.
tsswd: https://raw.githubusercontent.com/pablodanielrey/twss/master/owl/data/

y arme una rutina que busca los blank nodes y los reemplaza por iris de datos.
o sea generaría una instancia nombrada para que protegé lo reconozca ok.

abri el archivo en protegé y al fin veo las instancias dentro de individuals by Class

---

ahora si me lo interpreta pero todas las propiedades definidas en la ontología me las muestra como anotiaciones y no como propiedades en los individuals.
si bien están definidas en la ontología, y el archivo de individuals esta con datos correctos.
ontologia:
###  http://schema.org/name
<http://schema.org/name> rdf:type owl:DatatypeProperty ;
                         rdfs:domain owl:Thing ;
                         rdfs:range xsd:string .


datos ej:
@prefix schema: <http://schema.org/> .
@prefix twssd: <https://raw.githubusercontent.com/pablodanielrey/twss/master/owl/data/> .

twssd:1f950821-b321-4914-9263-a4a93af51986 a schema:Person ;
    schema:mainEntityOfPage <https://www.ecartelera.com/personas/robin-wright/> ;
    schema:name "Robin Wright" .

contra una que si muestra:
:pepe rdf:type owl:NamedIndividual ,
               <http://schema.org/Movie> ;
      <http://schema.org/name> "pepe" .


voy a probar agregando NamedIndividual a ver si lo entiende protegé.

NO funciona!!!. no es solo eso.

lo que estoy haciendo es abrir el archivo con los individuals en protegé y posteriormente importo mi ontología como indicó casco en el video para el ejemplo de schema.org
pero no la interpreta bien de esta forma. asi que voy por la mas simple.
voy a unificar los archivos de ontología y el de individuals en uno solo, que posteriormente abro con protegé.
modifico el script para realizar esto.

al manejarlo como tripletas dentro del mismo scraper esto es tan simple como una suma de grajos.
gfinal = gontology + g 

FUNCIONO!!!!. protegé me muestra la ontología y los individuals con las propiedaes correctamente, no en la parte de anotaciones!!!!!
perdí casi 1 día con esto. resumen, no la compliques anticipando funcionalidad cuando todavía no hiciste andar el prototipo.

------

Perfecto, hasta este punto tengo funcionando:
1 - la definición de una ontología con las clases de schema.org (solo las que uso)
2 - la carga del json-ld desde la página ecartelera y la transformación de ese json-ld al grafo de tripletas.
3 - el procesamiento de las tripletas para generar correctamente el grafo de individuals. (eliminación de blank nodes, etc)
4 - el merge entre la ontología que define las clases y los individuals procesados anteriormente.

ahora queda cargar los otros jsons-ld e ir mergeando la info. definiendo las clases que me faltan y analizando en protegé si funciona.

-----

toca el turno a imdb. modifico el script para 
1 - carga del json-ld
2 - lo parsea en el grafo de tripletas
3 - hace unión con el grafo anterior

y despues los puntos normales de unir con la ontología y escribir el turtle final.

FUNCIONAAA!!!.
lo único que veo es que como anotaciones me dejo las propiedaes que todavía no defini de schema.org en mi ontología.

paso siguiente definir esas propiedades faltantes.

schema:creator 
schema:contentRating
schema:datePublished
schema:keywords
schema:review
schema:trailer
schema:url
schema:productionCompany
schema:potentialAction
schema:releasedEvent
schema:actionPlatform
schema:inLanguage
schema:urlTemplate
schema:location
schema:availabilityStarts
schame:eligibleRegion
schema:category
schema:author
schema:dateCreated
schema:itemReviewed
schema:reviewBody
schema:reviewRating
schema:embedUrl
schema:thumbnail
schema:thumbnailUrl
schema:uploadDate


esta schema:url es raro porque la definí, pero la tengo definida como dataProperty
y en el archivo turtle esta:

schema:url <https://www.imdb.com/title/tt7126948/> .

o sea una objectProperty, la cambio.

funcionó. ahora protegé lo muestra como una propiedad dentor del individual.
sigo con el problema de que las iris con / final no las interpreta bien protegé. no muestra nada.


-----

defino todas las propiedades que me faltan para terminar con la ontología de schema.org

varias propiedades en schema.org se definen como objectProperty pero en los datos en la práctica aparecen como dataProperties.
asociadas a strings. ejemplos:
schema:contentRating
schema:datePublished

un problema grande es schema:url 
que en los datos aparece asociado a iris y asociado a strings.!!!!! TEMA A CONSULTAR!!!.
asumo que lo convertiría en irirs en los scrapers, sería la opción mas correcta.


termine de definir todas las propiedades y clases faltantes, corri el script nuevamente y abrí el archivo en protegé y me interpretó correctamente las propiedades.
no mas datos en anotaciones.

----

agrego metacritic y veo como resulta.
obvio, problemas con la librería de parseo nuevamnete.

pablo@xiaomi:/src/github/facu-infor/twss/entrega3$ python3 src/tp2_2_owl.py 
Descargando información de : https://www.ecartelera.com/peliculas/wonder-woman-1984
Traceback (most recent call last):
  File "/src/github/facu-infor/twss/entrega3/src/tp2_2_owl.py", line 100, in <module>
    g.parse(data=djson_ld, format='json-ld', publicID=url)
  File "/home/pablo/.local/lib/python3.9/site-packages/rdflib/graph.py", line 1078, in parse
    parser.parse(source, self, **args)
  File "/home/pablo/.local/lib/python3.9/site-packages/rdflib_jsonld/parser.py", line 95, in parse
    to_rdf(data, conj_sink, base, context_data)
  File "/home/pablo/.local/lib/python3.9/site-packages/rdflib_jsonld/parser.py", line 107, in to_rdf
    return parser.parse(data, context, dataset)
  File "/home/pablo/.local/lib/python3.9/site-packages/rdflib_jsonld/parser.py", line 125, in parse
    context.load(l_ctx, context.base)
  File "/home/pablo/.local/lib/python3.9/site-packages/rdflib_jsonld/context.py", line 200, in load
    self._prep_sources(base, source, sources)
  File "/home/pablo/.local/lib/python3.9/site-packages/rdflib_jsonld/context.py", line 220, in _prep_sources
    source = source_to_json(source_url)
  File "/home/pablo/.local/lib/python3.9/site-packages/rdflib_jsonld/util.py", line 23, in source_to_json
    source = create_input_source(source, format='json-ld')
  File "/home/pablo/.local/lib/python3.9/site-packages/rdflib/parser.py", line 193, in create_input_source
    input_source = URLInputSource(absolute_location, format)
  File "/home/pablo/.local/lib/python3.9/site-packages/rdflib/parser.py", line 113, in __init__
    file = urlopen(req)
  File "/usr/lib/python3.9/urllib/request.py", line 214, in urlopen
    return opener.open(url, data, timeout)
  File "/usr/lib/python3.9/urllib/request.py", line 523, in open
    response = meth(req, response)
  File "/usr/lib/python3.9/urllib/request.py", line 632, in http_response
    response = self.parent.error(
  File "/usr/lib/python3.9/urllib/request.py", line 561, in error
    return self._call_chain(*args)
  File "/usr/lib/python3.9/urllib/request.py", line 494, in _call_chain
    result = func(*args)
  File "/usr/lib/python3.9/urllib/request.py", line 641, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
pablo@xiaomi:/src/github/facu-infor/twss/entrega3$ 


parece que no encuentra algo del contexto también.
cambie para que el json lo busque por http y que tambien cambie el prefijo de https.

        json_ld['@context'] = json_ld['@context'].replace('http://schema.org','http://schema.org/docs/jsonldcontext.jsonld')
        json_ld['@context'] = json_ld['@context'].replace('https://schema.org','http://schema.org/docs/jsonldcontext.jsonld')

y salio funcionando. parece algo temporal.

19:31:08.971899 IP (tos 0x0, ttl 64, id 19553, offset 0, flags [DF], proto TCP (6), length 60)
    192.168.10.5.57782 > 172.217.172.110.80: Flags [S], cksum 0xe8e3 (correct), seq 21734641, win 64240, options [mss 1460,sackOK,TS val 2312580635 ecr 0,nop,wscale 7], length 0
19:31:08.978163 IP (tos 0x0, ttl 64, id 19554, offset 0, flags [DF], proto TCP (6), length 52)
    192.168.10.5.57782 > 172.217.172.110.80: Flags [.], cksum 0x5e81 (correct), ack 356733432, win 502, options [nop,nop,TS val 2312580641 ecr 1592126450], length 0
19:31:08.978319 IP (tos 0x0, ttl 64, id 19555, offset 0, flags [DF], proto TCP (6), length 289)
    192.168.10.5.57782 > 172.217.172.110.80: Flags [P.], cksum 0x99f6 (correct), seq 0:237, ack 1, win 502, options [nop,nop,TS val 2312580641 ecr 1592126450], length 237: HTTP, length: 237
	GET /docs/jsonldcontext.jsonld HTTP/1.1
	Accept-Encoding: identity
	Host: schema.org
	User-Agent: rdflib-5.0.0 (http://rdflib.net/; eikeon@eikeon.com)
	Accept: application/ld+json, application/json;q=0.9, */*;q=0.1
	Connection: close
	
19:31:09.159336 IP (tos 0x0, ttl 64, id 19556, offset 0, flags [DF], proto TCP (6), length 52)
    192.168.10.5.57782 > 172.217.172.110.80: Flags [.], cksum 0x5b19 (correct), ack 273, win 501, options [nop,nop,TS val 2312580822 ecr 1592126633], length 0
19:31:09.160324 IP (tos 0x0, ttl 64, id 19557, offset 0, flags [DF], proto TCP (6), length 52)
    192.168.10.5.57782 > 172.217.172.110.80: Flags [F.], cksum 0x5b16 (correct), seq 237, ack 274, win 501, op

-------------

parece que todo funciona ok, verifique las propieades y protegé me las tiró en propiedades y no anotaciones.
asi que agrego el ultimo sitio que es rottentomatoes

aparecieron algunas en anotaciones. 

schema:actors
schema:publisher
schema:dateModified
schema:embedUrl
schema:contentUrl
schema:thumbnailUrl
schema:sameAs

-------------

termine de definir las nuevas propieades y corrí de nuevo el script.
perfecto, reconoció todo.
ahora solo me falta analizar como hago para deduplicar los datos.
en principo el grafo resultante tiene muchos ids autogenerados pero que repiten su contenido.
o sea hay varios individuals personas que son las mismas personas.
lo mismo con la película de la mujer maravilla.
asi que lo primero que me viene a la cabeza es generar las iris en mí dominio de datos usando si tiene la propiedad name.

seria algo asi.
por cada blank node tenga alguna otra tripleta con schema:name
si la tiene, uso el valor de name de esa tripleta para generar la iri resultante.
como son tripletas ahora el tema de la deduplicación debería ser super simple (gracias casco!!) ya que las tripletas con las propiedades irían a la misma iri.
lo pruebo.





https://lov.linkeddata.es/dataset/lov/ 



----

Problema que veo en la definición del vocabulario.
protegé no permite manejar múltiples ontologías. para poder relacionarlas de forma simple.
si se podria importar ontologías de forma cruzada como hacemos con foaf. 
o sea,
definir una ontología /cines
definir una ontología /movies
abrir las ontologías en 2 proteges distintos
importar ontología en forma cruzada en cada uno de los protegé.

si me permitió importar los individuals dentro de la ontología que estoy trabajando.
asi es simple analizar que individuals fueron creados para cada clase. y no tengo que armar un filtro y escribir archivos separados como lo hice de ejemplo en tp1_2_owl.py





