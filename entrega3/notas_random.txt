

Proceso mental y motivaciones.
como la web semántica permite representar información desde múltiples fuentes, se me ocurrió hacer eso mismo con los tps.

tp1 exporatría en una ontología básica creada en protegé para experimentar la creación de ontologías simples cmo explicó casco en los videos.
en este caso voy a leer el archivo json merge que ya está estructurado.
esto me permite tener una idea de como es exportar datos estructurados de una base en formatos de la web semántica.
los tomo como si fueran "mis" datos generados en un sistema. (salas cines funciones, etc)

tp2 modificar los scrapers para manejar tripletas directamente, (o generar alguno simplificado ya que no tendría que manejar json-ld si no mas bien tripletas del grafo)
analizar si es que me conviene usar la ontología creada anteriormente o extender alguna, usar alguna combinación de las mismas.
la idea es experimentar un caso real sobre la web semántica donde se obtienen datos de múltiples ontologías.

en las etapas anteriores tendría en owl los datos para ser consumidos por la última etapa del sismtea.
armar algun merger para ver como me conviene tratar los datos. usar distintos grafos nombrados para almacenar los datos de distintas fuentes?.
o hacer merge directo (unión) de los grafos completos.


---
definición de la ontología.

como representar la duración de las películas en owl, ya que los tipos permitidos xsd:dateTime xsd:dateTimestamp
si o si llevan la fecha de acuerdo a la definición.
un posible datatype podría haber sido xsd:decimal y expresarla en minutos. (es la que tomé en la definición de twss_simple.ttl)
pero quería la posibilidad de expresarla en hh:mm:ss
veo que existe una ontología que es una recomendación para representar el tiempo.
https://www.w3.org/TR/owl-time/
la definición esta en : http://www.w3.org/2006/time#




----
Ya tengo los datos de tp1 en ontología y sus individuals.
ahora sigo con tp2 y la modificación del scraper.

idea, como comentó casco en el foro, esta bueno pensar en tripletas en vez de en clases.
asi que voy a hacer la prueba de cargar los jsons-ld que responden los sitios a tripletas y manejarlos internamente en el scraper como tripletas.
para todas las operaciones posteriores realizarlas en el grafo. (los merges, etc)
para esto voy a modificar el scraper que había realizado en el tp2. en vez de procesar los datos almacenados desde un archivo.

segunda idea, para evitar que haya problemas con internet y poder procesar mejor los datos, voy a encarar la solución de la siguiente forma:
1 - armar algun script que me descarge la info json-ld de los sitios en cuestión y las deje en archivos para sus posteriores análisis.
2 - armar un parser que procese la info en json-ld y la escriba en owl. (no debería ser complejo)

el punto 1 super simple. (utils/download_jsons.py)
el punto 2.
genero un parser para ver si puedo parsear la info json-ld a tripletas.
uso rdflib para testear.


primer problema encontrado:


  File "/home/pablo/.local/lib/python3.9/site-packages/rdflib_jsonld/context.py", line 65, in subcontext
    ctx.load(source)
  File "/home/pablo/.local/lib/python3.9/site-packages/rdflib_jsonld/context.py", line 200, in load
    self._prep_sources(base, source, sources)
  File "/home/pablo/.local/lib/python3.9/site-packages/rdflib_jsonld/context.py", line 213, in _prep_sources
    source = source_to_json(source_url)
  File "/home/pablo/.local/lib/python3.9/site-packages/rdflib_jsonld/util.py", line 28, in source_to_json
    return json.load(StringIO(stream.read().decode('utf-8')))
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte


cuando leo detenidamente el fuente de rdflib_jsonld está tratando de cargar contextos, no es relacionado a los datos de entrada del json que obtengo.
por lo que veo es un bug de schema.org

https://github.com/RDFLib/rdflib-jsonld/issues/84

efectivamente trata de descargar el context de schema.org ya que el json-ld de origen lo lista como contexto.
viendo el tráfico de red generado cuando ejecuto el script. veo que se comunica con schema.org

    192.168.10.5.45910 > 172.217.172.78.80: Flags [P.], cksum 0x913d (correct), seq 1:213, ack 1, win 502, options [nop,nop,TS val 1128910949 ecr 556658059], length 212: HTTP, length: 212
	GET / HTTP/1.1
	Accept-Encoding: identity
	Host: schema.org
	User-Agent: rdflib-5.0.0 (http://rdflib.net/; eikeon@eikeon.com)
	Accept: application/ld+json, application/json;q=0.9, */*;q=0.1
	Connection: close
	
19:05:23.272368 IP (tos 0x0, ttl 58, id 34438, offset 0, flags [none], proto TCP (6), length 52)
    172.217.172.78.80 > 192.168.10.5.45910: Flags [.], cksum 0x7e6a (correct), ack 213, win 261, options [nop,nop,TS val 556658075 ecr 1128910949], length 0
19:05:23.272417 IP (tos 0x0, ttl 58, id 34439, offset 0, flags [none], proto TCP (6), length 52)
    172.217.172.78.80 > 192.168.10.5.45910: Flags [.], cksum 0x7e6a (correct), ack 213, win 261, options [nop,nop,TS val 556658075 ecr 1128910949], length 0
19:05:23.445763 IP (tos 0x0, ttl 58, id 34553, offset 0, flags [none], proto TCP (6), length 303)
    172.217.172.78.80 > 192.168.10.5.45910: Flags [P.], cksum 0xcb52 (correct), seq 1:252, ack 213, win 261, options [nop,nop,TS val 556658250 ecr 1128910949], length 251: HTTP, length: 251
	HTTP/1.1 301 Moved Permanently
	Location: https://schema.org/
	X-Cloud-Trace-Context: 5dd336392bd7d9264f23ac7362a8d2e3;o=1
	Date: Sat, 08 May 2021 22:05:23 GMT
	Content-Type: text/html
	Server: Google Frontend
	Content-Length: 0
	Connection: close
	


curl -v -H "accept: application/ld+json" "http://schema.org"
https://github.com/schemaorg/schemaorg/issues/2578


el problema es generado por la cabecera link de shcema.org
es parte de la especificación de json-ld 1.1 para manejar la negociación de contenido y de donde descargar la info, pero parece que las librerías todavia no lo han actualizado.
asi que modifico HORRIBLEMENTE la librería como dicen en los issues de github para que carge del archivo local el contexto de schema.org


rdflib_jsonld/context.py

    def _prep_sources(self, base, inputs, sources, referenced_contexts=None,
            in_source_url=None):
        referenced_contexts = referenced_contexts or set()
        for source in inputs:
            if isinstance(source, str):
                source_url = urljoin(base, source)

                ''' TODO: horrible hack: sacado: https://github.com/RDFLib/rdflib-jsonld/issues/84 '''
                if "/schema.org" in source_url:
                    source_url = "src/jsonldcontext.jsonld"

                if source_url in referenced_contexts:
                    raise errors.RECURSIVE_CONTEXT_INCLUSION
                referenced_contexts.add(source_url)
                source = source_to_json(source_url)
                if CONTEXT not in source:
                    raise errors.INVALID_REMOTE_CONTEXT
            else:
                source_url = in_source_url

            if isinstance(source, dict):
                if CONTEXT in source:
                    source = source[CONTEXT]
                    source = source if isinstance(source, list) else [source]
            if isinstance(source, list):
                self._prep_sources(base, source, sources, referenced_contexts, source_url)
            else:
                sources.append((source_url, source))


-----

otra alternativa de procesarlo podría haber sido como comentó Leonardo en el post del foro.
cambiar el context de los json-ld para la url donde schema.org sirve los archivos 
https://schema.org/docs/jsonldcontext.jsonld

era mucho mas facil!!!

-----

ejemplo de ejecución del script para ver si convierte ok en el grafo a los archivos de json.

python3 src/utils/parse_jsons.py data/tp2/https___www.ecartelera.com_peliculas_wonder-woman-1984.json
python3 src/utils/parse_jsons.py data/tp2/https___www.imdb.com_title_tt7126948_.json 
python3 src/utils/parse_jsons.py data/tp2/https___www.metacritic.com_movie_wonder-woman-1984.json 
python3 src/utils/parse_jsons.py data/tp2/https___www.rottentomatoes.com_m_wonder_woman_1984.json 

asi que funciona bien en pasar los json-ld a turttle e interpretar el grafo.
lo único es que algunas iris al ser relativas y el parser del grafo no sabe cual es la url real las transofrma a iris del sistema de archivos.
ej:


[] a schema:Movie ;
    schema:actor [ a schema:Person ;
            schema:name "Pedro Pascal" ;
            schema:url <file:///name/nm0050959/> ],
        [ a schema:Person ;
            schema:name "Kristen Wiig" ;
            schema:url <file:///name/nm1325419/> ],


también noto que varios sitios representan la info con blank nodes y colecciones.

[] a schema:Movie ;
    schema:actor [ a schema:Person ;
            schema:name "Jamaal Burcher" ;
            schema:url <https://www.metacritic.com/person/jamaal-burcher> ],
        [ a schema:Person ;
            schema:name "Chris Silcox" ;
            schema:url <https://www.metacritic.com/person/chris-silcox> ],

en json-ld se ve bien el formato despues de serializar el grafo de tripletas.

[
  {
    "@id": "https://www.ecartelera.com/peliculas/wonder-woman-1984/",
    "@type": [
      "http://schema.org/Movie"
    ],
    "http://schema.org/actor": [
      {
        "@id": "_:N6c6979e8b5d44455853b74273b109464"
      },
      {
        "@id": "_:N88b28347cddf43578bcff381133b40cc"
      },

en ese caso las personas son blank nodes y usa referencias para ubicarlas dentro del json-ld

por ahora solo notarlo, para tenerlo en cuenta en la solución final.
ahora si entendiendo un poco el proceso armo un script que se conecte al sitio, descargue el json-ld y lo transforme a turtle.


----

ejecución inicial de una prueba del script.

python3 src/tp2_2_owl.py https://www.ecartelera.com/peliculas/wonder-woman-1984

hace merge de la info de tp1 y lo que lee del sitio en json-ld.
y lo escribe en un archivo merged.ttl

portegé no muestra cosas coherentes. muestra las clases pero todo lo demas como anotaciones.
no muestra correctamente las propiedades, etc.
tiene que estar faltando las clases que usa protegé para analizar y interpretar los datos en su interface.

----

para los individuals protegé le asigna la clase owl:NamedIndividual
y para las clases owl:Class

###  http://www.semanticweb.org/pablo/ontologies/2021/4/untitled-ontology-18#Pepe
:Pepe rdf:type owl:Class .

###  http://www.semanticweb.org/pablo/ontologies/2021/4/untitled-ontology-18#pepeInstance
:pepeInstance rdf:type owl:NamedIndividual ,
                       :Pepe .


ya hice el cambio a los scripts para que le agreguen eso a los datos del tp1.

---










https://lov.linkeddata.es/dataset/lov/ 



----

Problema que veo en la definición del vocabulario.
protegé no permite manejar múltiples ontologías. para poder relacionarlas de forma simple.
si se podria importar ontologías de forma cruzada como hacemos con foaf. 
o sea,
definir una ontología /cines
definir una ontología /movies
abrir las ontologías en 2 proteges distintos
importar ontología en forma cruzada en cada uno de los protegé.

si me permitió importar los individuals dentro de la ontología que estoy trabajando.
asi es simple analizar que individuals fueron creados para cada clase. y no tengo que armar un filtro y escribir archivos separados como lo hice de ejemplo en tp1_2_owl.py





