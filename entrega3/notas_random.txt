

Proceso mental y motivaciones.
como la web semántica permite representar información desde múltiples fuentes, se me ocurrió hacer eso mismo con los tps.

tp1 exporatría en una ontología básica creada en protegé para experimentar la creación de ontologías simples cmo explicó casco en los videos.
en este caso voy a leer el archivo json merge que ya está estructurado.
esto me permite tener una idea de como es exportar datos estructurados de una base en formatos de la web semántica.
los tomo como si fueran "mis" datos generados en un sistema y exportarlos al mundo usando tecnologías de la web semántica. (salas cines funciones, etc)

tp2 modificar los scrapers para manejar tripletas directamente, (generar alguno simplificado ya que no tendría que manejar json-ld si no mas bien tripletas del grafo)
analizar si es que me conviene usar la ontología creada anteriormente o extender alguna, usar alguna combinación de las mismas.
la idea es experimentar un caso real sobre la web semántica donde se obtienen datos de múltiples ontologías.

en las etapas anteriores tendría en owl los datos para ser consumidos por la última etapa del sismtea.
armar algun merger para ver como me conviene tratar los datos. usar distintos grafos nombrados para almacenar los datos de distintas fuentes?.
o hacer merge directo (unión) de los grafos completos.


---
definición de la ontología.

como representar la duración de las películas en owl, ya que los tipos permitidos xsd:dateTime xsd:dateTimestamp
si o si llevan la fecha de acuerdo a la definición.
un posible datatype podría haber sido xsd:decimal y expresarla en minutos. (es la que tomé en la definición de twss_simple.ttl)
pero quería la posibilidad de expresarla en hh:mm:ss
veo que existe una ontología que es una recomendación para representar el tiempo.
https://www.w3.org/TR/owl-time/
la definición esta en : http://www.w3.org/2006/time#
por ahora uso la solución simple que es representarla en un número dado en minutos.

----
Ya tengo los datos de tp1 en ontología y sus individuals correctamente generados por el script.

----
sigo con tp2 y la modificación del scraper.

idea, como comentó casco en el foro, esta bueno pensar en tripletas en vez de en clases.
asi que voy a hacer la prueba de cargar los jsons-ld que responden los sitios a tripletas y manejarlos internamente en el scraper como tripletas.
para todas las operaciones posteriores realizarlas en el grafo. (los merges, etc)
para esto voy a modificar el scraper que había realizado en el tp2. en vez de procesar los datos almacenados desde un archivo.

segunda idea, para evitar que haya problemas con internet y poder procesar mejor los datos, voy a encarar la solución de la siguiente forma:
1 - armar algun script que me descarge la info json-ld de los sitios en cuestión y las deje en archivos para sus posteriores análisis.
2 - armar un parser que procese la info en json-ld y la escriba en owl. (no debería ser complejo)

el punto 1 super simple. (utils/download_jsons.py)
el punto 2.
genero un parser para ver si puedo parsear la info json-ld a tripletas.
uso rdflib para testear.


primer problema encontrado (la librería rdflib y schema.org):


  File "/home/pablo/.local/lib/python3.9/site-packages/rdflib_jsonld/context.py", line 65, in subcontext
    ctx.load(source)
  File "/home/pablo/.local/lib/python3.9/site-packages/rdflib_jsonld/context.py", line 200, in load
    self._prep_sources(base, source, sources)
  File "/home/pablo/.local/lib/python3.9/site-packages/rdflib_jsonld/context.py", line 213, in _prep_sources
    source = source_to_json(source_url)
  File "/home/pablo/.local/lib/python3.9/site-packages/rdflib_jsonld/util.py", line 28, in source_to_json
    return json.load(StringIO(stream.read().decode('utf-8')))
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte


cuando leo detenidamente el fuente de rdflib_jsonld está tratando de cargar contextos, no es relacionado a los datos de entrada del json que obtengo.
por lo que veo es un bug de schema.org

https://github.com/RDFLib/rdflib-jsonld/issues/84

efectivamente trata de descargar el context de schema.org ya que el json-ld de origen lo lista como contexto.
viendo el tráfico de red generado cuando ejecuto el script. veo que se comunica con schema.org
usando tcpdump

    192.168.10.5.45910 > 172.217.172.78.80: Flags [P.], cksum 0x913d (correct), seq 1:213, ack 1, win 502, options [nop,nop,TS val 1128910949 ecr 556658059], length 212: HTTP, length: 212
	GET / HTTP/1.1
	Accept-Encoding: identity
	Host: schema.org
	User-Agent: rdflib-5.0.0 (http://rdflib.net/; eikeon@eikeon.com)
	Accept: application/ld+json, application/json;q=0.9, */*;q=0.1
	Connection: close
	
19:05:23.272368 IP (tos 0x0, ttl 58, id 34438, offset 0, flags [none], proto TCP (6), length 52)
    172.217.172.78.80 > 192.168.10.5.45910: Flags [.], cksum 0x7e6a (correct), ack 213, win 261, options [nop,nop,TS val 556658075 ecr 1128910949], length 0
19:05:23.272417 IP (tos 0x0, ttl 58, id 34439, offset 0, flags [none], proto TCP (6), length 52)
    172.217.172.78.80 > 192.168.10.5.45910: Flags [.], cksum 0x7e6a (correct), ack 213, win 261, options [nop,nop,TS val 556658075 ecr 1128910949], length 0
19:05:23.445763 IP (tos 0x0, ttl 58, id 34553, offset 0, flags [none], proto TCP (6), length 303)
    172.217.172.78.80 > 192.168.10.5.45910: Flags [P.], cksum 0xcb52 (correct), seq 1:252, ack 213, win 261, options [nop,nop,TS val 556658250 ecr 1128910949], length 251: HTTP, length: 251
	HTTP/1.1 301 Moved Permanently
	Location: https://schema.org/
	X-Cloud-Trace-Context: 5dd336392bd7d9264f23ac7362a8d2e3;o=1
	Date: Sat, 08 May 2021 22:05:23 GMT
	Content-Type: text/html
	Server: Google Frontend
	Content-Length: 0
	Connection: close
	


curl -v -H "accept: application/ld+json" "http://schema.org"
https://github.com/schemaorg/schemaorg/issues/2578


el problema es generado por la cabecera link de schema.org
es parte de la especificación de json-ld 1.1 para manejar la negociación de contenido y de donde descargar la info, pero parece que las librerías todavia no lo han actualizado.
asi que modifico HORRIBLEMENTE la librería como dicen en los issues de github para que carge del archivo local el contexto de schema.org
y descargo el archivo del context (src/jsonldcontext.jsonld) para leerlo del filesystem.

rdflib_jsonld/context.py

    def _prep_sources(self, base, inputs, sources, referenced_contexts=None,
            in_source_url=None):
        referenced_contexts = referenced_contexts or set()
        for source in inputs:
            if isinstance(source, str):
                source_url = urljoin(base, source)

                ''' TODO: horrible hack: sacado: https://github.com/RDFLib/rdflib-jsonld/issues/84 '''
                if "/schema.org" in source_url:
                    source_url = "src/jsonldcontext.jsonld"

                if source_url in referenced_contexts:
                    raise errors.RECURSIVE_CONTEXT_INCLUSION
                referenced_contexts.add(source_url)
                source = source_to_json(source_url)
                if CONTEXT not in source:
                    raise errors.INVALID_REMOTE_CONTEXT
            else:
                source_url = in_source_url

            if isinstance(source, dict):
                if CONTEXT in source:
                    source = source[CONTEXT]
                    source = source if isinstance(source, list) else [source]
            if isinstance(source, list):
                self._prep_sources(base, source, sources, referenced_contexts, source_url)
            else:
                sources.append((source_url, source))


-----

otra alternativa de procesarlo podría haber sido como comentó Leonardo en el post del foro.
cambiar el context de los json-ld para la url donde schema.org sirve los archivos 
https://schema.org/docs/jsonldcontext.jsonld

era mucho mas facil que andar debuggeando la libreria!!.

-----

ejemplo de ejecución del script para ver si convierte ok en grafo a los archivos de json.

python3 src/utils/parse_jsons.py data/tp2/https___www.ecartelera.com_peliculas_wonder-woman-1984.json
python3 src/utils/parse_jsons.py data/tp2/https___www.imdb.com_title_tt7126948_.json 
python3 src/utils/parse_jsons.py data/tp2/https___www.metacritic.com_movie_wonder-woman-1984.json 
python3 src/utils/parse_jsons.py data/tp2/https___www.rottentomatoes.com_m_wonder_woman_1984.json 

asi que funciona bien en pasar los json-ld a turttle e interpretar el grafo.
lo único es que algunas iris al ser relativas y el parser del grafo no sabe cual es la url real las transofrma a iris del sistema de archivos.
ej:


[] a schema:Movie ;
    schema:actor [ a schema:Person ;
            schema:name "Pedro Pascal" ;
            schema:url <file:///name/nm0050959/> ],
        [ a schema:Person ;
            schema:name "Kristen Wiig" ;
            schema:url <file:///name/nm1325419/> ],


también noto que varios sitios representan la info con blank nodes y colecciones.

[] a schema:Movie ;
    schema:actor [ a schema:Person ;
            schema:name "Jamaal Burcher" ;
            schema:url <https://www.metacritic.com/person/jamaal-burcher> ],
        [ a schema:Person ;
            schema:name "Chris Silcox" ;
            schema:url <https://www.metacritic.com/person/chris-silcox> ],

en json-ld se ve bien el formato despues de serializar el grafo de tripletas.

[
  {
    "@id": "https://www.ecartelera.com/peliculas/wonder-woman-1984/",
    "@type": [
      "http://schema.org/Movie"
    ],
    "http://schema.org/actor": [
      {
        "@id": "_:N6c6979e8b5d44455853b74273b109464"
      },
      {
        "@id": "_:N88b28347cddf43578bcff381133b40cc"
      },

en ese caso las personas son blank nodes y usa referencias para ubicarlas dentro del json-ld

por ahora solo notarlo, para tenerlo en cuenta en la solución final.
ahora si entendiendo un poco el proceso armo un script que se conecte al sitio, descargue el json-ld y lo transforme a turtle.


----

ejecución inicial de una prueba del script.

python3 src/tp2_2_owl.py https://www.ecartelera.com/peliculas/wonder-woman-1984

hace merge de la info de tp1 y lo que lee del sitio en json-ld.
y lo escribe en un archivo merged.ttl

portegé no muestra cosas coherentes. muestra las clases pero todo lo demas como anotaciones.
no muestra correctamente las propiedades, etc.
tiene que estar faltando las clases que usa protegé para analizar y interpretar los datos en su interface.

----

para los individuals protegé le asigna la clase owl:NamedIndividual
y para las clases owl:Class

###  http://www.semanticweb.org/pablo/ontologies/2021/4/untitled-ontology-18#Pepe
:Pepe rdf:type owl:Class .

###  http://www.semanticweb.org/pablo/ontologies/2021/4/untitled-ontology-18#pepeInstance
:pepeInstance rdf:type owl:NamedIndividual ,
                       :Pepe .


ya hice el cambio a los scripts para que le agreguen eso a los datos del tp1.

---

ahora el tema del procesamiento de los datos que vienen de los sitios con el context schema.org
lograr que protegé los lea correctamente del archivo escribo en turtle por el script
primero le voy a agregar esas clases. (NamedIndividual y Class)

sigue sin funcionar. no me muestra la instancia de schema:Movie con id: 
https://www.ecartelera.com/peliculas/wonder-woman-1984/

identifique cual es el problema.
protegé no lo toma como individual si tiene un / final en el iri.
claramente no es acorde con la sintaxis de las iris.

sintaxis de las iris: https://datatracker.ietf.org/doc/html/rfc3987
   
con un ejemplo como :

@prefix owl: <http://www.w3.org/2002/07/owl#> .

<http://schema.org/Movie> a owl:Class .

<https://www.ecartelera.com/peliculas/wonder-woman-1984> a <http://schema.org/Movie> .

me lo tomó ok el protegé

-----

bien entonces ahora tengo 2 opciones.
1 - extender mi ontología para abarcar las clases de schema.
2 - implementar una ontología especifica de schema y asociarla con la mía mediante owl, por ejemplo usando owl:sameAs

voy a probar con la opción 2 para ver como es la ontología de schema.org a ver si le puedo hacer un merge con la mía.
y replicar solo las propieades que usan los sitios actuales que estamos analizando en los tps


problema 1 definiendo schema.org
mainEntityOfPage --> dominio: Thing  range: CreativeWork, URL

veo los datos:

        [ a schema:Person ;
            schema:mainEntityOfPage <https://www.ecartelera.com/personas/robin-wright/> ;

la cual no es una iri válida, o sea que la debería tomar como una url, un texto. pero como está definida ahi sería una iri de un recurso.
probé y veo que si le saco la / final el sitio si redirige a la url con /, asi que sería correcto sacarle las /

con respecto a:
schema:countryOfOrigin "EE.UU." ;
no es una iri, si no un texto. asi que por ahora lo voy a modelar como un string.

duration:
schema:duration "PT2H31M" ;
tambien la modelo como string y como numero para abarcar mi modelo anterior.

url esta definido como hacia una iri    
schema:image [ a schema:ImageObject ;
    schema:url <https://img.ecartelera.com/carteles/13100/13170/009_p.jpg> ] ;

pero me parece mejor definirlo como string.

----

schema.org modela todo como clases. Text, etc.
pero en la práctica los datos los sitios los exportan como string.


--------

Problemón!!!! todos los jsons-ld consumidos de los sitios me generan blank nodes para los datos de las películas. usan colecciones para manejar a los actores, etc.
y no me los interpreta bien protegé.
así que tengo que convertilos a nodes normales con iris.

defini un pefijo para mi namespace de datos.
tsswd: https://raw.githubusercontent.com/pablodanielrey/twss/master/owl/data/

y arme una rutina que busca los blank nodes y los reemplaza por iris de datos.
o sea generaría una instancia nombrada para que protegé lo reconozca ok.

abri el archivo en protegé y al fin veo las instancias dentro de individuals by Class

---

ahora si me lo interpreta pero todas las propiedades definidas en la ontología me las muestra como anotiaciones y no como propiedades en los individuals.
si bien están definidas en la ontología, y el archivo de individuals esta con datos correctos.
ontologia:
###  http://schema.org/name
<http://schema.org/name> rdf:type owl:DatatypeProperty ;
                         rdfs:domain owl:Thing ;
                         rdfs:range xsd:string .


datos ej:
@prefix schema: <http://schema.org/> .
@prefix twssd: <https://raw.githubusercontent.com/pablodanielrey/twss/master/owl/data/> .

twssd:1f950821-b321-4914-9263-a4a93af51986 a schema:Person ;
    schema:mainEntityOfPage <https://www.ecartelera.com/personas/robin-wright/> ;
    schema:name "Robin Wright" .

contra una que si muestra:
:pepe rdf:type owl:NamedIndividual ,
               <http://schema.org/Movie> ;
      <http://schema.org/name> "pepe" .


voy a probar agregando NamedIndividual a ver si lo entiende protegé.

NO funciona!!!. no es solo eso.

lo que estoy haciendo es abrir el archivo con los individuals en protegé y posteriormente importo mi ontología como indicó casco en el video para el ejemplo de schema.org
pero no la interpreta bien de esta forma. asi que voy por la mas simple.
voy a unificar los archivos de ontología y el de individuals en uno solo, que posteriormente abro con protegé.
modifico el script para realizar esto.

al manejarlo como tripletas dentro del mismo scraper esto es tan simple como una suma de grajos.
gfinal = gontology + g 

FUNCIONO!!!!. protegé me muestra la ontología y los individuals con las propiedaes correctamente, no en la parte de anotaciones!!!!!
perdí casi 1 día con esto. resumen, no la compliques anticipando funcionalidad cuando todavía no hiciste andar el prototipo.

------

Perfecto, hasta este punto tengo funcionando:
1 - la definición de una ontología con las clases de schema.org (solo las que uso)
2 - la carga del json-ld desde la página ecartelera y la transformación de ese json-ld al grafo de tripletas.
3 - el procesamiento de las tripletas para generar correctamente el grafo de individuals. (eliminación de blank nodes, etc)
4 - el merge entre la ontología que define las clases y los individuals procesados anteriormente.

ahora queda cargar los otros jsons-ld e ir mergeando la info. definiendo las clases que me faltan y analizando en protegé si funciona.

-----

toca el turno a imdb. modifico el script para 
1 - carga del json-ld
2 - lo parsea en el grafo de tripletas
3 - hace unión con el grafo anterior

y despues los puntos normales de unir con la ontología y escribir el turtle final.

FUNCIONAAA!!!.
lo único que veo es que como anotaciones me dejo las propiedaes que todavía no defini de schema.org en mi ontología.

paso siguiente definir esas propiedades faltantes.

schema:creator 
schema:contentRating
schema:datePublished
schema:keywords
schema:review
schema:trailer
schema:url
schema:productionCompany
schema:potentialAction
schema:releasedEvent
schema:actionPlatform
schema:inLanguage
schema:urlTemplate
schema:location
schema:availabilityStarts
schame:eligibleRegion
schema:category
schema:author
schema:dateCreated
schema:itemReviewed
schema:reviewBody
schema:reviewRating
schema:embedUrl
schema:thumbnail
schema:thumbnailUrl
schema:uploadDate


esta schema:url es raro porque la definí, pero la tengo definida como dataProperty
y en el archivo turtle esta:

schema:url <https://www.imdb.com/title/tt7126948/> .

o sea una objectProperty, la cambio.

funcionó. ahora protegé lo muestra como una propiedad dentor del individual.
sigo con el problema de que las iris con / final no las interpreta bien protegé. no muestra nada.


-----

defino todas las propiedades que me faltan para terminar con la ontología de schema.org

varias propiedades en schema.org se definen como objectProperty pero en los datos en la práctica aparecen como dataProperties.
asociadas a strings. ejemplos:
schema:contentRating
schema:datePublished

un problema grande es schema:url 
que en los datos aparece asociado a iris y asociado a strings.!!!!! TEMA A CONSULTAR!!!.
asumo que lo convertiría en irirs en los scrapers, sería la opción mas correcta.


termine de definir todas las propiedades y clases faltantes, corri el script nuevamente y abrí el archivo en protegé y me interpretó correctamente las propiedades.
no mas datos en anotaciones.

----

agrego metacritic y veo como resulta.
obvio, problemas con la librería de parseo nuevamnete.

pablo@xiaomi:/src/github/facu-infor/twss/entrega3$ python3 src/tp2_2_owl.py 
Descargando información de : https://www.ecartelera.com/peliculas/wonder-woman-1984
Traceback (most recent call last):
  File "/src/github/facu-infor/twss/entrega3/src/tp2_2_owl.py", line 100, in <module>
    g.parse(data=djson_ld, format='json-ld', publicID=url)
  File "/home/pablo/.local/lib/python3.9/site-packages/rdflib/graph.py", line 1078, in parse
    parser.parse(source, self, **args)
  File "/home/pablo/.local/lib/python3.9/site-packages/rdflib_jsonld/parser.py", line 95, in parse
    to_rdf(data, conj_sink, base, context_data)
  File "/home/pablo/.local/lib/python3.9/site-packages/rdflib_jsonld/parser.py", line 107, in to_rdf
    return parser.parse(data, context, dataset)
  File "/home/pablo/.local/lib/python3.9/site-packages/rdflib_jsonld/parser.py", line 125, in parse
    context.load(l_ctx, context.base)
  File "/home/pablo/.local/lib/python3.9/site-packages/rdflib_jsonld/context.py", line 200, in load
    self._prep_sources(base, source, sources)
  File "/home/pablo/.local/lib/python3.9/site-packages/rdflib_jsonld/context.py", line 220, in _prep_sources
    source = source_to_json(source_url)
  File "/home/pablo/.local/lib/python3.9/site-packages/rdflib_jsonld/util.py", line 23, in source_to_json
    source = create_input_source(source, format='json-ld')
  File "/home/pablo/.local/lib/python3.9/site-packages/rdflib/parser.py", line 193, in create_input_source
    input_source = URLInputSource(absolute_location, format)
  File "/home/pablo/.local/lib/python3.9/site-packages/rdflib/parser.py", line 113, in __init__
    file = urlopen(req)
  File "/usr/lib/python3.9/urllib/request.py", line 214, in urlopen
    return opener.open(url, data, timeout)
  File "/usr/lib/python3.9/urllib/request.py", line 523, in open
    response = meth(req, response)
  File "/usr/lib/python3.9/urllib/request.py", line 632, in http_response
    response = self.parent.error(
  File "/usr/lib/python3.9/urllib/request.py", line 561, in error
    return self._call_chain(*args)
  File "/usr/lib/python3.9/urllib/request.py", line 494, in _call_chain
    result = func(*args)
  File "/usr/lib/python3.9/urllib/request.py", line 641, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
pablo@xiaomi:/src/github/facu-infor/twss/entrega3$ 


parece que no encuentra algo del contexto también.
cambie para que el json lo busque por http y que tambien cambie el prefijo de https.

        json_ld['@context'] = json_ld['@context'].replace('http://schema.org','http://schema.org/docs/jsonldcontext.jsonld')
        json_ld['@context'] = json_ld['@context'].replace('https://schema.org','http://schema.org/docs/jsonldcontext.jsonld')

y salio funcionando. parece algo temporal.

19:31:08.971899 IP (tos 0x0, ttl 64, id 19553, offset 0, flags [DF], proto TCP (6), length 60)
    192.168.10.5.57782 > 172.217.172.110.80: Flags [S], cksum 0xe8e3 (correct), seq 21734641, win 64240, options [mss 1460,sackOK,TS val 2312580635 ecr 0,nop,wscale 7], length 0
19:31:08.978163 IP (tos 0x0, ttl 64, id 19554, offset 0, flags [DF], proto TCP (6), length 52)
    192.168.10.5.57782 > 172.217.172.110.80: Flags [.], cksum 0x5e81 (correct), ack 356733432, win 502, options [nop,nop,TS val 2312580641 ecr 1592126450], length 0
19:31:08.978319 IP (tos 0x0, ttl 64, id 19555, offset 0, flags [DF], proto TCP (6), length 289)
    192.168.10.5.57782 > 172.217.172.110.80: Flags [P.], cksum 0x99f6 (correct), seq 0:237, ack 1, win 502, options [nop,nop,TS val 2312580641 ecr 1592126450], length 237: HTTP, length: 237
	GET /docs/jsonldcontext.jsonld HTTP/1.1
	Accept-Encoding: identity
	Host: schema.org
	User-Agent: rdflib-5.0.0 (http://rdflib.net/; eikeon@eikeon.com)
	Accept: application/ld+json, application/json;q=0.9, */*;q=0.1
	Connection: close
	
19:31:09.159336 IP (tos 0x0, ttl 64, id 19556, offset 0, flags [DF], proto TCP (6), length 52)
    192.168.10.5.57782 > 172.217.172.110.80: Flags [.], cksum 0x5b19 (correct), ack 273, win 501, options [nop,nop,TS val 2312580822 ecr 1592126633], length 0
19:31:09.160324 IP (tos 0x0, ttl 64, id 19557, offset 0, flags [DF], proto TCP (6), length 52)
    192.168.10.5.57782 > 172.217.172.110.80: Flags [F.], cksum 0x5b16 (correct), seq 237, ack 274, win 501, op

-------------

parece que todo funciona ok, verifique las propieades y protegé me las tiró en propiedades y no anotaciones.
asi que agrego el ultimo sitio que es rottentomatoes

aparecieron algunas en anotaciones. 

schema:actors
schema:publisher
schema:dateModified
schema:embedUrl
schema:contentUrl
schema:thumbnailUrl
schema:sameAs
schema:character

-------------

termine de definir las nuevas propieades y corrí de nuevo el script.
perfecto, reconoció todo.
ahora solo me falta analizar como hago para deduplicar los datos.
en principo el grafo resultante tiene muchos ids autogenerados pero que repiten su contenido.
o sea hay varios individuals personas que son las mismas personas.
lo mismo con la película de la mujer maravilla.
asi que lo primero que me viene a la cabeza es generar las iris en mí dominio de datos usando si tiene la propiedad name.

seria algo asi.
por cada blank node tenga alguna otra tripleta con schema:name
si la tiene, uso el valor de name de esa tripleta para generar la iri resultante.
como son tripletas ahora el tema de la deduplicación debería ser super simple (gracias casco!!) ya que las tripletas con las propiedades irían a la misma iri.
lo pruebo.

Funcionoooo!!!. ej:

twssd:Chris_Silcox a schema:Person,
        owl:NamedIndividual ;
    schema:name "Chris Silcox" ;
    schema:url <https://www.metacritic.com/person/chris-silcox> .


pero asi también veo cosas como:

<https://raw.githubusercontent.com/pablodanielrey/twss/master/owl/data/FilmWeek_%28KPCC_-_NPR_Los_Angeles%29> a schema:Organization,
        owl:NamedIndividual ;
    schema:name "FilmWeek (KPCC - NPR Los Angeles)" ;
    schema:url <https://www.rottentomatoes.com/source-3509> .

que es generado por el fuente:

                "publisher": {
                    "@type": "Organization",
                    "name": "FilmWeek (KPCC - NPR Los Angeles)",
                    "url": "https://www.rottentomatoes.com/source-3509"
                }

o sea que hay que de alguna forma analizar los datos que se obtienen de los fuentes json-ld por validez y coherencia. (como se hace? mas que usar algun validador de ontología? es suficiente?)

------------

veo que funcionó bien,
me hizo merge de las películas Wonder Woman 1984!!
tengo a un solo individual todas las tripletas de las distintas fuentes de datos.
(actores, reviews, imagenes, publishers, etc)

pero tengo 2 películas wonder woman 1984.
una con nombre, que es resultado del merge y del proceo anterior.
y otra que sale de ecartelera que la exporta con @id.
como tiene id en trutle es el iri de la película. asi que no es un blank node y no se genera el iri con el nombre como en los otros casos.

 {
        "@context": "http://schema.org",
        "@type": "Movie",
        "@id": "https://www.ecartelera.com/peliculas/wonder-woman-1984/",

pero ese id no es válido para protegé!
le tengo que sacar el / final y ahi si lo muestra.
no sería una iri válida.

convertido a turtle veo:

<https://www.ecartelera.com/peliculas/wonder-woman-1984/> a schema:Movie ;
    schema:actor [ a schema:Person ;
            schema:mainEntityOfPage <https://www.ecartelera.com/personas/kristen-wiig/> ;
            schema:name "Kristen Wiig" ],

  
tengo varias alternativas.
1 - le hago percha el @id de ecartelera antes de procesar todo.
esto generaría un blanknode para esta también y sería matcheada por la iri generada con el nombre en los pasos posteriores.
 pero me parece una solución específica a este caso y no correcta. debería buscar soluciones mas generales.
 aunque esto lo que estaría significando que incorporo a esa película a mi ontología. uso una iri interna para identificar el recurso de la película.

2 - validar de alguna forma que el grafo generado por el json-ld de cada fuente tenga iris válidas para protegé para sus recursos.
no solucionaría el tema del merge, ya que si dejamos la iri como: https://www.ecartelera.com/peliculas/wonder-woman-1984
va a ser distinto recurso que los otros wonder woman.
(es la que voy a intentar realizar)

resolvió el problema ok. pero me suena que no se puede realizar eso!!!
lo deje comentado en el código como una función para verificar con diego y con casco.
básicamente busco en sujetos y objetos si son referencias a iris y si tienen el / final se lo elimino.
"""
/////////////////////////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////////////////////////

    VERIFICAR CON CASCO Y CON DIEGO SI ESTO ESTA OK !!!!
    CREO QUE ESTARIA MAL MODIFICAR LAS IRIs!!

/////////////////////////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////////////////////////
"""
def validate_iris_for_protege(g:Graph):
    ''' 
        hago una simple validación y cambio de iris 
        para que protege me los muestre ok.
        si termina en / le saco la /
        NOTA: NO se si esto esta ok, ya que las iris no son urls para buscar cosas, si 
        no son identificadores que no tienen mayor sentido que identificar!!
    '''
    subjects = set()
    objects = set()
    for st,sp,so in g:
        if isinstance(st,URIRef) and str(st).endswith('/'):
            subjects.add(st)

        if isinstance(so,URIRef) and str(so).endswith('/'):
            objects.add(so)
        
    for s in subjects:
        niri = URIRef(str(s[:-1]))
        for st, sp, so in g.triples((s,None,None)):
            assert st == s
            print(f'agregando {niri}')
            g.add((niri, sp, so))
            print(f'Eliminanod {st}')
            g.remove((st, sp, so))

    for o in objects:
        niri = URIRef(str(o[:-1]))
        for st, sp, so in g.triples((None,None,o)):
            assert so == o
            print(f'agregando {niri}')
            g.add((st, sp, niri))
            print(f'Eliminanod {so}')
            g.remove((st, sp, so))

-----------------------------


se muestra bien ahora en protegé. pero me hace mucho ruido eso lo de la modificación de la iri.


--------------------

armo un merger.py para mergear todos los grafos. los de ontologías y los de los individuals.
defini una ontología que relaciona a las 2 que definí para cada uno de los "sistemas"
twss.ttl
donde se setean owl:equivalentClass y owl:equivalentProperty
por ejemplo:

schema:actor  owl:equivalentProperty twss:actor
schema:actors owl:equivalentProperty twss:actor

y clases

schema:Person owl:equivalentClass twss:Person
schema:Movie owl:equivalentClass twss:Movie

etc


FUNCIONAAA!!!.
veo por ejemplo que la película que existía en los 2 tps. Wonder Woman 1984 
fue mergeada completa.
tiene las propieades correctas. 
schema:actors
schema:actor
twss:actor
y que por ejemplo Pedro Pascal, se encuentra como objeto en todas esas propieades.
y la iri es el mismo individual Perdo Pascal.
que tiene de nombre:
twss:nombre
schema:name 

----------------------


En resumen, creo que logré mis objetivos que eran.
1 - generar una ontología y representar datos que tengo estructurados ya en un sistema interno
2 - generar una ontología que se ajuste a alguna standrad de internet y consumir datos de otros sitios directamente sin pasar por modelos de clases, objetos, etc como lo comentaba diego.
3 - realizar algun merge de ontologías y relacionarlas para representar lo que se haría en le mundo real.

en resumen.
se tiene :



----------------
/////////////////////////////////////////////////////
/////////////////////////////////////////////////////


tengo miedo de no haber cumplido con lo que el tp3 significa, asi que voy a realizar una variante.
voy a generar otra ontología extendida de schema para que cubra los campos de mi ontología simple del tp1.
y generar otro script tp1_2_owl_extended.py
que cargue los datos del tp1 dentro de esa ontología extendida.


genero las siguientes clases colgadas de la jerarquía de twss_schema.ttl

Cinema
ShowRoom
Show

y algunas propieades
ahora genero el script que tome los datos de tp1 y los incluya en un grafo generado con esta ontología.
-
generé el archivo con la nueva ontología y me muestra correctamente los cines, las salas y los shows.
pero si uso mi namespace de mi ontología no toma las películas y las personas de las ontologías importadas. me genera clases nuevas colgadas de thing.
o sea:

al usar:

@prefix : <https://raw.githubusercontent.com/pablodanielrey/twss/master/owl/twss_schema_extended.ttl#> .

<https://raw.githubusercontent.com/pablodanielrey/twss/master/owl/twss_schema_extended.ttl> a owl:Ontology ;
    owl:imports <https://raw.githubusercontent.com/pablodanielrey/twss/master/owl/twss_schema.ttl> .


y generar los individuals:


twssd:Los_Intrusos a owl:NamedIndividual,
        :Movie ;
    :actor twssd:Andrew_Ellis,
        twssd:Ian_Kenny,
        twssd:Jake_Curran,
        twssd:Maisie_Williams,
        ......
        .....

esa película la cuelga de owl:Thing. en vez de schema:Movie
o sea la importación de ontologías no funciona como herencia de ontologías!!. owl:import no quiere decir que te importa todas las clases dentro de ese namespace.

---

modifico el script para usar las schemas correctas en las personas y las movies.

-

duda. no se en el caso de las synopsis si usar la propiedad ?
abstract o text
uso text

tengo que agregar las propiedades que me faltan.

schema:text
twss:movie
twss:movieFormat

algo interesante!!!!! 
definir una propieadad en una ontología pero usando el namespace de otra.
ej:
en twss_schema_extended defini:

#################################################################
#    Data properties
#################################################################

###  http://schema.org/text
<http://schema.org/text> rdf:type owl:DatatypeProperty ;
                         rdfs:domain <http://schema.org/CreativeWork> ;
                         rdfs:range xsd:string .


###  https://raw.githubusercontent.com/pablodanielrey/twss/master/owl/twss_schema_extended.ttl#movieFormat
:movieFormat rdf:type owl:DatatypeProperty ;
             rdfs:domain :Show ;
             rdfs:range xsd:string .

asi que si uso schema_extended para los datos (individuals) como contexto. PUEDO!! usar el namespace de 
schema:text y no usar twss (como en el caso de la propiedad movieFormat) 
en resumen.
se pueden definir ontologías que agreguen funcionalidad a otras existentes dentro del mismo namespace de iris de la anterior!!!. me acaba de volar la cabeza con la posibilidades que brinda.



--------


ahora si tengo los datos del tp1 en un archivo turtle usando la ontología extendida. (creo que esto va mas en linea con lo que se pedía en el tp3)
voy a modificar el script que unifica todo y realizar la versión 2 final de la solución.
en este caso se mantiene todo dentro de una ontologia original y una extendida de esa misma.
sin relación entre ontologías usanod owl:equivalentClass y owl:equivalentProperty

<http://schema.org/actor> owl:equivalentProperty <https://raw.githubusercontent.com/pablodanielrey/twss/master/owl/twss_simple.ttl#actor> .
<http://schema.org/actors> owl:equivalentProperty <https://raw.githubusercontent.com/pablodanielrey/twss/master/owl/twss_simple.ttl#actor> .
<http://schema.org/director> owl:equivalentProperty <https://raw.githubusercontent.com/pablodanielrey/twss/master/owl/twss_simple.ttl#director> .
<http://schema.org/name> owl:equivalentProperty <https://raw.githubusercontent.com/pablodanielrey/twss/master/owl/twss_simple.ttl#name> .
<http://schema.org/Movie> owl:equivalentClass <https://raw.githubusercontent.com/pablodanielrey/twss/master/owl/twss_simple.ttl#Movie> .
<http://schema.org/Person> owl:equivalentClass <https://raw.githubusercontent.com/pablodanielrey/twss/master/owl/twss_simple.ttl#Person> .


-----


bueno termine el script que mergea las 2 fuentes.
merger_v2.py

funciona bien, protegé me lee bien todos los datos y me muestra bien los individuals.
también hizo merge correctamente de todos los datos. las películas están ok.

---

solo me falta agregar una propiedad que se me chispoteó y la estoy viendo.
schema:character
no la puedo definir como indica en schema.org porque los datos solo asocian strings y no Persons.
así que la defino como dataproperty
la defino en la schema original twss_schema.ttl

----
funcionó perfecto.

analizando los datos veo que use schema:text y schema:description 
para la sinopsis de las películas. modifico los scripts para usar description.


---


funcionó!!!.
asi que solo me queda completar los arhcivos de texto explicando un poco.
y realizar la entrega.
llegué nuevamente con lo justo en tiempo.


















https://lov.linkeddata.es/dataset/lov/ 



