inicio el análisis del tp.
no me pude conectar a la teoría por problemas en el laburo pero ya vi todos los videos.

super super super interesante el tema de la inferencia y sparql como lenguaje de consulta.
sparql y el motor de inferencia era lo que faltaba para poder realizar pruebas simples y evaluar los efectos de esas alternativas de implementación sobre una web semántica en general.

por ejemplo algunas de mis dudas principales todavía son:
1 - veneficios/desventajas de armar una ontología completa independiente por sobre reusar conceptos y hacer merge de ontologías existentes.
2 - efectos de la inferencia sobre el grafo mergeado de varias ontologías, por ejemplo cambiar el rango de una propiedad para abarcar mas clases, y analizar esos efectos en todo el grafo mergeado.
3 - ventajas y desventajas de particionar nuestra ontología en dominios especificos a una solución. (ej: como lo que vimos en el tp1 y tp2. al mergear la info podríamos haber estructurado de distinta forma las ontologías que creamos) 
por ejemplo: 
ontología tp1 - solo con los conceptos manejados en tp1
ontologia tp2 - solo con los conceptos manejados en tp2
ontología tp3 = merge entre las 2 ontologías anteriores y usar sameAs, equivalentProperty, equivalentClass, etc para linkear conceptos repetidos de las ontologías anteriores.

otra posible solución
ontologia tp1
extensión ontologia tp1 para abarcar conceptos de tp2
extensión ontología tp2 para abrcar conceptos tp3

también, efectos de :
armar en mismo archivo, modificandolo y llevando distintas versiones del mismo, cada una con cambios incrementales? 
o armarlo en distintos archivos y usar owl:import a cada extensión de la ontología?
etc.

usando sparql e inferencia podemos ver los resultados que tiene una solución o la otra. si generan efectos no deseados usando inferencia, etc.
usando protegé no me quedaron claras estas cosas, claramente porque no lo se usar correctamente y me falta práctica.

---

también permitiría analizar un poco patrones de diseño de ontologías.
parecido a los patrones de diseño del paradigma orientado a objetos.
casco publicó un link a material sobre eso en el foro.
https://patterns.dataincubator.org/book/
que profundidad de ontología usar? usar subclases o usar composición?, etc.
y ver rápidamente los resultados de esos patrones de diseño. si general algún efecto no deseado. etc.

---


material publicado por leonel sobre wikidata y dbpedia.

https://recyt.fecyt.es/index.php/ThinkEPI/article/view/thinkepi.2018.31

---

ALUCINANTE el artículo!!. lo dejo como material dentro del tp.
y casi que responde a las consultas básicas del tp y aclara las diferencias entre los 2 proyectos!.

el artículo que pasó diego en el foro para el trabajo esta bueno. es medio pesado con las estadísticas pero da un panorama muchisimo mas completo de lo que es dbpedia.

----

después de leer todo lo de dbpedia me quedó otro sabor sobre lo que pensaba de dbpedia.
me gusta mucho que haya sido pensado como una ontología universal. por lo que entendí, si bien se centra en el procesamiento de los datos de wikipedia, la clave fue realizar los mapeos 
de los datos de wikipedia con una ontología pensada y estrucutrada para representar los conceptos independientemente de como estén estructurados en wikipedia.
y me gustó también inferir propiedades y características mediante extractores NLP para tratar de representarlos con clases y propieades de la ontología. 
El ejemplo del asignar género a la persona linkeada en base a la cantidad de veces que se usa she, he dentro del artículo de wikipedia, etc.

estoy pensando que en este momento invertiría lo que hice en el tp2, (usar schema.org como ontología), y pasar a usar dbpedia como la ontología base. que además ya tiene equivalencias definidas a las clases de schema.org.
use schema.org debido a que la info retornada de las fuentes está en json-ld usando "clases" de schema.org, pero podría usar inferencia para obtener la clase equivalente en la ontología de dbpedia,
y almacenarla de esa forma en mis datos. es un paso mas, pero creo que con todas las herraminetas que aprendimos es trivial realizarlo y tendría almacenado los datos de forma mas genérica y compatible con el mundo.

a lo sumo almacenar la info con las 2 clases.
    rdf:type schema:Person
    rdf:type dbo:Person

la primera la obtengo de los datos exportados por las fuentes de las pelis.
la segunda la obtendría de la inferencia sobre la ontología de dbpedia.

---

igual me falta leer sobre wikidata así que sigo analizando las cosas.
ahora a ver el punto sparql de dbpedia.

---


select ?property ?object where {
 dbr:National_University_of_La_Plata ?property ?object.
}
Explore:

Los distintos formatos para los resultados
1 . En la pantalla que muestra resultados de queries en HTML, ¿qué pasa cuando hace click en una URI que pertenece al dominio dbpedia.org? 
¿qué pasa si la URI está afuera? 
¿A que cree que se debe la diferencia en comportamiento? (recuerde que la clickear la URI estamos dereferenciando el recurso, como discutimos en la clase respecto a Linked Data)

para el caso de las propiedades. tomo como ejemplo algunas internas a dbpedia y otras externas a dbpedia.
http://www.w3.org/1999/02/22-rdf-syntax-ns#type
http://www.w3.org/2000/01/rdf-schema#label
http://xmlns.com/foaf/0.1/name
http://purl.org/dc/terms/subject
http://www.w3.org/2002/07/owl#sameAs
http://www.w3.org/ns/prov#wasDerivedFrom


http://dbpedia.org/ontology/wikiPageWikiLink
http://dbpedia.org/property/name

si consulto la propiedad name por ejemplo primero me redirecciona a la iri con https. y después me responde con un html con una descripción para humanos de la propiedad.
en esa respuesta se ve que las cabeceras te dan la url del recurso en distintos formatos (entre ellos turtle). (que también tiene una redirección a la iri de https)
por ej describo bien el camino.

pablo@xiaomi:/src/github/facu-infor/twss$ curl -I http://dbpedia.org/property/name
HTTP/1.1 303 See Other
Server: nginx/1.18.0
Date: Mon, 31 May 2021 14:03:08 GMT
Content-Type: text/html
Content-Length: 153
Connection: keep-alive
Location: https://dbpedia.org/property/name
Access-Control-Allow-Credentials: true
Access-Control-Allow-Methods: HEAD, GET, POST, OPTIONS
Access-Control-Allow-Headers: Depth,DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Accept-Encoding

pablo@xiaomi:/src/github/facu-infor/twss$ curl -I https://dbpedia.org/property/name
HTTP/2 200 
date: Mon, 31 May 2021 14:03:16 GMT
content-type: text/html; charset=UTF-8
content-length: 11893
vary: Accept-Encoding
server: Virtuoso/08.03.3319 (Linux) x86_64-centos_6-linux-glibc2.12  VDB
expires: Mon, 07 Jun 2021 14:03:16 GMT
link: <http://creativecommons.org/licenses/by-sa/3.0/>;rel="license", <http://dbpedia.org/data4/name.rdf>; rel="alternate"; type="application/rdf+xml"; title="Structured Descriptor Document (RDF/XML format)", <http://dbpedia.org/data4/name.n3>; rel="alternate"; type="text/n3"; title="Structured Descriptor Document (N3 format)", <http://dbpedia.org/data4/name.ttl>; rel="alternate"; type="text/turtle"; title="Structured Descriptor Document (Turtle format)", <http://dbpedia.org/data4/name.json>; rel="alternate"; type="application/json"; title="Structured Descriptor Document (RDF/JSON format)", <http://dbpedia.org/data4/name.atom>; rel="alternate"; type="application/atom+xml"; title="OData (Atom+Feed format)", <http://dbpedia.org/sparql?default-graph-uri=http%3A%2F%2Fdbpedia.org&query=DESCRIBE%20%3Chttp%3A%2F%2Fdbpedia.org%2Fproperty%2Fname%3E&format=text%2Fcsv>; rel="alternate"; type="text/csv"; title="Structured Descriptor Document (CSV format)", <http://dbpedia.org/sparql?default-graph-uri=http%3A%2F%2Fdbpedia.org&query=DESCRIBE%20%3Chttp%3A%2F%2Fdbpedia.org%2Fproperty%2Fname%3E&format=text%2Fcxml>; rel="alternate"; type="text/cxml"; title="Structured Descriptor Document (CXML format)", <http://dbpedia.org/data4/name.ntriples>; rel="alternate"; type="text/plain"; title="Structured Descriptor Document (N-Triples format)", <http://dbpedia.org/sparql?default-graph-uri=http%3A%2F%2Fdbpedia.org&query=DESCRIBE%20%3Chttp%3A%2F%2Fdbpedia.org%2Fproperty%2Fname%3E&format=application%2Fmicrodata%2Bjson>; rel="alternate"; type="application/microdata+json"; title="Structured Descriptor Document (Microdata/JSON format)", <http://dbpedia.org/sparql?default-graph-uri=http%3A%2F%2Fdbpedia.org&query=DESCRIBE%20%3Chttp%3A%2F%2Fdbpedia.org%2Fproperty%2Fname%3E&format=text%2Fhtml>; rel="alternate"; type="text/html"; title="Structured Descriptor Document (Microdata/HTML format)", <http://dbpedia.org/sparql?default-graph-uri=http%3A%2F%2Fdbpedia.org&query=DESCRIBE%20%3Chttp%3A%2F%2Fdbpedia.org%2Fproperty%2Fname%3E&format=application%2Fld%2Bjson>; rel="alternate"; type="application/ld+json"; title="Structured Descriptor Document (JSON-LD format)", <http://dbpedia.org/property/name>; rel="http://xmlns.com/foaf/0.1/primaryTopic", <http://dbpedia.org/property/name>; rev="describedby", <http://dbpedia.mementodepot.org/timegate/http://dbpedia.org/property/name>; rel="timegate"
cache-control: max-age=604800
access-control-allow-credentials: true
access-control-allow-methods: HEAD, GET, POST, OPTIONS
access-control-allow-headers: Depth,DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Accept-Encoding
accept-ranges: bytes

pablo@xiaomi:/src/github/facu-infor/twss$ curl -I http://dbpedia.org/data4/name.ttl
HTTP/1.1 303 See Other
Server: nginx/1.18.0
Date: Mon, 31 May 2021 14:03:30 GMT
Content-Type: text/html
Content-Length: 153
Connection: keep-alive
Location: https://dbpedia.org/data4/name.ttl
Access-Control-Allow-Credentials: true
Access-Control-Allow-Methods: HEAD, GET, POST, OPTIONS
Access-Control-Allow-Headers: Depth,DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Accept-Encoding

pablo@xiaomi:/src/github/facu-infor/twss$ curl -I https://dbpedia.org/data4/name.ttl
HTTP/2 200 
date: Mon, 31 May 2021 14:03:36 GMT
content-type: text/turtle; charset=UTF-8
content-length: 306
server: Virtuoso/08.03.3319 (Linux) x86_64-centos_6-linux-glibc2.12  VDB
x-sparql-default-graph: http://dbpedia.org
content-disposition: filename=sparql_2021-05-31_14-01-47Z.ttl
expires: Mon, 07 Jun 2021 14:03:36 GMT
cache-control: max-age=604800
access-control-allow-credentials: true
access-control-allow-methods: HEAD, GET, POST, OPTIONS
access-control-allow-headers: Depth,DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Accept-Encoding
accept-ranges: bytes



por ejemplo si requerimos el turtle de la iri de la propiedad sin analizar la cabecera link de la respuesta vemos:

pablo@xiaomi:/src/github/facu-infor/twss$ curl -H 'accept: text/turtle' https://dbpedia.org/property/name
<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 2.0//EN">
<html><head>
<title>406 Not Acceptable</title>
</head><body>
<h1>406 Not Acceptable</h1>
<p>An appropriate representation of the requested resource name could not be found on this server.</p>
Available variant(s):
<ul>
<li><a href="name">name</a> , type text/html, charset UTF-8</li>
</ul>
</body></html>

------

ahora analicemos que pasa con una propiedad externa, ej:

http://www.georss.org/georss/point


pablo@xiaomi:/src/github/facu-infor/twss$ curl -I http://www.georss.org/georss/point
HTTP/1.1 200 OK
Server: openresty
Date: Mon, 31 May 2021 14:06:38 GMT
Content-Type: text/html; charset=UTF-8
Connection: keep-alive
X-Adblock-Key: MFwwDQYJKoZIhvcNAQEBBQADSwAwSAJBANDrp2lz7AOmADaN8tA50LsWcjLFyQFcb/P2Txc58oYOeILb3vBw7J6f4pamkAQVSQuqYsKx3YzdUHCvbVZvFUsCAwEAAQ==_I6k/NpDijLD71zgmVmqDBczFx9clwPdVGeqWdd7JgieT8eIERejv3Lk7tCORlQffHJ1kVBS18YI6RJmhm6+Dhw==

no hay redirección de nada. veo si pidiendo el content type turtle me responde distinto.

pablo@xiaomi:/src/github/facu-infor/twss$ curl -H 'accept:text/turtle' -I http://www.georss.org/georss/point
HTTP/1.1 200 OK
Server: openresty
Date: Mon, 31 May 2021 14:07:09 GMT
Content-Type: text/html; charset=UTF-8
Connection: keep-alive
X-Adblock-Key: MFwwDQYJKoZIhvcNAQEBBQADSwAwSAJBANDrp2lz7AOmADaN8tA50LsWcjLFyQFcb/P2Txc58oYOeILb3vBw7J6f4pamkAQVSQuqYsKx3YzdUHCvbVZvFUsCAwEAAQ==_I6k/NpDijLD71zgmVmqDBczFx9clwPdVGeqWdd7JgieT8eIERejv3Lk7tCORlQffHJ1kVBS18YI6RJmhm6+Dhw==

tampoco hay redirección.
analizo la respuesta de ese sitio.

pablo@xiaomi:/src/github/facu-infor/twss$ curl -H 'accept:text/turtle' http://www.georss.org/georss/point

es un html que en este caso no describe nada, y no tiene contenido asociado a la propiedad. es una página que indica que el dominio está en venta.
por lo que no sería una iri desreferenciable!.
el recurso no tiene presencia en la web.

---

otra propiedad externa:

http://www.w3.org/2000/01/rdf-schema#label


pablo@xiaomi:/src/github/facu-infor/twss$ curl -I http://www.w3.org/2000/01/rdf-schema#label
HTTP/1.1 200 OK
date: Mon, 31 May 2021 14:14:27 GMT
content-location: rdf-schema.ttl
vary: negotiate,accept,accept-charset,upgrade-insecure-requests
tcn: choice
access-control-allow-credentials: true
access-control-allow-methods: GET, HEAD, OPTIONS
access-control-allow-headers: Link, Location, Content-Type, Accept, Vary
access-control-expose-headers: Location, Link, Vary, Last-Modified, ETag, Allow, Content-Length, Accept
access-control-allow-origin: *
last-modified: Tue, 25 Feb 2014 02:53:20 GMT
etag: "ee4-4f33230d4a800;586929c6c2725"
accept-ranges: bytes
content-length: 3812
cache-control: max-age=21600
expires: Mon, 31 May 2021 20:14:27 GMT
access-control-allow-origin: *
content-type: text/turtle; charset=utf-8
x-backend: www-mirrors

pablo@xiaomi:/src/github/facu-infor/twss$ curl http://www.w3.org/2000/01/rdf-schema#label
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix owl: <http://www.w3.org/2002/07/owl#> .
@prefix dc: <http://purl.org/dc/elements/1.1/> .

<http://www.w3.org/2000/01/rdf-schema#> a owl:Ontology ;
	dc:title "The RDF Schema vocabulary (RDFS)" .

...
....

en este caso aunque no explicite el content type que requiero ya me tira el turtle.
o sea que este recurso es desreferenciable y tiene presencia en la web.

-----

otra propiedad, por ejemplo lo de foaf.

pablo@xiaomi:/src/github/facu-infor/twss$ curl http://xmlns.com/foaf/0.1/name
<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 2.0//EN">
<html><head>
<title>303 See Other</title>
</head><body>
<h1>See Other</h1>
<p>The answer to your request is located <a href="http://xmlns.com/foaf/spec/">here</a>.</p>
<hr>
<address>Apache/2.4.7 (Ubuntu) Server at xmlns.com Port 80</address>
</body></html>

pablo@xiaomi:/src/github/facu-infor/twss$ curl -I http://xmlns.com/foaf/0.1/name
HTTP/1.1 303 See Other
Date: Mon, 31 May 2021 14:08:19 GMT
Server: Apache/2.4.7 (Ubuntu)
Access-Control-Allow-Origin: *
Location: http://xmlns.com/foaf/spec/
Content-Type: text/html; charset=iso-8859-1

pablo@xiaomi:/src/github/facu-infor/twss$ curl -I http://xmlns.com/foaf/spec/
HTTP/1.1 200 OK
Date: Mon, 31 May 2021 14:08:32 GMT
Server: Apache/2.4.7 (Ubuntu)
Last-Modified: Tue, 14 Jan 2014 19:54:30 GMT
ETag: "3672a-4eff38f801066"
Accept-Ranges: bytes
Content-Length: 223018
Vary: Accept-Encoding
Content-Type: text/html

pablo@xiaomi:/src/github/facu-infor/twss$ curl http://xmlns.com/foaf/spec/
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head
xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
xmlns:foaf="http://xmlns.com/foaf/0.1/"
xmlns:vs="http://www.w3.org/2003/06/sw-vocab-status/ns#"
xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
xmlns:owl="http://www.w3.org/2002/07/owl#"

esto es un recurso html, la descripción para humanos.
por lo que veo en este caso la página de foaf DICE explícitamente que se puede obtener mediante content negociation usando la uri del namespace. http://xmlns.com/foaf/0.1/

pero no me lo restá resolviendo mediante content negociation

pablo@xiaomi:/src/github/facu-infor/twss$ curl -H 'accept:text/turtle' -I http://xmlns.com/foaf/0.1/
HTTP/1.1 200 OK
Date: Mon, 31 May 2021 14:42:16 GMT
Server: Apache/2.4.7 (Ubuntu)
Access-Control-Allow-Origin: *
Last-Modified: Sun, 23 Jan 2011 19:42:36 GMT
ETag: "2e12b-49a88af74aad6"
Accept-Ranges: bytes
Content-Length: 188715
Vary: Accept-Encoding
Content-Type: text/html

pablo@xiaomi:/src/github/facu-infor/twss$ curl -H 'accept:text/turtle' http://xmlns.com/foaf/0.1/

esto me retorna un html tal cual dice en el content-type.

lo consulté en el foro a ver si puedo asumir que eso igualmente tiene presencia en la web y es desreferenciable.

----

para le caso de los objetos, las iris que representan recursos pasa algo similar que con la de las propiedades.
recursos internos a dbpedia tienen representación para pc desreferenciable.
recrusos externos no siempre.

ej con el recurso interno a dbpedia 

pablo@xiaomi:/src/github/facu-infor/twss$ curl -I https://dbpedia.org/resource/Buenos_Aires_Province
HTTP/2 303 
date: Mon, 31 May 2021 15:08:54 GMT
content-type: text/html; charset=UTF-8
content-length: 0
server: Virtuoso/08.03.3319 (Linux) x86_64-centos_6-linux-glibc2.12  VDB
location: http://dbpedia.org/page/Buenos_Aires_Province
access-control-allow-credentials: true
access-control-allow-methods: HEAD, GET, POST, OPTIONS
access-control-allow-headers: Depth,DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Accept-Encoding

pablo@xiaomi:/src/github/facu-infor/twss$ curl -H 'accept:text/turtle' -I https://dbpedia.org/resource/Buenos_Aires_Province
HTTP/2 303 
date: Mon, 31 May 2021 15:09:18 GMT
content-type: text/turtle
content-length: 0
server: Virtuoso/08.03.3319 (Linux) x86_64-centos_6-linux-glibc2.12  VDB
tcn: choice
vary: negotiate,accept
alternates: {"/data/Buenos_Aires_Province.atom" 0.500000 {type application/atom+xml}}, {"/data/Buenos_Aires_Province.jrdf" 0.600000 {type application/rdf+json}}, {"/data/Buenos_Aires_Province.jsod" 0.500000 {type application/odata+json}}, {"/data/Buenos_Aires_Province.json" 0.600000 {type application/json}}, {"/data/Buenos_Aires_Province.jsonld" 0.500000 {type application/ld+json}}, {"/data/Buenos_Aires_Province.n3" 0.800000 {type text/n3}}, {"/data/Buenos_Aires_Province.nt" 0.800000 {type text/rdf+n3}}, {"/data/Buenos_Aires_Province.ntriples" 0.500000 {type application/n-triples}}, {"/data/Buenos_Aires_Province.ttl" 0.700000 {type text/turtle}}, {"/data/Buenos_Aires_Province.xml" 0.950000 {type application/rdf+xml}}
link: <http://creativecommons.org/licenses/by-sa/3.0/>;rel="license",<http://dbpedia.mementodepot.org/timegate/http://dbpedia.org/resource/Buenos_Aires_Province>; rel="timegate"
location: http://dbpedia.org/data/Buenos_Aires_Province.ttl
access-control-allow-credentials: true
access-control-allow-methods: HEAD, GET, POST, OPTIONS
access-control-allow-headers: Depth,DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Accept-Encoding

pablo@xiaomi:/src/github/facu-infor/twss$ curl -H 'accept:text/turtle' -I https://dbpedia.org/data/Buenos_Aires_Province.ttl
HTTP/2 200 
date: Mon, 31 May 2021 15:09:54 GMT
content-type: text/turtle; charset=UTF-8
content-length: 277174
server: Virtuoso/08.03.3319 (Linux) x86_64-centos_6-linux-glibc2.12  VDB
expires: Mon, 07 Jun 2021 15:09:54 GMT
link: <http://creativecommons.org/licenses/by-sa/3.0/>;rel="license",<http://dbpedia.org/data/Buenos_Aires_Province.xml>; rel="alternate"; type="application/rdf+xml"; title="Structured Descriptor Document (RDF/XML format)", <http://dbpedia.org/data/Buenos_Aires_Province.n3>; rel="alternate"; type="text/n3"; title="Structured Descriptor Document (N3/Turtle format)", <http://dbpedia.org/data/Buenos_Aires_Province.json>; rel="alternate"; type="application/json"; title="Structured Descriptor Document (RDF/JSON format)", <http://dbpedia.org/data/Buenos_Aires_Province.atom>; rel="alternate"; type="application/atom+xml"; title="OData (Atom+Feed format)", <http://dbpedia.org/data/Buenos_Aires_Province.jsod>; rel="alternate"; type="application/odata+json"; title="OData (JSON format)", <http://dbpedia.org/page/Buenos_Aires_Province>; rel="alternate"; type="text/html"; title="XHTML+RDFa", <http://dbpedia.org/resource/Buenos_Aires_Province>; rel="http://xmlns.com/foaf/0.1/primaryTopic", <http://dbpedia.org/resource/Buenos_Aires_Province>; rev="describedby", <http://dbpedia.mementodepot.org/timegate/http://dbpedia.org/data/Buenos_Aires_Province.ttl>; rel="timegate"
x-sparql-default-graph: http://dbpedia.org
content-disposition: filename=sparql_2021-05-31_15-09-54Z.ttl
cache-control: max-age=604800
access-control-allow-credentials: true
access-control-allow-methods: HEAD, GET, POST, OPTIONS
access-control-allow-headers: Depth,DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Accept-Encoding
accept-ranges: bytes

esto me retorna un turtle.

---

para un objeto externo a dbpedia.

pablo@xiaomi:/src/github/facu-infor/twss$ curl -I http://viaf.org/viaf/146084247
HTTP/1.1 303 See Other
Server: Apache-Coyote/1.1
Location: https://viaf.org/viaf/146084247/
Content-Type: text/plain
Content-Length: 43
Date: Mon, 31 May 2021 15:11:50 GMT

pablo@xiaomi:/src/github/facu-infor/twss$ curl -I https://viaf.org/viaf/146084247
HTTP/1.1 303 See Other
Server: Apache-Coyote/1.1
Location: https://viaf.org/viaf/146084247/
Content-Type: text/plain
Content-Length: 43
Date: Mon, 31 May 2021 15:11:56 GMT

pablo@xiaomi:/src/github/facu-infor/twss$ curl -I https://viaf.org/viaf/146084247/
HTTP/1.1 200 OK
Server: Apache-Coyote/1.1
srwRequestMethod: HEAD
Access-Control-Allow-Origin: *
Access-Control-Allow-Methods: GET
X-FRAME-OPTIONS: SAMEORIGIN
X-Content-Security-Policy: allow self
Content-Location: viaf.html
Cache-Control: max-age=604800
Expires: Mon, 07 Jun 2021 15:12:10 GMT
Content-Type: text/html;charset=ISO-8859-1
Content-Length: 0
Date: Mon, 31 May 2021 15:12:10 GMT

pablo@xiaomi:/src/github/facu-infor/twss$ curl -H 'accept: text/turtle' -I https://viaf.org/viaf/146084247/
HTTP/1.1 200 OK
Server: Apache-Coyote/1.1
srwRequestMethod: HEAD
Access-Control-Allow-Origin: *
Access-Control-Allow-Methods: GET
X-FRAME-OPTIONS: SAMEORIGIN
X-Content-Security-Policy: allow self
Cache-Control: max-age=604800
Expires: Mon, 07 Jun 2021 15:12:24 GMT
Content-Type: text/xml;charset=ISO-8859-1
Content-Length: 0
Date: Mon, 31 May 2021 15:12:23 GMT


pablo@xiaomi:/src/github/facu-infor/twss$ curl -H 'accept: text/turtle' https://viaf.org/viaf/146084247/

<?xml version="1.0"  encoding="UTF-8"?> 
<?xml-stylesheet type="text/xsl" href="/viaf/xsl/searchRetrieveResponse_en.xsl"?>
<ns1:VIAFCluster xmlns="http://viaf.org/viaf/terms#" xmlns:foaf="http://xmlns.com/foaf/0.1/" xmlns:owl="http://www.w3.org/2002/07/owl#" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:void="http://rdfs.org/ns/void#" xmlns:ns1="http://viaf.org/viaf/terms#"><ns1:viafID>146084247</ns1:viafID><ns1:Document about="http://viaf.org/viaf/146084247/"><ns1:inDataset resource="http://viaf.org/viaf/data"/><ns1:primaryTopic resource="http://viaf.org/viaf/146084247"/></ns1:Document><ns1:nameType>Corporate</ns1:nameType><ns1:sources><ns1:source nsid="A000189928">RERO|A000189928</ns1:source><ns1:source nsid="987007269238505171">J9U|987007269238505171</ns1:source><ns1:source nsid="n50078155">LC|n  50078155</ns1:source><ns1:source nsid="FRBNF123217697">BNF|12321769</ns1:source><ns1:source nsid="http://d-nb.info/gnd/35897-6">DNB|000358975</ns1:source><ns1:source nsid="0000000459043590">ISNI|000000

en este caso me retorna un xml. que puede ser interpertado por una máquina, pero no corresponde con el content-type requerido.
igualmente en este caso sería desreferenciable y tendría presencia.

----

en otro caos no se encuentra nada. no sería desreferenciable.

pablo@xiaomi:/src/github/facu-infor/twss$ curl -I http://rdf.freebase.com/ns/m.07_yzv
HTTP/1.1 404 Not Found
Date: Mon, 31 May 2021 15:40:14 GMT
Content-Type: text/html; charset=UTF-8
Server: ghs
Content-Length: 1572
X-XSS-Protection: 0
X-Frame-Options: SAMEORIGIN


----

Ejercicio 3: Realizar consultas Sparql en dbpedia
Para cada caso reporte la consulta sparql correspondiente y el resultado de la misma. En las consultas, de preferencia al uso de clases y propiedades en la ontología de dbpedia (dbo) 

a) Obtener a los escritores que hayan nacido en una ciudad de Argentina. 

inicio a consultar algunas cosas para ver que me trae.
analizo por ejemplo a borjes. https://dbpedia.org/page/Jorge_Luis_Borges

SELECT DISTINCT ?p ?o
			WHERE
			{
			dbr:Jorge_Luis_Borges ?p ?o .
			}



veo algunas cosas que puedo usar como filtro.

dbp:occupation == Writer
dbo:birthPlace = dbr:Argentina, dbr:Buenos_Aires
dbp:nationality = dbr:Argentines ---> pero pueden ser naturalizados, asi que esto no lo tendría en cuenta.
rdf:type = yago:*writer --> pero es de otro dataset asi que no lo tengo en cuenta.
rdf:type = dbo:Writer

dbo:author --> dbr:* --> rdf:type = dbo:Book, dbo:WrittenWork
dbo:writer --> dbr:* --> rdf:type = dbo:Work
dbp:author --> dbr:* --> rdf:type = dbo:WrittenWork
dbp:writer --> dbr:* --> rdf:type = dbo:WrittenWork

o sea por un lado debemos obtener las personas que sean escritores.

SELECT DISTINCT ?s ?p
			WHERE
			{
			?s ?p ?o.
                        ?o rdf:type dbo:WrittenWork
			}
esto tira autores de cosas escritas pero que pueden ser páginas de wikipedia, etc.
asi que no va, me parece que la consulta está orientado a obras literarias.

SELECT DISTINCT ?s ?p ?o ?c
			WHERE
			{
			?s ?p ?o .
?o rdf:type dbo:Book .
?s dbo:birthPlace dbr:Argentina .
			}            

lo puedo expandir con ciudades que pertenecen a argentina.

SELECT DISTINCT ?s ?p ?o ?c
			WHERE
			{
			?s ?p ?o .
?o rdf:type dbo:Book .
?s dbo:birthPlace ?c .
?c dbo:country dbr:Argentina .

			}


tengo que restringirlo a que sea autor de la obra si no me trae personas relacionadas igual que no son 
los autores.

SELECT DISTINCT ?s ?p ?o ?c
			WHERE
			{
			?s ?p ?o .
?o rdf:type dbo:Book .
?o dbo:author ?s .
?s dbo:birthPlace ?c .
?c dbo:country dbr:Argentina .

			}

desambiguo a los autores. y chequeo por pais ya que no en todos los casos tengo la ciudad en la que nacieron.

SELECT DISTINCT ?s 
			WHERE
{
    		?s ?p ?o .
    ?o rdf:type dbo:Book .
    ?o dbo:author ?s .
    { 
        ?s dbo:birthPlace ?c .
        ?c dbo:country dbr:Argentina .
    } UNION {
        ?s dbo:birthPlace dbr:Argentina
    }
}


me da la misma cantidad de autores, inclusive si consulto solo que hayan nacido en argentina, sin involucrar la ciudad.
pero no me gusta el resultado. son muy pocos.
asi que reformulo la consulta para analizar un poco otros resultados.

la mas trivial:

SELECT DISTINCT ?s
			WHERE
			{
    ?s dbo:birthPlace dbr:Argentina .
    ?s rdf:type dbo:Writer .
			}

esta me gusta mas ya que abarca a mayores resultados sin especificar la ciudad.
ej:
http://dbpedia.org/resource/Betina_Gonzalez
es escritora, que según la info nació en el AMBA.
Betina González (Villa Ballester, 1972) es una escritora argentina, primera mujer en ganar el Premio Tusquets de Novela.​ (es)
pero en la info de dbpedia solo se indica que nació en argentina.


---
b) Obtener a los escritores que hayan nacido en una ciudad de Uruguay.

es lo mismo, que haya nacido en Uruguay ya implica que nació en una ciudad de uruguay.

SELECT DISTINCT ?s
			WHERE
			{
                ?s dbo:birthPlace dbr:Uruguay .
                ?s rdf:type dbo:Writer .
			}

por lo que veo dbpedia infiere siempre cuando tiene la info de la ciudad. le pone también la info del país 
en birthPlace.


---

c) Utilizando el keyword filter (vea sección 6.3.2.6 del libro), obtener a los escritores que hayan nacido en 
una ciudad de Argentina o de Uruguay 

SELECT DISTINCT ?s
			WHERE
			{
    ?s dbo:birthPlace ?pais .
    ?s rdf:type dbo:Writer .
    filter (?pais = dbr:Argentina || ?pais = dbr:Uruguay) .
			}

--

d) Utilizando el keyword union (vea sección 6.3.2.6 del libro), obtener a los escritores que hayan nacido en una ciudad de Argentina o de Uruguay

SELECT DISTINCT ?s
			WHERE
			{
                ?s rdf:type dbo:Writer .
                { ?s dbo:birthPlace dbr:Argentina . }
                UNION 
                { ?s dbo:birthPlace dbr:Uruguay .}
			}


--

e) ¿Qué diferencia hay entre c y d? ¿En cual se deben recuperar/analizar menor número de tripletas?

asumo que la union analiza mayor número de tripletas.
si pensamos que  para obtener la union de los conjuntos hay que procesar los 2 conjuntos primero.
en pseudocódigo:

c1 = for autor in autores if autor dbo:birthPlace dbr:Argentina
c2 = for autor in autores if autor dbo:birthPlace dbr:Uruguay
c3 = c1 + c2

entonces se procesaría el dataset de autores 2 veces. (la variable ?s)
para el caso del filter el if va adentro del "for"

c3 = for autor in autores if autor dbo:birthPlace dbr:Argentina or autor dbo:birthPlace dbr:Uruguay

en este caso se recorre solamente 1 vez el dataset de autores.
pero esto es una suposición. tengo que buscar algún motor que me permita explorar como realiza la consulta el mismo motor y que optimizaciones hace.

---


f) ¿Cuantos empleados tiene la compañía mas grande en dbpedia, y en que país está ubicada? 
(obtenga la lista de todas las compañías y los países donde están ubicadas ordenada de forma descendiente 
por numero de empleados)


primero analizo como se obtienen las comañías. asumo que es rdf:type dbo:Company
efectivamente:

rdfs:label	
        company (en)
        Unternehmen (de)
        empresa (es)
        empresa (pt)

asi que filtro primero por eso:

SELECT DISTINCT ?s
			WHERE
			{
?s rdf:type dbo:Company .
}

agrego la ubicación

SELECT DISTINCT ?s ?l
			WHERE
			{
?s rdf:type dbo:Company .
?s dbo:location ?l .

			}


analizo propieades en dbpedia y ver:
http://dbpedia.org/ontology/numberOfEmployees

asi que formulo nuevamente la consulta:

SELECT DISTINCT ?s ?l ?n
			WHERE
			{
                ?s rdf:type dbo:Company .
                ?s dbo:location ?l .
                OPTIONAL
                {
                ?s dbo:numberOfEmployees ?n .
                }
			}

veo que esta consulta me trae distintas cantidades de empleados para distintas ubicaciones de la misma empresa.
asi que tengo que agregarlas.
analizando el capítulo del libro que habla de agregaciones (6.3.2.1) - Segunda Edición!.

SELECT DISTINCT ?s SUM(?n) as ?empleados
			WHERE
			{
                ?s rdf:type dbo:Company .
                ?s dbo:location ?l .
                OPTIONAL
                {
                ?s dbo:numberOfEmployees ?n
                }
			}
            GROUP BY ?s


ahora trato de ordenar esa lista.

SELECT DISTINCT ?s SUM(?n) as ?empleados
			WHERE
			{
                ?s rdf:type dbo:Company .
                ?s dbo:location ?l .
                OPTIONAL
                {
                ?s dbo:numberOfEmployees ?n
                }
			}
            GROUP BY ?s
            order by desc(?empleados)

funcionó.
me ordenó en forma descendente la cantidad de empleados.
pero veo algo raro. me da: https://dbpedia.org/page/Eureka_Forbes 
como la mayor empresa.
me imaginaba google, amazon, etc.
analizando un poco veo que eureka forbes tiene dbo:location
dbr:Maharashtra
dbr:India
dbr:Mumbai

a ver si me está agregando esas 3 ubicaciones?. porque mumbai -> Maharashtra -> India
o sea es una ciudad de un estado de un pais. no se debería estar agregando.

ejecuto la consulta usando :

SELECT DISTINCT ?s ?l ?n
			WHERE
			{
                ?s rdf:type dbo:Company .
                ?s dbo:location ?l .
                OPTIONAL
                {
                ?s dbo:numberOfEmployees ?n
                }
			}
            order by desc(?n)

y veo que efectivamente me muestra 3 veces eureka.
y cada una con 5000000 empleados.
si corro la consulta agregada entonces me muestra 15000000 empleados.
efectivamente estoy agregando algo que no debería.

el patrón se repite para todas las empresas. así que elimino la agregación de los empleados ya que no es necesaria.
y agrego que las ubicaciones sean paises.

SELECT DISTINCT ?s ?l ?n
			WHERE
			{
                ?s rdf:type dbo:Company .
                ?s dbo:location ?l .
                ?l rdf:type dbo:Country .
                OPTIONAL
                {
                ?s dbo:numberOfEmployees ?n
                }
			}
            order by desc(?n)


lo opcional es para obtener tambien las empresas que no tiene dbpedia registrado ningún empleado.
que estén al final del listado.

-------------------
-------------------
-------------------
-----

arranco con la parte de wikidata.
https://www.wikidata.org/wiki/Wikidata:Introduction


---

después de leer miles de artículos no encuentro bien donde están definidos los aspectos técnicos de wikidata y el manejo interno.
como los artículos que existen para dbpedia, no veo nada similar para wikidata.

en la documentación encuentro que existen bots que generan información dentro de la wikidata.
y editores de wikidata. pero no la conversión automática de wikipedia --> wikidata.

solo que fue un proyecto creado inicialmente para mantener de forma estructurada los links entre las wikis multilenguaje, 
así que cada vez que se crea un artículo en wikipedia se crea el correspondiente Q.
y que fue creciendo para darle una base estructurada a todos los proyectos wikimedia.

----

me creo una cuenta dentro de wikidata para analizar como es la creación del contenido.
tiene una estructura distinta de la wikipedia y tiene mucha documentación asociada.
mediante los asistentes no veo simple lo que necesito.

----

uso la misma cuenta para acceder a la wikipedia y tratar de ingresar algún artículo ahi.
no veo forma de incluir información a wikidata a partir de wikipedia.
los editores de wiki me permiten generar una página, insertar referencias, enlaces, 
imágnenes, etc. pero no veo en el editor gráfico info sobre wikidata.
explícitamente que se transforma de esa info en wikidata.

---

de todo lo que estuve leyendo asumo que todo lo que es infobox sale de wikidata.
y esa es la forma de incluir esos datos dentro de los artículos de wikipedia.
pruebo editar la página de la universidad y veo :

{{short description|Public research university in Argentina}}
{{Coord|34|54|46.69|S|57|57|04.88|W|type:edu|display=title}}
{{Infobox university
| name              = La Plata National University
| native_name       = ''Universidad Nacional de La Plata''
| image             = UNLP Logo.jpg
| image_size        = 180px
| image_alt         = 
| caption           = Seal of the National University of La Plata
| latin_name        = 
| motto             = {{lang|la|Pro scientia et patria}} ([[Latin]])
| motto_lang        = 
| mottoeng          = For science and the fatherland
.....
....
...

lo que no me define es si eso sale de la wikidata o esta editado dentro de wikipedia.
hubiera esperado algo como:

{{Infobox university qid=Q784171}}

de ahi podría haber inferido que la info está cargada en wikidata y se exporta a wikipedia.
pero con lo que veo así como está no puedo inferir nada. a lo sumo lo contrario.
que se carga mediante un template de infobox de valores dentro de wikipedia y ahi le asigno valores (basicamente la sintaxis de arriba)
y eso se inserta automáticamente en el Q784171 correspondiente generado para la página. o sea se generan las tripletas correspondientes y eso
va a parar a wikidata. 
ahora si este es el proceso entonces no me cuadra que cada sentencia en wikidata tiene una referencia, es requerido por wikidata por lo que averigué.
para indicar que una sentencia es verificada.

voy a seguir investigando. para mi me suena que es a la inversa: wikidata ---> wikipedia
es lo lógico.

---

hago algunas consutlas sparql para analizar un poco la cosa. a ver si puedo identificar algo.

select distinct ?s ?p
  where {
     ?s ?p wd:Q784171
  } limit 10000

pero todo el tema de las iris con Q hacen dificil entenderlo.
la idea es analizar también a la vez si puedo identificar mas del proceso 

wikipedia ---> wikidata

y también la parte identificación del Qnumber a partir de la url de la wiki. una de las preguntas que tenemos que responder.

primer idea es hacer una consulta a wikidata sparql usando como parámetro la url de la wiki.
en algún lado de wikidata tiene que haber alguna tripleta :

Q784171 wikiurl National_University_of_La_Plata

estoy pensando en el concepto de wikilink o intrawiki link etc.
eso seguro es la propiedad que estoy buscando.

--

Wikilink (Q58494516)
hyperlink between pages of the same Wikimedia wiki

----

select distinct ?o
  where {
     wd:Q784171 wd:Q58494516 ?o
  } limit 100


nada de nada. no me respondió ningún resultado.

---

viendo la consulta, me acabo de dar cuenta que puse a la inversa en la consulta general de arriba!!.
el sujeto es la universidad y el objeto tiene que ser la url de la wiki.
pruebo de nuevo a ver que veo!!.

select distinct ?p ?o
  where {
     wd:Q784171 ?p ?o
  } limit 10000


JOOOOOOOOOOOOOOOO. datos, queridos datos que entiendo un poco mas. no solo statements como la consulta anterior.
claro anteriormente lo que estaba consultando es que statements tienen a la univerisad como objeto.
por el esquema de datos de wikidata van a ser solo sentencias.

--

referencia:
https://www.mediawiki.org/wiki/Wikibase/Indexing/RDF_Dump_Format#Full_list_of_prefixes

 PREFIX wdt: <http://www.wikidata.org/prop/direct/>
 PREFIX wd: <http://www.wikidata.org/entity/>
 PREFIX wdtn: <http://www.wikidata.org/prop/direct-normalized/>
 PREFIX p: <http://www.wikidata.org/prop/>
 PREFIX wds: <http://www.wikidata.org/entity/statement/>

.----

buscando en google.

How do I link wikidata to Wikipedia?
From a Wikipedia page, you can go to the link "Wikidata item", using "Tools" in the side pane (in the left), to see and edit it. Also in Tools, there is another link to "page information", where is "Wikidata item ID", that contains the QID (for example: Q171 or "None").

pero siempre hay info no técnica. quiero saber la propiedad que linkea la wiki con la entidad Q de wikidata.
obvio sin leerme todo la info de wikidata jajaj.

---

si una página en wikipedia si o si tiene si Qnumber en wikidata.
para ver cual es la propiedad que los asocia entonces se me ocurrió
listas todas las propieades ordenadas en forma ascendente.

la wiki mas chica va a tener la menos cantidad de esas propiedades, pero va a tener seguro la propiedad que asocia la pagina en wikipedia con la entidad de wikidata.

select distinct SUM(?p) as ?propieades
  where {
     ?s ?p ?o
  } order by asc(?propieades) .
 limit 10 .

tiro error por todas partes. acomodo la sintaxis

select (SUM(?p) as ?propieades)
  where {
     ?s ?p ?o
  } order by asc(?propieades)
   limit 10

obvio como no podía ser de otra forma. tomar todas las tripletas tardaría años!!.

SPARQL-QUERY: queryStr=select (SUM(?p) as ?propieades)
  where {
     ?s ?p ?o
  } order by asc(?propieades)
   limit 10

java.util.concurrent.TimeoutException
	at java.util.concurrent.FutureTask.get(FutureTask.java:205)
	at com.bigdata.rdf.sail.webapp.BigdataServlet.submitApiTask(BigdataServlet.java:292)
	at com.bigdata.rdf.sail.webapp.QueryServlet.doSparqlQuery(QueryServlet.java:678)
	at com.bigdata.rdf.sail.webapp.QueryServlet.doGet(QueryServlet.java:290)
	at com.bigdata.rdf.sail.webapp.RESTServlet.doGet(RESTServlet.java:240)
	at com.bigdata.rdf.sail.webapp.MultiTenancyServlet.doGet(MultiTenancyServlet.java:273)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1655)
	at org.wikidata.query.rdf.blazegraph.throttling.ThrottlingFi


---

de nuevo en google.

Sitelinks (also known as interwiki links or interlanguage links) are special links that contain a site and a title, and go from individual items in Wikidata to pages on other Wikimedia sites such as Wikipedia, Wikisource and Wikivoyage.

https://www.wikidata.org/wiki/Help:Sitelinks

como indica el artículo el punto central para mi tiene que ser el Q entity de wikidata. de la entidad de la universidad.


----


analizando los ejemplos que tira el sparql de wikidata veo uno que me puede servir:

#Returns a list of Wikidata items for a given list of Wikipedia article names
#List of Wikipedia article names (lemma) is like "WIKIPEDIA ARTICLE NAME"@LANGUAGE CODE with de for German, en for English, etc.
#Language version and project is defined in schema:isPartOF with de.wikipedia.org for German Wikipedia, es.wikivoyage for Spanish Wikivoyage, etc.

SELECT ?lemma ?item WHERE {
  VALUES ?lemma {
    "Wikipedia"@de
    "Wikidata"@de
    "Berlin"@de
    "Technische Universität Berlin"@de
  }
  ?sitelink schema:about ?item;
    schema:isPartOf <https://de.wikipedia.org/>;
    schema:name ?lemma.
}

por lo que veo usa schema:name para el nombre del artículo en la wikipedia!!!!!!!
joooooooooooooooooooooooo y yo buscando una propiedad Pnnnnn
ahora si que me terminé de marear.
los prefijos (https://www.mediawiki.org/wiki/Wikibase/Indexing/RDF_Dump_Format#Full_list_of_prefixes) indican:

PREFIX schema: <http://schema.org/>

whhaattt!!! wikidata usa schmema.org/name para asociar a la entidad wikidata con la página wikipedia??????????????

me parece que estoy entendiendo mal.
lo pruebo:

select ?s where {
    ?s schema:name "National_University_of_La_Plata"@en .
  }

nadaaa. obvio.

select ?s where {
    ?s schema:name "National University of La Plata"@en .
  }

yes. me trae el subject!!!.

<https://en.wikipedia.org/wiki/National_University_of_La_Plata>

ahora si hago lo contrario.


select ?s ?p where {
    ?s ?p <https://en.wikipedia.org/wiki/National_University_of_La_Plata>
  }

no me había avivado de usarlo como objeto asi directamente con <> como si fuera una iri!!
nada no me trajo nada
para ver si es tema de sintaxis me fijo la inversa, debería traer aunque sea la tripleta schema:name .... anterior.

select ?s ?p where {
    <https://en.wikipedia.org/wiki/National_University_of_La_Plata> ?p ?s
  }

resultado!!!

wd:Q784171	schema:about
en	schema:inLanguage
schema:Article	rdf:type
National University of La Plata	schema:name
<https://en.wikipedia.org/>	schema:isPartOf


ahora entonces veo que la propiedad que asocia el Qnumber con la iri de la universidad es schema:about

select ?s where {
      ?s schema:about wd:Q784171
  }


bingooooo!!!

<https://ar.wikipedia.org/wiki/%D8%AC%D8%A7%D9%85%D8%B9%D8%A9_%D9%84%D8%A7_%D8%A8%D9%84%D8%A7%D8%AA%D8%A7_%D8%A7%D9%84%D9%88%D8%B7%D9%86%D9%8A%D8%A9>
<https://arz.wikipedia.org/wiki/%D8%AC%D8%A7%D9%85%D8%B9%D8%A9_%D9%84%D8%A7%D8%A8%D9%84%D8%A7%D8%AA%D8%A7_%D8%A7%D9%84%D9%88%D8%B7%D9%86%D9%8A%D9%87>
<https://az.wikipedia.org/wiki/La_Plata_Milli_Universiteti>
<https://de.wikipedia.org/wiki/Universidad_Nacional_de_La_Plata>
<https://en.wikipedia.org/wiki/National_University_of_La_Plata>
<https://eo.wikipedia.org/wiki/Nacia_Universitato_La_Plata>
<https://es.wikinews.org/wiki/Categor%C3%AD


en resumen.
despues de tanto hacer pruebas y marearme la propiedad que representa los sitelinks es schema:about
genera la estrella que muestra en el gráfico de sitelinks.
en realidad es la inversa a la propiedad que buscaba, pero la que me resuelve mas directo lo que quiero hacer

la pregunta del trabajo es: 


5. ¿Dado el articulo en Wikipedia de "National University of La Plata", como infiero la URL del recurso correspondiente en Wikidata? 

el recurso correspondiente podría inferirlo haciendo la consulta sparql siguiente:

url = url del articulo de wikipedia.
select ?q where {
   <url> schema:about ?q 
}

en particular en nuestro caso tendríamos.

select ?p where {
      <https://es.wikipedia.org/wiki/Universidad_Nacional_de_La_Plata> schema:about ?p
  }

jooooo tengo aunque sea un resultado!!!!, me siento mucho mas realizado, jajaja mas que decir. anda a la interface que te da la wikipedia y ahi te da los datos. 

TOTALMENTE loco!! pense que la propiedad, que yo identifico como principal, en la que se basa toda la creación de la wikidata iba a ser de un prefijo interno a wikidata y no un prefijo como schema.org!!!.

---

espero que todo el análisis esté bien. voy a arrancar a hacer las consultas y lo voy a reever a todo con algunos días de que me quede en la cabeza.
a ver si identifico errores.

----


6. ¿Que diferencias y similitudes encuentra con DBpedia?

tirando ideas de lo que se me ocurre ahora con la info analizada.

primero dbpedia tiene una ontología creada para representar mas conceptos del mundo real.
como si definieramos la ontología pensando en el mundo y no en como está estructurado internamente esa información a nivel de sistema.
o sea que en las transfromaciones de wikipedia ---> dbpedia  se pierden conceptos internos de como está estructurado. 

wikidata tiene una ontología creada para representar conceptos internos de los proyectos wikimedia.
todo el tema de las sentencias, que tienen referencias para evaluar si está verificado o no una afirmación, ayuda a tener datos del mundo real y representar esa info pero lo veo mas que lo diseñaron
con el objetivo de poder saber internamente que tripletas estarían mas verificadas que otras. Es mas, lo ponen como una restricción en la carga de las sentencias dentro de wikidata.
la estructura de links con nodo central, que linkea todas las wikipedias de los distintos lenguajes a la entidad de wikidata, también es un ejemplo de una estructura que mas que 
representar conocimiento, soluciona un problema real interno de links entre las distintas wikis. (mas pensado en la estructura del grafo) 
son mas aspectos pensados para estructurar la información, mas que pensado para ser entendido por humanos directamente.

igual entro en conflicto un poco cuando pienso en que algo de eso tiene dbpedia, ya que por ejemplo
usa distintos prefijos para representar información diseñada (dbo) e información obtenida mediante los extractores sin tener un mapeo definido (dbp)
igualmente dbpedia está represendando conceptos mas del mundo real, una "seguridad" de la veracidad de esa información. mas que un concepto de como estructurar la información internamente.
o sea en este caso el grafo es el mismo.

algo ---> dbo:Pesona ---> persona
          dbp:Person ---> persona

la estructura del grafo no cambia. solo que dbo y dbp permiten asociar esa "seguridad" que le damos al dato.
en cambio en la wikidata si.

wiki_es ----> sameAs ----> wiki_en
wiki_en ----> sameAs ----> wiki_de
wiki_es ----> sameAs ----> wiki_de
....

no es la misma estructura que inventar una entidad central (la Qnumber de wikidata) para linkearlas a todas:

entidad ---> sameAs ----> wiki_en
entidad ---> sameAs ----> wiki_es
entidad ---> sameAs ----> wiki_de


encuentro analogías locas que se me ocurren de otras materias que estoy cursando, 
por ejemplo wikidata me hace acordar mas a el metamodelo de UML (MOF) sirve para estructurar y definir exactamente el modelo UML y como modelar.
y dbpedia lo veo mas como el modelo en UML en si. 
uno es mas abstracto y el otro se acerca mas al mundo real (en este caso el dominio del problema).


---

otra diferencia que se me ocurre es en el proceso de modelado de la ontología.
wikidata dijeron hagamos algo que se pueda adaptar a todas las situaciones sin tener que cambiar la ontología.
justamente la definición que para cada propiedad y entidad de la ontología le asignen un Pnúmero y Qnúmero, ya te indica que es un concepto abstracto.
Q111 será perro
Q122 será gato
Q113 será clase de animal domesticable
....
con esto sea el concepto que tengan que representar, saben que usando esa ontología y vocabulario lo van a poder hacer. no tienen que pensar en cada concepto que quieren representar apriori.

en cambio en dbpedia los conceptos son parte del mundo.
ej: dbo:birthPlace, dbo:author, etc son exactamente conceptos del mundo real, con dominio y rangos mas definidos!!.
un autor a priori fue diseñado para ser una persona y aplicarse a cosas que puedan tener autoría.
para representar un nuevo concepto no contemplado dentro de la ontología debemos modificar si o si la ontología, extenderla, etc.

claramente los Q y los P son conceptos mas abstractos que dbo:autor y dbo:birthPlace. sin modificar la ontología se pueden extender.
solo hace falta de "crear una instancia" de un P y asignarle un número fijo y listo. eso pasa a ser un concepto adicional antes no representado. la clave aca es NO modificar la definición de la ontología.
pasaría a ser una especie de invidividual, que su iri puede representar a su vez un concepto que relaciona otros individuals, o los clasifica, etc.
no se si lo estoy poniendo bien en palabras, pero básicamente todo se resume a, uno es mas abstracto y el otro es mas concreto.

----

la forma mas clara que lo puedo expresar es.
dbpedia se concentra en representar conocimiento
wikidata se concentra además de representar conocimiento, en la forma de representarlo.

--

esto igual lo pense sin siquiera ver dominio y rangos, ni la ontología en croncreto de wikidata. todo lo que me encontré es info sobre como ir generando visualmente los datos dentro de wikipedia y wikidata.
me está volviendo loco, busco y busco y no puedo encontrar en ningún lado un turtle de la ontología de wikidata. jajaa.
todo es páginas y páginas de texto escrito y links entre si.
como lo de schema.org que en vez de tener una ontología clara escrita en máquina, tenemos páginas que explican lo que significan esos conceptos, la estructura, etc pero todo para humanos.

--

por ahi este análisis es totalmente erróneo y cambie con el tiempo a medida que vaya invesigando mas de wikidata, pero es la primer sensación que me da cuando leí la documentación de wikipedia y los artículos de dbpedia.
y el proceso que necesité para entender las estructuras/ontologías de cada uno de los proyectos. claramente apurado leyendo lo menos posible para entender un concepto debido a los tiempos.
voy a hacer las consultas a ver si me queda mas claro como maneja todo wikidata. así que por ahora solo es una anotación.

----

croe que encontré oro!!!.
https://homepages.dcc.ufmg.br/~laender/material/gdw2015-EGK+2014.pdf

In spite of all this, Wikidata has hardly been used in the semantic web community
so far. The simple reason is that, until recently, the data was not available in RDF. To
change this, we have developed RDF encodings for Wikidata, implemented a tool for
creating file exports, and set up a site where the results are published. Regular RDF
dumps of the data can now be found at http://tools.wmflabs.org/wikidata-exports/rdf/. This
paper describes the design underlying these exports and introduces the new datasets:
– In Section 2, we introduce the data model of Wikidata, which governs the structure
of the content that we want to export.


lo bajo como material y lo leo.


-----

ME QUIERO MATAR!!!.

Site links are exported using the schema.org property about to associate a Wikipedia
page URL with its Wikidata item, and the schema.org property inLanguage to define the
BCP 47 language code of a Wikipedia page. Table 2 (bottom) gives an example

aca estaba!!!. leyendo material y buscando por google podría haberme evitado bastantes pruebas!!.

---


entendiendo un poco mas encuentro de forma simple lo que me va a ayudar un poco mas.

https://www.wikidata.org/wiki/Wikidata:RDF
https://www.wikidata.org/wiki/Wikidata:Database_download#RDF_dumps
https://www.mediawiki.org/wiki/Wikibase/Indexing/RDF_Dump_Format
https://dumps.wikimedia.org/wikidatawiki/entities/ <----- al fin!!!

mmm aunque ahora que lo analizo solo están los lexemas. mmmm lexemasss??
y el completo pesa 80GB!!! 


ahora pensando en el tema.
en el pdf que baje ultimo me dicen que las urls de wikidata soportan content negociation.
y de estos lexemas tengo los prefijos. lo que concluyo que un rdf de la ontología debería poder obtenerlo de los prefijos.
como hicimos en nuestros ejemplos de los tps.
ej:

@prefix wd: <http://www.wikidata.org/entity/> .

debería poder consultar http://www.wikidata.org/entity/
aceptando y text/turtle y mediante todo lo que explicó casco en la teoría tendría que llegar a algo que 
se asemeje a una ontología para ese prefijo. no? 
pruebo.

pablo@xiaomi:/src/github/facu-infor/twss/entrega5/material$ curl -I http://www.wikidata.org/entity/
HTTP/1.1 301 TLS Redirect
Date: Thu, 03 Jun 2021 16:57:37 GMT
Server: Varnish
X-Varnish: 1073661091
X-Cache: cp1079 int
X-Cache-Status: int-front
Server-Timing: cache;desc="int-front", host;desc="cp1079"
Permissions-Policy: interest-cohort=()
Set-Cookie: WMF-Last-Access=03-Jun-2021;Path=/;HttpOnly;secure;Expires=Mon, 05 Jul 2021 12:00:00 GMT
Set-Cookie: WMF-Last-Access-Global=03-Jun-2021;Path=/;Domain=.wikidata.org;HttpOnly;secure;Expires=Mon, 05 Jul 2021 12:00:00 GMT
X-Client-IP: 191.83.209.88
Location: https://www.wikidata.org/entity/
Content-Length: 0
Connection: keep-alive

pablo@xiaomi:/src/github/facu-infor/twss/entrega5/material$ curl -H 'accept:text/turtle' -I http://www.wikidata.org/entity/
HTTP/1.1 301 TLS Redirect
Date: Thu, 03 Jun 2021 16:57:53 GMT
Server: Varnish
X-Varnish: 1049914519
X-Cache: cp1079 int
X-Cache-Status: int-front
Server-Timing: cache;desc="int-front", host;desc="cp1079"
Permissions-Policy: interest-cohort=()
Set-Cookie: WMF-Last-Access=03-Jun-2021;Path=/;HttpOnly;secure;Expires=Mon, 05 Jul 2021 12:00:00 GMT
Set-Cookie: WMF-Last-Access-Global=03-Jun-2021;Path=/;Domain=.wikidata.org;HttpOnly;secure;Expires=Mon, 05 Jul 2021 12:00:00 GMT
X-Client-IP: 191.83.209.88
Location: https://www.wikidata.org/entity/
Content-Length: 0
Connection: keep-alive

pablo@xiaomi:/src/github/facu-infor/twss$ curl -H 'accept:text/turtle' -I https://www.wikidata.org/entity/
HTTP/2 303 
date: Thu, 03 Jun 2021 17:04:56 GMT
server: mw1267.eqiad.wmnet
location: https://www.wikidata.org/wiki/Special:EntityData/
content-length: 256
content-type: text/html; charset=iso-8859-1
age: 0
x-cache: cp1081 miss, cp1083 pass
x-cache-status: pass
server-timing: cache;desc="pass", host;desc="cp1083"
strict-transport-security: max-age=106384710; includeSubDomains; preload
report-to: { "group": "wm_nel", "max_age": 86400, "endpoints": [{ "url": "https://intake-logging.wikimedia.org/v1/events?stream=w3c.reportingapi.network_error&schema_uri=/w3c/reportingapi/network_error/1.0.0" }] }
nel: { "report_to": "wm_nel", "max_age": 86400, "failure_fraction": 0.05, "success_fraction": 0.0}
permissions-policy: interest-cohort=()
set-cookie: WMF-Last-Access=03-Jun-2021;Path=/;HttpOnly;secure;Expires=Mon, 05 Jul 2021 12:00:00 GMT
set-cookie: WMF-Last-Access-Global=03-Jun-2021;Path=/;Domain=.wikidata.org;HttpOnly;secure;Expires=Mon, 05 Jul 2021 12:00:00 GMT
x-client-ip: 191.83.209.88
set-cookie: GeoIP=AR:B:La_Plata:-34.93:-57.95:v4; Path=/; secure; Domain=.wikidata.org

pablo@xiaomi:/src/github/facu-infor/twss$ curl -H 'accept:text/turtle' -I https://www.wikidata.org/wiki/Special:EntityData/
HTTP/2 200 
date: Thu, 03 Jun 2021 17:05:18 GMT
server: mw1320.eqiad.wmnet
x-content-type-options: nosniff
p3p: CP="See https://www.wikidata.org/wiki/Special:CentralAutoLogin/P3P for more info."
content-language: en
x-frame-options: DENY
vary: Accept-Encoding,Cookie,Authorization
expires: Thu, 01 Jan 1970 00:00:00 GMT
pragma: no-cache
content-type: text/html; charset=UTF-8
age: 0
x-cache: cp1089 miss, cp1083 pass
x-cache-status: pass
server-timing: cache;desc="pass", host;desc="cp1083"
strict-transport-security: max-age=106384710; includeSubDomains; preload
report-to: { "group": "wm_nel", "max_age": 86400, "endpoints": [{ "url": "https://intake-logging.wikimedia.org/v1/events?stream=w3c.reportingapi.network_error&schema_uri=/w3c/reportingapi/network_error/1.0.0" }] }
nel: { "report_to": "wm_nel", "max_age": 86400, "failure_fraction": 0.05, "success_fraction": 0.0}
permissions-policy: interest-cohort=()
set-cookie: WMF-Last-Access=03-Jun-2021;Path=/;HttpOnly;secure;Expires=Mon, 05 Jul 2021 12:00:00 GMT
set-cookie: WMF-Last-Access-Global=03-Jun-2021;Path=/;Domain=.wikidata.org;HttpOnly;secure;Expires=Mon, 05 Jul 2021 12:00:00 GMT
x-client-ip: 191.83.209.88
cache-control: private, s-maxage=0, max-age=0, must-revalidate
set-cookie: GeoIP=AR:B:La_Plata:-34.93:-57.95:v4; Path=/; secure; Domain=.wikidata.org
accept-ranges: bytes


como me retorna 200 entonces me fijo si ese contentido me retorna algo parecido a un turtle.
no no.
y chequeo lo que explicó casco en el foro, por ahí a veces está incluído en el html mismo un link especificando el contenido en rdf, como en foaf.
y veo que si. pero a una entidad en particular!!! no a la ontología de ese prefijo!!

https://www.wikidata.org/wiki/Special:EntityData/

contenido: 

Entity Data
Jump to navigationJump to search
This page provides a linked data interface to entity values. Please provide the entity ID in the URL, using subpage syntax.

Content negotiation applies based on you client's Accept header. This means that the entity data will be provided in the format preferred by your client. For a web browser, this will be HTML, causing your browser to be redirected to the regular entity page.
You can explicitly request a specific data format by adding the appropriate file extension to the entity ID: Q23.json will return data in the JSON format, Q23.ttl will return RDF/Turtle, and so on. Supported formats are: json, php, n3, ttl, nt, rdf, jsonld, html.
For more information about this special page, read Wikibase/EntityData.

Return to Wikidata:Main Page.

----

es como el huevo y la gallina. no quiero una entidad, quiero la ontología que define como definimos la entidad!!.
la class!!! el rdf:type.
jajaj 

un poco esto es a lo que me refería cuando pensaba de las diferencias entre dbpedia y wikidata.
wikidata es demasiado abstracto.
ya veo que investigando mas encuentro que la ontología de wikidata es una entidad en si!!! jajajja 
mi cabeza se siente como cuando vió la película inceptión la primera vez.

----









--- 
nota p143 es importado de wikimedia.
