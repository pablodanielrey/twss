inicio el análisis del tp.
no me pude conectar a la teoría por problemas en el laburo pero ya vi todos los videos.

super super super interesante el tema de la inferencia y sparql como lenguaje de consulta.
sparql y el motor de inferencia era lo que faltaba para poder realizar pruebas simples y evaluar los efectos de esas alternativas de implementación sobre una web semántica en general.

por ejemplo algunas de mis dudas principales todavía son:
1 - veneficios/desventajas de armar una ontología completa independiente por sobre reusar conceptos y hacer merge de ontologías existentes.
2 - efectos de la inferencia sobre el grafo mergeado de varias ontologías, por ejemplo cambiar el rango de una propiedad para abarcar mas clases, y analizar esos efectos en todo el grafo mergeado.
3 - ventajas y desventajas de particionar nuestra ontología en dominios especificos a una solución. (ej: como lo que vimos en el tp1 y tp2. al mergear la info podríamos haber estructurado de distinta forma las ontologías que creamos) 
por ejemplo: 
ontología tp1 - solo con los conceptos manejados en tp1
ontologia tp2 - solo con los conceptos manejados en tp2
ontología tp3 = merge entre las 2 ontologías anteriores y usar sameAs, equivalentProperty, equivalentClass, etc para linkear conceptos repetidos de las ontologías anteriores.

otra posible solución
ontologia tp1
extensión ontologia tp1 para abarcar conceptos de tp2
extensión ontología tp2 para abrcar conceptos tp3

también, efectos de :
armar en mismo archivo, modificandolo y llevando distintas versiones del mismo, cada una con cambios incrementales? 
o armarlo en distintos archivos y usar owl:import a cada extensión de la ontología?
etc.

usando sparql e inferencia podemos ver los resultados que tiene una solución o la otra. si generan efectos no deseados usando inferencia, etc.
usando protegé no me quedaron claras estas cosas, claramente porque no lo se usar correctamente y me falta práctica.

---

también permitiría analizar un poco patrones de diseño de ontologías sin necesidad de desarrollar con python.
parecido a los patrones de diseño del paradigma orientado a objetos.
casco publicó un link a material sobre eso en el foro.
https://patterns.dataincubator.org/book/
que profundidad de ontología usar? usar subclases o usar composición?, etc.
y ver rápidamente los resultados de esos patrones de diseño. si general algún efecto no deseado. etc.

---


material publicado por leonel sobre wikidata y dbpedia.

https://recyt.fecyt.es/index.php/ThinkEPI/article/view/thinkepi.2018.31

---

ALUCINANTE el artículo!!. lo dejo como material dentro del tp.
y casi que responde a las consultas básicas del tp y aclara las diferencias entre los 2 proyectos!.

el artículo que pasó diego en el foro para el trabajo esta bueno da un panorama muchisimo mas completo de lo que es dbpedia.

----

después de leer todo lo de dbpedia me quedó otro sabor sobre lo que pensaba de dbpedia.
me gusta mucho que haya sido pensado como una ontología universal. por lo que entendí, si bien se centra en el procesamiento de los datos de wikipedia, la clave fue realizar los mapeos 
de los datos de wikipedia con una ontología pensada y estrucutrada para representar los conceptos independientemente de como estén estructurados en wikipedia.
y me gustó también inferir propiedades y características mediante extractores NLP para tratar de representarlos con clases y propieades de la ontología. 
El ejemplo del asignar género a la persona linkeada en base a la cantidad de veces que se usa she, he dentro del artículo de wikipedia, etc.

estoy pensando que en este momento invertiría lo que hice en el tp2, (usar schema.org como ontología), y pasar a usar dbpedia como la ontología base. que además ya tiene equivalencias definidas a las clases de schema.org.
use schema.org debido a que la info retornada de las fuentes está en json-ld usando "clases" de schema.org, pero podría usar inferencia para obtener la clase equivalente en la ontología de dbpedia,
y almacenarla de esa forma en mis datos. es un paso mas, pero creo que con todas las herraminetas que aprendimos es trivial realizarlo y tendría almacenado los datos de forma mas genérica y compatible con el mundo.

a lo sumo almacenar la info con las 2 clases.
    rdf:type schema:Person
    rdf:type dbo:Person

la primera la obtengo de los datos exportados por las fuentes de las pelis.
la segunda la obtendría de la inferencia sobre la ontología de dbpedia.

---

igual me falta leer sobre wikidata así que sigo analizando las cosas.
ahora a ver el punto sparql de dbpedia.

---


select ?property ?object where {
 dbr:National_University_of_La_Plata ?property ?object.
}
Explore:

Los distintos formatos para los resultados
1 . En la pantalla que muestra resultados de queries en HTML, ¿qué pasa cuando hace click en una URI que pertenece al dominio dbpedia.org? 
¿qué pasa si la URI está afuera? 
¿A que cree que se debe la diferencia en comportamiento? (recuerde que la clickear la URI estamos dereferenciando el recurso, como discutimos en la clase respecto a Linked Data)

para el caso de las propiedades. tomo como ejemplo algunas internas a dbpedia y otras externas a dbpedia.
http://www.w3.org/1999/02/22-rdf-syntax-ns#type
http://www.w3.org/2000/01/rdf-schema#label
http://xmlns.com/foaf/0.1/name
http://purl.org/dc/terms/subject
http://www.w3.org/2002/07/owl#sameAs
http://www.w3.org/ns/prov#wasDerivedFrom


http://dbpedia.org/ontology/wikiPageWikiLink
http://dbpedia.org/property/name

si consulto la propiedad name por ejemplo primero me redirecciona a la iri con https. y después me responde con un html con una descripción para humanos de la propiedad.
en esa respuesta se ve que las cabeceras te dan la url del recurso en distintos formatos (entre ellos turtle). (que también tiene una redirección a la iri de https)
por ej describo bien el camino.

pablo@xiaomi:/src/github/facu-infor/twss$ curl -I http://dbpedia.org/property/name
HTTP/1.1 303 See Other
Server: nginx/1.18.0
Date: Mon, 31 May 2021 14:03:08 GMT
Content-Type: text/html
Content-Length: 153
Connection: keep-alive
Location: https://dbpedia.org/property/name
Access-Control-Allow-Credentials: true
Access-Control-Allow-Methods: HEAD, GET, POST, OPTIONS
Access-Control-Allow-Headers: Depth,DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Accept-Encoding

pablo@xiaomi:/src/github/facu-infor/twss$ curl -I https://dbpedia.org/property/name
HTTP/2 200 
date: Mon, 31 May 2021 14:03:16 GMT
content-type: text/html; charset=UTF-8
content-length: 11893
vary: Accept-Encoding
server: Virtuoso/08.03.3319 (Linux) x86_64-centos_6-linux-glibc2.12  VDB
expires: Mon, 07 Jun 2021 14:03:16 GMT
link: <http://creativecommons.org/licenses/by-sa/3.0/>;rel="license", <http://dbpedia.org/data4/name.rdf>; rel="alternate"; type="application/rdf+xml"; title="Structured Descriptor Document (RDF/XML format)", <http://dbpedia.org/data4/name.n3>; rel="alternate"; type="text/n3"; title="Structured Descriptor Document (N3 format)", <http://dbpedia.org/data4/name.ttl>; rel="alternate"; type="text/turtle"; title="Structured Descriptor Document (Turtle format)", <http://dbpedia.org/data4/name.json>; rel="alternate"; type="application/json"; title="Structured Descriptor Document (RDF/JSON format)", <http://dbpedia.org/data4/name.atom>; rel="alternate"; type="application/atom+xml"; title="OData (Atom+Feed format)", <http://dbpedia.org/sparql?default-graph-uri=http%3A%2F%2Fdbpedia.org&query=DESCRIBE%20%3Chttp%3A%2F%2Fdbpedia.org%2Fproperty%2Fname%3E&format=text%2Fcsv>; rel="alternate"; type="text/csv"; title="Structured Descriptor Document (CSV format)", <http://dbpedia.org/sparql?default-graph-uri=http%3A%2F%2Fdbpedia.org&query=DESCRIBE%20%3Chttp%3A%2F%2Fdbpedia.org%2Fproperty%2Fname%3E&format=text%2Fcxml>; rel="alternate"; type="text/cxml"; title="Structured Descriptor Document (CXML format)", <http://dbpedia.org/data4/name.ntriples>; rel="alternate"; type="text/plain"; title="Structured Descriptor Document (N-Triples format)", <http://dbpedia.org/sparql?default-graph-uri=http%3A%2F%2Fdbpedia.org&query=DESCRIBE%20%3Chttp%3A%2F%2Fdbpedia.org%2Fproperty%2Fname%3E&format=application%2Fmicrodata%2Bjson>; rel="alternate"; type="application/microdata+json"; title="Structured Descriptor Document (Microdata/JSON format)", <http://dbpedia.org/sparql?default-graph-uri=http%3A%2F%2Fdbpedia.org&query=DESCRIBE%20%3Chttp%3A%2F%2Fdbpedia.org%2Fproperty%2Fname%3E&format=text%2Fhtml>; rel="alternate"; type="text/html"; title="Structured Descriptor Document (Microdata/HTML format)", <http://dbpedia.org/sparql?default-graph-uri=http%3A%2F%2Fdbpedia.org&query=DESCRIBE%20%3Chttp%3A%2F%2Fdbpedia.org%2Fproperty%2Fname%3E&format=application%2Fld%2Bjson>; rel="alternate"; type="application/ld+json"; title="Structured Descriptor Document (JSON-LD format)", <http://dbpedia.org/property/name>; rel="http://xmlns.com/foaf/0.1/primaryTopic", <http://dbpedia.org/property/name>; rev="describedby", <http://dbpedia.mementodepot.org/timegate/http://dbpedia.org/property/name>; rel="timegate"
cache-control: max-age=604800
access-control-allow-credentials: true
access-control-allow-methods: HEAD, GET, POST, OPTIONS
access-control-allow-headers: Depth,DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Accept-Encoding
accept-ranges: bytes

pablo@xiaomi:/src/github/facu-infor/twss$ curl -I http://dbpedia.org/data4/name.ttl
HTTP/1.1 303 See Other
Server: nginx/1.18.0
Date: Mon, 31 May 2021 14:03:30 GMT
Content-Type: text/html
Content-Length: 153
Connection: keep-alive
Location: https://dbpedia.org/data4/name.ttl
Access-Control-Allow-Credentials: true
Access-Control-Allow-Methods: HEAD, GET, POST, OPTIONS
Access-Control-Allow-Headers: Depth,DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Accept-Encoding

pablo@xiaomi:/src/github/facu-infor/twss$ curl -I https://dbpedia.org/data4/name.ttl
HTTP/2 200 
date: Mon, 31 May 2021 14:03:36 GMT
content-type: text/turtle; charset=UTF-8
content-length: 306
server: Virtuoso/08.03.3319 (Linux) x86_64-centos_6-linux-glibc2.12  VDB
x-sparql-default-graph: http://dbpedia.org
content-disposition: filename=sparql_2021-05-31_14-01-47Z.ttl
expires: Mon, 07 Jun 2021 14:03:36 GMT
cache-control: max-age=604800
access-control-allow-credentials: true
access-control-allow-methods: HEAD, GET, POST, OPTIONS
access-control-allow-headers: Depth,DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Accept-Encoding
accept-ranges: bytes



por ejemplo si requerimos el turtle de la iri de la propiedad sin analizar la cabecera link de la respuesta vemos:

pablo@xiaomi:/src/github/facu-infor/twss$ curl -H 'accept: text/turtle' https://dbpedia.org/property/name
<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 2.0//EN">
<html><head>
<title>406 Not Acceptable</title>
</head><body>
<h1>406 Not Acceptable</h1>
<p>An appropriate representation of the requested resource name could not be found on this server.</p>
Available variant(s):
<ul>
<li><a href="name">name</a> , type text/html, charset UTF-8</li>
</ul>
</body></html>

------

ahora analicemos que pasa con una propiedad externa, ej:

http://www.georss.org/georss/point


pablo@xiaomi:/src/github/facu-infor/twss$ curl -I http://www.georss.org/georss/point
HTTP/1.1 200 OK
Server: openresty
Date: Mon, 31 May 2021 14:06:38 GMT
Content-Type: text/html; charset=UTF-8
Connection: keep-alive
X-Adblock-Key: MFwwDQYJKoZIhvcNAQEBBQADSwAwSAJBANDrp2lz7AOmADaN8tA50LsWcjLFyQFcb/P2Txc58oYOeILb3vBw7J6f4pamkAQVSQuqYsKx3YzdUHCvbVZvFUsCAwEAAQ==_I6k/NpDijLD71zgmVmqDBczFx9clwPdVGeqWdd7JgieT8eIERejv3Lk7tCORlQffHJ1kVBS18YI6RJmhm6+Dhw==

no hay redirección de nada. veo si pidiendo el content type turtle me responde distinto.

pablo@xiaomi:/src/github/facu-infor/twss$ curl -H 'accept:text/turtle' -I http://www.georss.org/georss/point
HTTP/1.1 200 OK
Server: openresty
Date: Mon, 31 May 2021 14:07:09 GMT
Content-Type: text/html; charset=UTF-8
Connection: keep-alive
X-Adblock-Key: MFwwDQYJKoZIhvcNAQEBBQADSwAwSAJBANDrp2lz7AOmADaN8tA50LsWcjLFyQFcb/P2Txc58oYOeILb3vBw7J6f4pamkAQVSQuqYsKx3YzdUHCvbVZvFUsCAwEAAQ==_I6k/NpDijLD71zgmVmqDBczFx9clwPdVGeqWdd7JgieT8eIERejv3Lk7tCORlQffHJ1kVBS18YI6RJmhm6+Dhw==

tampoco hay redirección.
analizo la respuesta de ese sitio.

pablo@xiaomi:/src/github/facu-infor/twss$ curl -H 'accept:text/turtle' http://www.georss.org/georss/point

es un html que en este caso no describe nada, y no tiene contenido asociado a la propiedad. es una página que indica que el dominio está en venta.
por lo que no sería una iri desreferenciable!.
el recurso no tiene presencia en la web.

---

otra propiedad externa:

http://www.w3.org/2000/01/rdf-schema#label


pablo@xiaomi:/src/github/facu-infor/twss$ curl -I http://www.w3.org/2000/01/rdf-schema#label
HTTP/1.1 200 OK
date: Mon, 31 May 2021 14:14:27 GMT
content-location: rdf-schema.ttl
vary: negotiate,accept,accept-charset,upgrade-insecure-requests
tcn: choice
access-control-allow-credentials: true
access-control-allow-methods: GET, HEAD, OPTIONS
access-control-allow-headers: Link, Location, Content-Type, Accept, Vary
access-control-expose-headers: Location, Link, Vary, Last-Modified, ETag, Allow, Content-Length, Accept
access-control-allow-origin: *
last-modified: Tue, 25 Feb 2014 02:53:20 GMT
etag: "ee4-4f33230d4a800;586929c6c2725"
accept-ranges: bytes
content-length: 3812
cache-control: max-age=21600
expires: Mon, 31 May 2021 20:14:27 GMT
access-control-allow-origin: *
content-type: text/turtle; charset=utf-8
x-backend: www-mirrors

pablo@xiaomi:/src/github/facu-infor/twss$ curl http://www.w3.org/2000/01/rdf-schema#label
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix owl: <http://www.w3.org/2002/07/owl#> .
@prefix dc: <http://purl.org/dc/elements/1.1/> .

<http://www.w3.org/2000/01/rdf-schema#> a owl:Ontology ;
	dc:title "The RDF Schema vocabulary (RDFS)" .

...
....

en este caso aunque no explicite el content type que requiero ya me tira el turtle.
o sea que este recurso es desreferenciable y tiene presencia en la web.

-----

otra propiedad, por ejemplo lo de foaf.

pablo@xiaomi:/src/github/facu-infor/twss$ curl http://xmlns.com/foaf/0.1/name
<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 2.0//EN">
<html><head>
<title>303 See Other</title>
</head><body>
<h1>See Other</h1>
<p>The answer to your request is located <a href="http://xmlns.com/foaf/spec/">here</a>.</p>
<hr>
<address>Apache/2.4.7 (Ubuntu) Server at xmlns.com Port 80</address>
</body></html>

pablo@xiaomi:/src/github/facu-infor/twss$ curl -I http://xmlns.com/foaf/0.1/name
HTTP/1.1 303 See Other
Date: Mon, 31 May 2021 14:08:19 GMT
Server: Apache/2.4.7 (Ubuntu)
Access-Control-Allow-Origin: *
Location: http://xmlns.com/foaf/spec/
Content-Type: text/html; charset=iso-8859-1

pablo@xiaomi:/src/github/facu-infor/twss$ curl -I http://xmlns.com/foaf/spec/
HTTP/1.1 200 OK
Date: Mon, 31 May 2021 14:08:32 GMT
Server: Apache/2.4.7 (Ubuntu)
Last-Modified: Tue, 14 Jan 2014 19:54:30 GMT
ETag: "3672a-4eff38f801066"
Accept-Ranges: bytes
Content-Length: 223018
Vary: Accept-Encoding
Content-Type: text/html

pablo@xiaomi:/src/github/facu-infor/twss$ curl http://xmlns.com/foaf/spec/
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head
xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
xmlns:foaf="http://xmlns.com/foaf/0.1/"
xmlns:vs="http://www.w3.org/2003/06/sw-vocab-status/ns#"
xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#"
xmlns:owl="http://www.w3.org/2002/07/owl#"

esto es un recurso html, la descripción para humanos.
por lo que veo en este caso la página de foaf DICE explícitamente que se puede obtener mediante content negociation usando la uri del namespace. http://xmlns.com/foaf/0.1/

pero no me lo restá resolviendo mediante content negociation

pablo@xiaomi:/src/github/facu-infor/twss$ curl -H 'accept:text/turtle' -I http://xmlns.com/foaf/0.1/
HTTP/1.1 200 OK
Date: Mon, 31 May 2021 14:42:16 GMT
Server: Apache/2.4.7 (Ubuntu)
Access-Control-Allow-Origin: *
Last-Modified: Sun, 23 Jan 2011 19:42:36 GMT
ETag: "2e12b-49a88af74aad6"
Accept-Ranges: bytes
Content-Length: 188715
Vary: Accept-Encoding
Content-Type: text/html

pablo@xiaomi:/src/github/facu-infor/twss$ curl -H 'accept:text/turtle' http://xmlns.com/foaf/0.1/

esto me retorna un html tal cual dice en el content-type.

lo consulto en el foro a ver como interpretar todo esto.

----

para le caso de los objetos, las iris que representan recursos pasa algo similar que con la de las propiedades.
recursos internos a dbpedia tienen representación para pc desreferenciable.
recrusos externos no siempre.

ej con el recurso interno a dbpedia 

pablo@xiaomi:/src/github/facu-infor/twss$ curl -I https://dbpedia.org/resource/Buenos_Aires_Province
HTTP/2 303 
date: Mon, 31 May 2021 15:08:54 GMT
content-type: text/html; charset=UTF-8
content-length: 0
server: Virtuoso/08.03.3319 (Linux) x86_64-centos_6-linux-glibc2.12  VDB
location: http://dbpedia.org/page/Buenos_Aires_Province
access-control-allow-credentials: true
access-control-allow-methods: HEAD, GET, POST, OPTIONS
access-control-allow-headers: Depth,DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Accept-Encoding

pablo@xiaomi:/src/github/facu-infor/twss$ curl -H 'accept:text/turtle' -I https://dbpedia.org/resource/Buenos_Aires_Province
HTTP/2 303 
date: Mon, 31 May 2021 15:09:18 GMT
content-type: text/turtle
content-length: 0
server: Virtuoso/08.03.3319 (Linux) x86_64-centos_6-linux-glibc2.12  VDB
tcn: choice
vary: negotiate,accept
alternates: {"/data/Buenos_Aires_Province.atom" 0.500000 {type application/atom+xml}}, {"/data/Buenos_Aires_Province.jrdf" 0.600000 {type application/rdf+json}}, {"/data/Buenos_Aires_Province.jsod" 0.500000 {type application/odata+json}}, {"/data/Buenos_Aires_Province.json" 0.600000 {type application/json}}, {"/data/Buenos_Aires_Province.jsonld" 0.500000 {type application/ld+json}}, {"/data/Buenos_Aires_Province.n3" 0.800000 {type text/n3}}, {"/data/Buenos_Aires_Province.nt" 0.800000 {type text/rdf+n3}}, {"/data/Buenos_Aires_Province.ntriples" 0.500000 {type application/n-triples}}, {"/data/Buenos_Aires_Province.ttl" 0.700000 {type text/turtle}}, {"/data/Buenos_Aires_Province.xml" 0.950000 {type application/rdf+xml}}
link: <http://creativecommons.org/licenses/by-sa/3.0/>;rel="license",<http://dbpedia.mementodepot.org/timegate/http://dbpedia.org/resource/Buenos_Aires_Province>; rel="timegate"
location: http://dbpedia.org/data/Buenos_Aires_Province.ttl
access-control-allow-credentials: true
access-control-allow-methods: HEAD, GET, POST, OPTIONS
access-control-allow-headers: Depth,DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Accept-Encoding

pablo@xiaomi:/src/github/facu-infor/twss$ curl -H 'accept:text/turtle' -I https://dbpedia.org/data/Buenos_Aires_Province.ttl
HTTP/2 200 
date: Mon, 31 May 2021 15:09:54 GMT
content-type: text/turtle; charset=UTF-8
content-length: 277174
server: Virtuoso/08.03.3319 (Linux) x86_64-centos_6-linux-glibc2.12  VDB
expires: Mon, 07 Jun 2021 15:09:54 GMT
link: <http://creativecommons.org/licenses/by-sa/3.0/>;rel="license",<http://dbpedia.org/data/Buenos_Aires_Province.xml>; rel="alternate"; type="application/rdf+xml"; title="Structured Descriptor Document (RDF/XML format)", <http://dbpedia.org/data/Buenos_Aires_Province.n3>; rel="alternate"; type="text/n3"; title="Structured Descriptor Document (N3/Turtle format)", <http://dbpedia.org/data/Buenos_Aires_Province.json>; rel="alternate"; type="application/json"; title="Structured Descriptor Document (RDF/JSON format)", <http://dbpedia.org/data/Buenos_Aires_Province.atom>; rel="alternate"; type="application/atom+xml"; title="OData (Atom+Feed format)", <http://dbpedia.org/data/Buenos_Aires_Province.jsod>; rel="alternate"; type="application/odata+json"; title="OData (JSON format)", <http://dbpedia.org/page/Buenos_Aires_Province>; rel="alternate"; type="text/html"; title="XHTML+RDFa", <http://dbpedia.org/resource/Buenos_Aires_Province>; rel="http://xmlns.com/foaf/0.1/primaryTopic", <http://dbpedia.org/resource/Buenos_Aires_Province>; rev="describedby", <http://dbpedia.mementodepot.org/timegate/http://dbpedia.org/data/Buenos_Aires_Province.ttl>; rel="timegate"
x-sparql-default-graph: http://dbpedia.org
content-disposition: filename=sparql_2021-05-31_15-09-54Z.ttl
cache-control: max-age=604800
access-control-allow-credentials: true
access-control-allow-methods: HEAD, GET, POST, OPTIONS
access-control-allow-headers: Depth,DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Accept-Encoding
accept-ranges: bytes

esto me retorna un turtle.

---

para un objeto externo a dbpedia.

pablo@xiaomi:/src/github/facu-infor/twss$ curl -I http://viaf.org/viaf/146084247
HTTP/1.1 303 See Other
Server: Apache-Coyote/1.1
Location: https://viaf.org/viaf/146084247/
Content-Type: text/plain
Content-Length: 43
Date: Mon, 31 May 2021 15:11:50 GMT

pablo@xiaomi:/src/github/facu-infor/twss$ curl -I https://viaf.org/viaf/146084247
HTTP/1.1 303 See Other
Server: Apache-Coyote/1.1
Location: https://viaf.org/viaf/146084247/
Content-Type: text/plain
Content-Length: 43
Date: Mon, 31 May 2021 15:11:56 GMT

pablo@xiaomi:/src/github/facu-infor/twss$ curl -I https://viaf.org/viaf/146084247/
HTTP/1.1 200 OK
Server: Apache-Coyote/1.1
srwRequestMethod: HEAD
Access-Control-Allow-Origin: *
Access-Control-Allow-Methods: GET
X-FRAME-OPTIONS: SAMEORIGIN
X-Content-Security-Policy: allow self
Content-Location: viaf.html
Cache-Control: max-age=604800
Expires: Mon, 07 Jun 2021 15:12:10 GMT
Content-Type: text/html;charset=ISO-8859-1
Content-Length: 0
Date: Mon, 31 May 2021 15:12:10 GMT

pablo@xiaomi:/src/github/facu-infor/twss$ curl -H 'accept: text/turtle' -I https://viaf.org/viaf/146084247/
HTTP/1.1 200 OK
Server: Apache-Coyote/1.1
srwRequestMethod: HEAD
Access-Control-Allow-Origin: *
Access-Control-Allow-Methods: GET
X-FRAME-OPTIONS: SAMEORIGIN
X-Content-Security-Policy: allow self
Cache-Control: max-age=604800
Expires: Mon, 07 Jun 2021 15:12:24 GMT
Content-Type: text/xml;charset=ISO-8859-1
Content-Length: 0
Date: Mon, 31 May 2021 15:12:23 GMT


pablo@xiaomi:/src/github/facu-infor/twss$ curl -H 'accept: text/turtle' https://viaf.org/viaf/146084247/

<?xml version="1.0"  encoding="UTF-8"?> 
<?xml-stylesheet type="text/xsl" href="/viaf/xsl/searchRetrieveResponse_en.xsl"?>
<ns1:VIAFCluster xmlns="http://viaf.org/viaf/terms#" xmlns:foaf="http://xmlns.com/foaf/0.1/" xmlns:owl="http://www.w3.org/2002/07/owl#" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:void="http://rdfs.org/ns/void#" xmlns:ns1="http://viaf.org/viaf/terms#"><ns1:viafID>146084247</ns1:viafID><ns1:Document about="http://viaf.org/viaf/146084247/"><ns1:inDataset resource="http://viaf.org/viaf/data"/><ns1:primaryTopic resource="http://viaf.org/viaf/146084247"/></ns1:Document><ns1:nameType>Corporate</ns1:nameType><ns1:sources><ns1:source nsid="A000189928">RERO|A000189928</ns1:source><ns1:source nsid="987007269238505171">J9U|987007269238505171</ns1:source><ns1:source nsid="n50078155">LC|n  50078155</ns1:source><ns1:source nsid="FRBNF123217697">BNF|12321769</ns1:source><ns1:source nsid="http://d-nb.info/gnd/35897-6">DNB|000358975</ns1:source><ns1:source nsid="0000000459043590">ISNI|000000

en este caso me retorna un xml. que puede ser interpertado por una máquina, pero no corresponde con el content-type requerido.
igualmente en este caso sería desreferenciable y tendría presencia.

----

en otro caos no se encuentra nada. no sería desreferenciable.

pablo@xiaomi:/src/github/facu-infor/twss$ curl -I http://rdf.freebase.com/ns/m.07_yzv
HTTP/1.1 404 Not Found
Date: Mon, 31 May 2021 15:40:14 GMT
Content-Type: text/html; charset=UTF-8
Server: ghs
Content-Length: 1572
X-XSS-Protection: 0
X-Frame-Options: SAMEORIGIN


----

Ejercicio 3: Realizar consultas Sparql en dbpedia
Para cada caso reporte la consulta sparql correspondiente y el resultado de la misma. En las consultas, de preferencia al uso de clases y propiedades en la ontología de dbpedia (dbo) 

a) Obtener a los escritores que hayan nacido en una ciudad de Argentina. 

inicio a consultar algunas cosas para ver que me trae.
analizo por ejemplo a borjes. https://dbpedia.org/page/Jorge_Luis_Borges

SELECT DISTINCT ?p ?o
			WHERE
			{
			dbr:Jorge_Luis_Borges ?p ?o .
			}



veo algunas cosas que puedo usar como filtro.

dbp:occupation == Writer
dbo:birthPlace = dbr:Argentina, dbr:Buenos_Aires
dbp:nationality = dbr:Argentines ---> pero pueden ser naturalizados, asi que esto no lo tendría en cuenta.
rdf:type = yago:*writer --> pero es de otro dataset asi que no lo tengo en cuenta.
rdf:type = dbo:Writer

dbo:author --> dbr:* --> rdf:type = dbo:Book, dbo:WrittenWork
dbo:writer --> dbr:* --> rdf:type = dbo:Work
dbp:author --> dbr:* --> rdf:type = dbo:WrittenWork
dbp:writer --> dbr:* --> rdf:type = dbo:WrittenWork

o sea por un lado debemos obtener las personas que sean escritores.

SELECT DISTINCT ?s ?p
			WHERE
			{
			?s ?p ?o.
                        ?o rdf:type dbo:WrittenWork
			}
esto tira autores de cosas escritas pero que pueden ser páginas de wikipedia, etc.
asi que no va, me parece que la consulta está orientado a obras literarias.

SELECT DISTINCT ?s ?p ?o ?c
			WHERE
			{
			?s ?p ?o .
?o rdf:type dbo:Book .
?s dbo:birthPlace dbr:Argentina .
			}            

lo puedo expandir con ciudades que pertenecen a argentina.

SELECT DISTINCT ?s ?p ?o ?c
			WHERE
			{
			?s ?p ?o .
?o rdf:type dbo:Book .
?s dbo:birthPlace ?c .
?c dbo:country dbr:Argentina .

			}


tengo que restringirlo a que sea autor de la obra si no me trae personas relacionadas igual que no son 
los autores.

SELECT DISTINCT ?s ?p ?o ?c
			WHERE
			{
			?s ?p ?o .
?o rdf:type dbo:Book .
?o dbo:author ?s .
?s dbo:birthPlace ?c .
?c dbo:country dbr:Argentina .

			}

desambiguo a los autores. y chequeo por pais ya que no en todos los casos tengo la ciudad en la que nacieron.

SELECT DISTINCT ?s 
			WHERE
{
    		?s ?p ?o .
    ?o rdf:type dbo:Book .
    ?o dbo:author ?s .
    { 
        ?s dbo:birthPlace ?c .
        ?c dbo:country dbr:Argentina .
    } UNION {
        ?s dbo:birthPlace dbr:Argentina
    }
}


me da la misma cantidad de autores, inclusive si consulto solo que hayan nacido en argentina, sin involucrar la ciudad.
pero no me gusta el resultado. son muy pocos.
asi que reformulo la consulta para analizar un poco otros resultados.

la mas trivial:

SELECT DISTINCT ?s
			WHERE
			{
    ?s dbo:birthPlace dbr:Argentina .
    ?s rdf:type dbo:Writer .
			}

esta me gusta mas ya que abarca a mayores resultados sin especificar la ciudad.
ej:
http://dbpedia.org/resource/Betina_Gonzalez
es escritora, que según la info nació en el AMBA.
Betina González (Villa Ballester, 1972) es una escritora argentina, primera mujer en ganar el Premio Tusquets de Novela.​ (es)
pero en la info de dbpedia solo se indica que nació en argentina.


---
b) Obtener a los escritores que hayan nacido en una ciudad de Uruguay.

es lo mismo, que haya nacido en Uruguay ya implica que nació en una ciudad de uruguay.

SELECT DISTINCT ?s
			WHERE
			{
                ?s dbo:birthPlace dbr:Uruguay .
                ?s rdf:type dbo:Writer .
			}

por lo que veo dbpedia infiere siempre cuando tiene la info de la ciudad. le pone también la info del país 
en birthPlace.


---

c) Utilizando el keyword filter (vea sección 6.3.2.6 del libro), obtener a los escritores que hayan nacido en 
una ciudad de Argentina o de Uruguay 

SELECT DISTINCT ?s
			WHERE
			{
    ?s dbo:birthPlace ?pais .
    ?s rdf:type dbo:Writer .
    filter (?pais = dbr:Argentina || ?pais = dbr:Uruguay) .
			}

--

d) Utilizando el keyword union (vea sección 6.3.2.6 del libro), obtener a los escritores que hayan nacido en una ciudad de Argentina o de Uruguay

SELECT DISTINCT ?s
			WHERE
			{
                ?s rdf:type dbo:Writer .
                { ?s dbo:birthPlace dbr:Argentina . }
                UNION 
                { ?s dbo:birthPlace dbr:Uruguay .}
			}


--

e) ¿Qué diferencia hay entre c y d? ¿En cual se deben recuperar/analizar menor número de tripletas?

asumo que la union analiza mayor número de tripletas.
si pensamos que  para obtener la union de los conjuntos hay que procesar los 2 conjuntos primero.
en pseudocódigo:

c1 = for autor in autores if autor dbo:birthPlace dbr:Argentina
c2 = for autor in autores if autor dbo:birthPlace dbr:Uruguay
c3 = c1 + c2

entonces se procesaría el dataset de autores 2 veces. (la variable ?s)
para el caso del filter el if va adentro del "for"

c3 = for autor in autores if autor dbo:birthPlace dbr:Argentina or autor dbo:birthPlace dbr:Uruguay

en este caso se recorre solamente 1 vez el dataset de autores.
pero esto es una suposición. tengo que buscar algún motor que me permita explorar como realiza la consulta el mismo motor y que optimizaciones hace.
me imagino que cada motor realiza distintas optimizaciones.


---


f) ¿Cuantos empleados tiene la compañía mas grande en dbpedia, y en que país está ubicada? 
(obtenga la lista de todas las compañías y los países donde están ubicadas ordenada de forma descendiente 
por numero de empleados)


primero analizo como se obtienen las comañías. asumo que es rdf:type dbo:Company
efectivamente:

rdfs:label	
        company (en)
        Unternehmen (de)
        empresa (es)
        empresa (pt)

asi que filtro primero por eso:

SELECT DISTINCT ?s
			WHERE
			{
?s rdf:type dbo:Company .
}

agrego la ubicación

SELECT DISTINCT ?s ?l
			WHERE
			{
?s rdf:type dbo:Company .
?s dbo:location ?l .

			}


analizo propieades en dbpedia y ver:
http://dbpedia.org/ontology/numberOfEmployees

asi que formulo nuevamente la consulta:

SELECT DISTINCT ?s ?l ?n
			WHERE
			{
                ?s rdf:type dbo:Company .
                ?s dbo:location ?l .
                OPTIONAL
                {
                ?s dbo:numberOfEmployees ?n .
                }
			}

veo que esta consulta me trae distintas cantidades de empleados para distintas ubicaciones de la misma empresa.
asi que tengo que agregarlas.
analizando el capítulo del libro que habla de agregaciones (6.3.2.1) - Segunda Edición!.

SELECT DISTINCT ?s SUM(?n) as ?empleados
			WHERE
			{
                ?s rdf:type dbo:Company .
                ?s dbo:location ?l .
                OPTIONAL
                {
                ?s dbo:numberOfEmployees ?n
                }
			}
            GROUP BY ?s


ahora trato de ordenar esa lista.

SELECT DISTINCT ?s SUM(?n) as ?empleados
			WHERE
			{
                ?s rdf:type dbo:Company .
                ?s dbo:location ?l .
                OPTIONAL
                {
                ?s dbo:numberOfEmployees ?n
                }
			}
            GROUP BY ?s
            order by desc(?empleados)

funcionó.
me ordenó en forma descendente la cantidad de empleados.
pero veo algo raro. me da: https://dbpedia.org/page/Eureka_Forbes 
como la mayor empresa.
me imaginaba google, amazon, etc.
analizando un poco veo que eureka forbes tiene dbo:location
dbr:Maharashtra
dbr:India
dbr:Mumbai

a ver si me está agregando esas 3 ubicaciones?. porque mumbai -> Maharashtra -> India
o sea es una ciudad de un estado de un pais. no se debería estar agregando.

ejecuto la consulta usando :

SELECT DISTINCT ?s ?l ?n
			WHERE
			{
                ?s rdf:type dbo:Company .
                ?s dbo:location ?l .
                OPTIONAL
                {
                ?s dbo:numberOfEmployees ?n
                }
			}
            order by desc(?n)

y veo que efectivamente me muestra 3 veces eureka.
y cada una con 5000000 empleados.
si corro la consulta agregada entonces me muestra 15000000 empleados.
efectivamente estoy agregando algo que no debería.

el patrón se repite para todas las empresas. así que elimino la agregación de los empleados ya que no es necesaria.
y agrego que las ubicaciones sean paises.

SELECT DISTINCT ?s ?l ?n
			WHERE
			{
                ?s rdf:type dbo:Company .
                ?s dbo:location ?l .
                ?l rdf:type dbo:Country .
                OPTIONAL
                {
                ?s dbo:numberOfEmployees ?n
                }
			}
            order by desc(?n)


lo opcional es para obtener tambien las empresas que no tiene dbpedia registrado ningún empleado.
que estén al final del listado.

-------------------
-------------------
-------------------
-----

arranco con la parte de wikidata.
https://www.wikidata.org/wiki/Wikidata:Introduction


---

después de leer miles de artículos no encuentro bien donde están definidos los aspectos técnicos de wikidata y el manejo interno.
como los artículos que existen para dbpedia, no veo nada similar para wikidata.

en la documentación encuentro que existen bots que generan información dentro de la wikidata.
y editores de wikidata. pero no la conversión automática de wikipedia --> wikidata.

solo que fue un proyecto creado inicialmente para mantener de forma estructurada los links entre las wikis multilenguaje, 
así que cada vez que se crea un artículo en wikipedia se crea el correspondiente Q.
y que fue creciendo para darle una base estructurada a todos los proyectos wikimedia.

----

me creo una cuenta dentro de wikidata para analizar como es la creación del contenido.
tiene una estructura distinta de la wikipedia y tiene mucha documentación asociada.
mediante los asistentes no veo simple lo que necesito.

----

uso la misma cuenta para acceder a la wikipedia y tratar de ingresar algún artículo ahi.
no veo forma de incluir información a wikidata a partir de wikipedia.
los editores de wiki me permiten generar una página, insertar referencias, enlaces, 
imágnenes, etc. pero no veo en el editor gráfico info sobre wikidata.
explícitamente que se transforma de esa info en wikidata.

---

de todo lo que estuve leyendo asumo que todo lo que es infobox sale de wikidata.
y esa es la forma de incluir esos datos dentro de los artículos de wikipedia.
pruebo editar la página de la universidad y veo :

{{short description|Public research university in Argentina}}
{{Coord|34|54|46.69|S|57|57|04.88|W|type:edu|display=title}}
{{Infobox university
| name              = La Plata National University
| native_name       = ''Universidad Nacional de La Plata''
| image             = UNLP Logo.jpg
| image_size        = 180px
| image_alt         = 
| caption           = Seal of the National University of La Plata
| latin_name        = 
| motto             = {{lang|la|Pro scientia et patria}} ([[Latin]])
| motto_lang        = 
| mottoeng          = For science and the fatherland
.....
....
...

lo que no me define es si eso sale de la wikidata o esta editado dentro de wikipedia.
hubiera esperado algo como:

{{Infobox university qid=Q784171}}

de ahi podría haber inferido que la info está cargada en wikidata y se exporta a wikipedia.
pero con lo que veo así como está no puedo inferir nada. a lo sumo lo contrario.
que se carga mediante un template de infobox de valores dentro de wikipedia y ahi le asigno valores (basicamente la sintaxis de arriba)
y eso se inserta automáticamente en el Q784171 correspondiente generado para la página. o sea se generan las tripletas correspondientes y eso
va a parar a wikidata. 
ahora si este es el proceso entonces no me cuadra que cada sentencia en wikidata necesita una referencia, es requerido por wikidata por lo que averigué.
para indicar que una sentencia es verificada. y en este template no se pone en ningún lado la referencia (la fuente de donde es sacada la información)

voy a seguir investigando. para mi me suena que es a la inversa: wikidata ---> wikipedia
es lo lógico.
pero el bootstrap de wikidata inicial y la actualización diaria de wikidata sin wikidateros sería imposible.
asi que tiene que existir el mecanismo de wikipedia --> wikidata.

obviamente existe porque veo propiedades del estilo.

p143 es importado de wikimedia.

---

hago algunas consutlas sparql para analizar un poco la cosa. a ver si puedo identificar algo.

select distinct ?s ?p
  where {
     ?s ?p wd:Q784171
  } limit 10000

pero todo el tema de las iris con Q hacen dificil entenderlo.
la idea es analizar también a la vez si puedo identificar mas del proceso 

wikipedia ---> wikidata

y también la parte identificación del Qnumber a partir de la url de la wiki. una de las preguntas que tenemos que responder.

primer idea es hacer una consulta a wikidata sparql usando como parámetro la url de la wiki.
en algún lado de wikidata tiene que haber alguna tripleta :

Q784171 wikiurl National_University_of_La_Plata

estoy pensando en el concepto de wikilink o intrawiki link etc.
eso seguro es la propiedad que estoy buscando.

--

Wikilink (Q58494516)
hyperlink between pages of the same Wikimedia wiki

----

select distinct ?o
  where {
     wd:Q784171 wd:Q58494516 ?o
  } limit 100


nada de nada. no me respondió ningún resultado.

---

viendo la consulta, me acabo de dar cuenta que puse a la inversa en la consulta general de arriba!!.
el sujeto es la universidad y el objeto tiene que ser la url de la wiki.
pruebo de nuevo a ver que veo!!.

select distinct ?p ?o
  where {
     wd:Q784171 ?p ?o
  } limit 10000


JOOOOOOOOOOOOOOOO. datos, queridos datos que entiendo un poco mas. no solo statements como la consulta anterior.
claro anteriormente lo que estaba consultando es que statements tienen a la univerisad como objeto.
por el esquema de datos de wikidata van a ser solo sentencias.

--

referencia:
https://www.mediawiki.org/wiki/Wikibase/Indexing/RDF_Dump_Format#Full_list_of_prefixes

 PREFIX wdt: <http://www.wikidata.org/prop/direct/>
 PREFIX wd: <http://www.wikidata.org/entity/>
 PREFIX wdtn: <http://www.wikidata.org/prop/direct-normalized/>
 PREFIX p: <http://www.wikidata.org/prop/>
 PREFIX wds: <http://www.wikidata.org/entity/statement/>

.----

buscando en google.

How do I link wikidata to Wikipedia?
From a Wikipedia page, you can go to the link "Wikidata item", using "Tools" in the side pane (in the left), to see and edit it. Also in Tools, there is another link to "page information", where is "Wikidata item ID", that contains the QID (for example: Q171 or "None").

pero siempre hay info no técnica. quiero saber la propiedad que linkea la wiki con la entidad Q de wikidata.
obvio sin leerme todo la info de wikidata jajaj.

---

si una página en wikipedia si o si tiene si Qnumber en wikidata.
para ver cual es la propiedad que los asocia entonces se me ocurrió
listas todas las propieades ordenadas en forma ascendente.

la wiki mas chica va a tener la menos cantidad de esas propiedades, pero va a tener seguro la propiedad que asocia la pagina en wikipedia con la entidad de wikidata.

select distinct SUM(?p) as ?propieades
  where {
     ?s ?p ?o
  } order by asc(?propieades) .
 limit 10 .

tiro error por todas partes. acomodo la sintaxis

select (SUM(?p) as ?propieades)
  where {
     ?s ?p ?o
  } order by asc(?propieades)
   limit 10

obvio como no podía ser de otra forma. tomar todas las tripletas tardaría años!!.

SPARQL-QUERY: queryStr=select (SUM(?p) as ?propieades)
  where {
     ?s ?p ?o
  } order by asc(?propieades)
   limit 10

java.util.concurrent.TimeoutException
	at java.util.concurrent.FutureTask.get(FutureTask.java:205)
	at com.bigdata.rdf.sail.webapp.BigdataServlet.submitApiTask(BigdataServlet.java:292)
	at com.bigdata.rdf.sail.webapp.QueryServlet.doSparqlQuery(QueryServlet.java:678)
	at com.bigdata.rdf.sail.webapp.QueryServlet.doGet(QueryServlet.java:290)
	at com.bigdata.rdf.sail.webapp.RESTServlet.doGet(RESTServlet.java:240)
	at com.bigdata.rdf.sail.webapp.MultiTenancyServlet.doGet(MultiTenancyServlet.java:273)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:865)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1655)
	at org.wikidata.query.rdf.blazegraph.throttling.ThrottlingFi


---

de nuevo en google.

Sitelinks (also known as interwiki links or interlanguage links) are special links that contain a site and a title, and go from individual items in Wikidata to pages on other Wikimedia sites such as Wikipedia, Wikisource and Wikivoyage.

https://www.wikidata.org/wiki/Help:Sitelinks

como indica el artículo el punto central para mi tiene que ser el Q entity de wikidata. de la entidad de la universidad.


----


analizando los ejemplos que tira el sparql de wikidata veo uno que me puede servir:

#Returns a list of Wikidata items for a given list of Wikipedia article names
#List of Wikipedia article names (lemma) is like "WIKIPEDIA ARTICLE NAME"@LANGUAGE CODE with de for German, en for English, etc.
#Language version and project is defined in schema:isPartOF with de.wikipedia.org for German Wikipedia, es.wikivoyage for Spanish Wikivoyage, etc.

SELECT ?lemma ?item WHERE {
  VALUES ?lemma {
    "Wikipedia"@de
    "Wikidata"@de
    "Berlin"@de
    "Technische Universität Berlin"@de
  }
  ?sitelink schema:about ?item;
    schema:isPartOf <https://de.wikipedia.org/>;
    schema:name ?lemma.
}

por lo que veo usa schema:name para el nombre del artículo en la wikipedia!!!!!!!
joooooooooooooooooooooooo y yo buscando una propiedad Pnnnnn
ahora si que me terminé de marear.
los prefijos (https://www.mediawiki.org/wiki/Wikibase/Indexing/RDF_Dump_Format#Full_list_of_prefixes) indican:

PREFIX schema: <http://schema.org/>

whhaattt!!! wikidata usa schmema.org/name para asociar a la entidad wikidata con la página wikipedia??????????????

me parece que estoy entendiendo mal.
lo pruebo:

select ?s where {
    ?s schema:name "National_University_of_La_Plata"@en .
  }

nadaaa. obvio.

select ?s where {
    ?s schema:name "National University of La Plata"@en .
  }

yes. me trae el subject!!!.

<https://en.wikipedia.org/wiki/National_University_of_La_Plata>

ahora si hago lo contrario.


select ?s ?p where {
    ?s ?p <https://en.wikipedia.org/wiki/National_University_of_La_Plata>
  }

no me había avivado de usarlo como objeto asi directamente con <> como si fuera una iri!!
nada no me trajo nada
para ver si es tema de sintaxis me fijo la inversa, debería traer aunque sea la tripleta schema:name .... anterior.

select ?s ?p where {
    <https://en.wikipedia.org/wiki/National_University_of_La_Plata> ?p ?s
  }

resultado!!!

wd:Q784171	schema:about
en	schema:inLanguage
schema:Article	rdf:type
National University of La Plata	schema:name
<https://en.wikipedia.org/>	schema:isPartOf


ahora entonces veo que la propiedad que asocia el Qnumber con la iri de la universidad es schema:about

select ?s where {
      ?s schema:about wd:Q784171
  }


bingooooo!!!

<https://ar.wikipedia.org/wiki/%D8%AC%D8%A7%D9%85%D8%B9%D8%A9_%D9%84%D8%A7_%D8%A8%D9%84%D8%A7%D8%AA%D8%A7_%D8%A7%D9%84%D9%88%D8%B7%D9%86%D9%8A%D8%A9>
<https://arz.wikipedia.org/wiki/%D8%AC%D8%A7%D9%85%D8%B9%D8%A9_%D9%84%D8%A7%D8%A8%D9%84%D8%A7%D8%AA%D8%A7_%D8%A7%D9%84%D9%88%D8%B7%D9%86%D9%8A%D9%87>
<https://az.wikipedia.org/wiki/La_Plata_Milli_Universiteti>
<https://de.wikipedia.org/wiki/Universidad_Nacional_de_La_Plata>
<https://en.wikipedia.org/wiki/National_University_of_La_Plata>
<https://eo.wikipedia.org/wiki/Nacia_Universitato_La_Plata>
<https://es.wikinews.org/wiki/Categor%C3%AD


en resumen.
despues de tanto hacer pruebas y marearme la propiedad que representa los sitelinks es schema:about
genera la estrella que muestra en el gráfico de sitelinks.
en realidad es la inversa a la propiedad que buscaba, pero la que me resuelve mas directo lo que quiero hacer

la pregunta del trabajo es: 


5. ¿Dado el articulo en Wikipedia de "National University of La Plata", como infiero la URL del recurso correspondiente en Wikidata? 

el recurso correspondiente podría inferirlo haciendo la consulta sparql siguiente:

url = url del articulo de wikipedia.
select ?q where {
   <url> schema:about ?q 
}

en particular en nuestro caso tendríamos.

select ?p where {
      <https://es.wikipedia.org/wiki/Universidad_Nacional_de_La_Plata> schema:about ?p
  }

jooooo tengo aunque sea un resultado!!!!, me siento mucho mas realizado, jajaja mas que decir. anda a la interface que te da la wikipedia y ahi te da los datos. 

TOTALMENTE loco!! pense que la propiedad, que yo identifico como principal, en la que se basa toda la creación de la wikidata iba a ser de un prefijo interno a wikidata y no un prefijo como schema.org!!!.

---

espero que todo el análisis esté bien. voy a arrancar a hacer las consultas y lo voy a reever a todo con algunos días de que me quede en la cabeza.
a ver si identifico errores.

----


6. ¿Que diferencias y similitudes encuentra con DBpedia?

tirando ideas de lo que se me ocurre ahora con la info analizada.

primero dbpedia tiene una ontología creada para representar mas conceptos del mundo real.
como si definieramos la ontología pensando en el mundo y no en como está estructurado internamente esa información a nivel de sistema.
o sea que en las transfromaciones de wikipedia ---> dbpedia  se pierden conceptos internos de como está estructurado. 

wikidata tiene una ontología creada para representar conceptos internos de los proyectos wikimedia.
todo el tema de las sentencias, que tienen referencias para evaluar si está verificado o no una afirmación, ayuda a tener datos del mundo real y representar esa info pero lo veo mas que lo diseñaron
con el objetivo de poder saber internamente que tripletas estarían mas verificadas que otras. Es mas, lo ponen como una restricción en la carga de las sentencias dentro de wikidata.
la estructura de links con nodo central, que linkea todas las wikipedias de los distintos lenguajes a la entidad de wikidata, también es un ejemplo de una estructura que mas que 
representar conocimiento, soluciona un problema real interno de links entre las distintas wikis. (mas pensado en la estructura del grafo) 
son mas aspectos pensados para estructurar la información, mas que pensado para ser entendido por humanos directamente.

igual entro en conflicto un poco cuando pienso en que algo de eso tiene dbpedia, ya que por ejemplo
usa distintos prefijos para representar información diseñada (dbo) e información obtenida mediante los extractores sin tener un mapeo definido (dbp)
igualmente dbpedia está represendando conceptos mas del mundo real, una "seguridad" de la veracidad de esa información. mas que un concepto de como estructurar la información internamente.
o sea en este caso el grafo es el mismo.

algo ---> dbo:Pesona ---> persona
          dbp:Person ---> persona

la estructura del grafo no cambia. solo que dbo y dbp permiten asociar esa "seguridad" que le damos al dato.
en cambio en la wikidata si.

wiki_es ----> sameAs ----> wiki_en
wiki_en ----> sameAs ----> wiki_de
wiki_es ----> sameAs ----> wiki_de
....

no es la misma estructura que inventar una entidad central (la Qnumber de wikidata) para linkearlas a todas:

entidad ---> sameAs ----> wiki_en
entidad ---> sameAs ----> wiki_es
entidad ---> sameAs ----> wiki_de


encuentro analogías locas que se me ocurren de otras materias que estoy cursando, 
por ejemplo wikidata me hace acordar mas a el metamodelo de UML (MOF) sirve para estructurar y definir exactamente el modelo UML y como modelar.
y dbpedia lo veo mas como el modelo en UML en si. 
uno es mas abstracto y el otro se acerca mas al mundo real (en este caso el dominio del problema).


---

otra diferencia que se me ocurre es en el proceso de modelado de la ontología.
wikidata dijeron hagamos algo que se pueda adaptar a todas las situaciones sin tener que cambiar la ontología.
justamente la definición que para cada propiedad y entidad de la ontología le asignen un Pnúmero y Qnúmero, ya te indica que es un concepto abstracto.
Q111 será perro
Q122 será gato
Q113 será clase de animal domesticable
....
con esto sea el concepto que tengan que representar, saben que usando esa ontología y vocabulario lo van a poder hacer. no tienen que pensar en cada concepto que quieren representar apriori.

en cambio en dbpedia los conceptos son parte del mundo.
ej: dbo:birthPlace, dbo:author, etc son exactamente conceptos del mundo real, con dominio y rangos mas definidos!!.
un autor a priori fue diseñado para ser una persona y aplicarse a cosas que puedan tener autoría.
para representar un nuevo concepto no contemplado dentro de la ontología debemos modificar si o si la ontología, extenderla, etc.

claramente los Q y los P son conceptos mas abstractos que dbo:autor y dbo:birthPlace. sin modificar la ontología se pueden extender.
solo hace falta de "crear una instancia" de un P y asignarle un número fijo y listo. eso pasa a ser un concepto adicional antes no representado. la clave aca es NO modificar la definición de la ontología.
pasaría a ser una especie de invidividual, que su iri puede representar a su vez un concepto que relaciona otros individuals, o los clasifica, etc.
no se si lo estoy poniendo bien en palabras, pero básicamente todo se resume a, uno es mas abstracto y el otro es mas concreto.

----

la forma mas clara que lo puedo expresar es.
dbpedia se concentra en representar conocimiento
wikidata se concentra además de representar conocimiento, en la forma de representarlo.

--

esto igual lo pense sin siquiera ver dominio y rangos, ni la ontología en croncreto de wikidata. todo lo que me encontré es info sobre como ir generando visualmente los datos dentro de wikipedia y wikidata.
me está volviendo loco, busco y busco y no puedo encontrar en ningún lado un turtle de la ontología de wikidata. jajaa.
todo es páginas y páginas de texto escrito y links entre si.
como lo de schema.org que en vez de tener una ontología clara escrita en máquina, tenemos páginas que explican lo que significan esos conceptos, la estructura, etc pero todo para humanos.

--

por ahi este análisis es totalmente erróneo y cambie con el tiempo a medida que vaya invesigando mas de wikidata, pero es la primer sensación que me da cuando leí la documentación de wikipedia y los artículos de dbpedia.
y el proceso que necesité para entender las estructuras/ontologías de cada uno de los proyectos. claramente apurado leyendo lo menos posible para entender un concepto debido a los tiempos.
voy a hacer las consultas a ver si me queda mas claro como maneja todo wikidata. así que por ahora solo es una anotación.

----

croe que encontré oro!!!.
https://homepages.dcc.ufmg.br/~laender/material/gdw2015-EGK+2014.pdf

In spite of all this, Wikidata has hardly been used in the semantic web community
so far. The simple reason is that, until recently, the data was not available in RDF. To
change this, we have developed RDF encodings for Wikidata, implemented a tool for
creating file exports, and set up a site where the results are published. Regular RDF
dumps of the data can now be found at http://tools.wmflabs.org/wikidata-exports/rdf/. This
paper describes the design underlying these exports and introduces the new datasets:
– In Section 2, we introduce the data model of Wikidata, which governs the structure
of the content that we want to export.


lo bajo como material y lo leo.


-----

ME QUIERO MATAR!!!.

Site links are exported using the schema.org property about to associate a Wikipedia
page URL with its Wikidata item, and the schema.org property inLanguage to define the
BCP 47 language code of a Wikipedia page. Table 2 (bottom) gives an example

aca estaba!!!. leyendo material y buscando por google podría haberme evitado bastantes pruebas!!.

---


entendiendo un poco mas encuentro de forma simple lo que me va a ayudar un poco mas.

https://www.wikidata.org/wiki/Wikidata:RDF
https://www.wikidata.org/wiki/Wikidata:Database_download#RDF_dumps
https://www.mediawiki.org/wiki/Wikibase/Indexing/RDF_Dump_Format
https://dumps.wikimedia.org/wikidatawiki/entities/ <----- al fin!!!

mmm aunque ahora que lo analizo solo están los lexemas. mmmm lexemasss??
y el completo pesa 80GB!!! 


ahora pensando en el tema.
en el pdf que baje ultimo me dicen que las urls de wikidata soportan content negociation.
y de estos lexemas tengo los prefijos. lo que concluyo que un rdf de la ontología debería poder obtenerlo de los prefijos.
como hicimos en nuestros ejemplos de los tps.
ej:

@prefix wd: <http://www.wikidata.org/entity/> .

debería poder consultar http://www.wikidata.org/entity/
aceptando y text/turtle y mediante todo lo que explicó casco en la teoría tendría que llegar a algo que 
se asemeje a una ontología para ese prefijo. no? 
pruebo.

pablo@xiaomi:/src/github/facu-infor/twss/entrega5/material$ curl -I http://www.wikidata.org/entity/
HTTP/1.1 301 TLS Redirect
Date: Thu, 03 Jun 2021 16:57:37 GMT
Server: Varnish
X-Varnish: 1073661091
X-Cache: cp1079 int
X-Cache-Status: int-front
Server-Timing: cache;desc="int-front", host;desc="cp1079"
Permissions-Policy: interest-cohort=()
Set-Cookie: WMF-Last-Access=03-Jun-2021;Path=/;HttpOnly;secure;Expires=Mon, 05 Jul 2021 12:00:00 GMT
Set-Cookie: WMF-Last-Access-Global=03-Jun-2021;Path=/;Domain=.wikidata.org;HttpOnly;secure;Expires=Mon, 05 Jul 2021 12:00:00 GMT
X-Client-IP: 191.83.209.88
Location: https://www.wikidata.org/entity/
Content-Length: 0
Connection: keep-alive

pablo@xiaomi:/src/github/facu-infor/twss/entrega5/material$ curl -H 'accept:text/turtle' -I http://www.wikidata.org/entity/
HTTP/1.1 301 TLS Redirect
Date: Thu, 03 Jun 2021 16:57:53 GMT
Server: Varnish
X-Varnish: 1049914519
X-Cache: cp1079 int
X-Cache-Status: int-front
Server-Timing: cache;desc="int-front", host;desc="cp1079"
Permissions-Policy: interest-cohort=()
Set-Cookie: WMF-Last-Access=03-Jun-2021;Path=/;HttpOnly;secure;Expires=Mon, 05 Jul 2021 12:00:00 GMT
Set-Cookie: WMF-Last-Access-Global=03-Jun-2021;Path=/;Domain=.wikidata.org;HttpOnly;secure;Expires=Mon, 05 Jul 2021 12:00:00 GMT
X-Client-IP: 191.83.209.88
Location: https://www.wikidata.org/entity/
Content-Length: 0
Connection: keep-alive

pablo@xiaomi:/src/github/facu-infor/twss$ curl -H 'accept:text/turtle' -I https://www.wikidata.org/entity/
HTTP/2 303 
date: Thu, 03 Jun 2021 17:04:56 GMT
server: mw1267.eqiad.wmnet
location: https://www.wikidata.org/wiki/Special:EntityData/
content-length: 256
content-type: text/html; charset=iso-8859-1
age: 0
x-cache: cp1081 miss, cp1083 pass
x-cache-status: pass
server-timing: cache;desc="pass", host;desc="cp1083"
strict-transport-security: max-age=106384710; includeSubDomains; preload
report-to: { "group": "wm_nel", "max_age": 86400, "endpoints": [{ "url": "https://intake-logging.wikimedia.org/v1/events?stream=w3c.reportingapi.network_error&schema_uri=/w3c/reportingapi/network_error/1.0.0" }] }
nel: { "report_to": "wm_nel", "max_age": 86400, "failure_fraction": 0.05, "success_fraction": 0.0}
permissions-policy: interest-cohort=()
set-cookie: WMF-Last-Access=03-Jun-2021;Path=/;HttpOnly;secure;Expires=Mon, 05 Jul 2021 12:00:00 GMT
set-cookie: WMF-Last-Access-Global=03-Jun-2021;Path=/;Domain=.wikidata.org;HttpOnly;secure;Expires=Mon, 05 Jul 2021 12:00:00 GMT
x-client-ip: 191.83.209.88
set-cookie: GeoIP=AR:B:La_Plata:-34.93:-57.95:v4; Path=/; secure; Domain=.wikidata.org

pablo@xiaomi:/src/github/facu-infor/twss$ curl -H 'accept:text/turtle' -I https://www.wikidata.org/wiki/Special:EntityData/
HTTP/2 200 
date: Thu, 03 Jun 2021 17:05:18 GMT
server: mw1320.eqiad.wmnet
x-content-type-options: nosniff
p3p: CP="See https://www.wikidata.org/wiki/Special:CentralAutoLogin/P3P for more info."
content-language: en
x-frame-options: DENY
vary: Accept-Encoding,Cookie,Authorization
expires: Thu, 01 Jan 1970 00:00:00 GMT
pragma: no-cache
content-type: text/html; charset=UTF-8
age: 0
x-cache: cp1089 miss, cp1083 pass
x-cache-status: pass
server-timing: cache;desc="pass", host;desc="cp1083"
strict-transport-security: max-age=106384710; includeSubDomains; preload
report-to: { "group": "wm_nel", "max_age": 86400, "endpoints": [{ "url": "https://intake-logging.wikimedia.org/v1/events?stream=w3c.reportingapi.network_error&schema_uri=/w3c/reportingapi/network_error/1.0.0" }] }
nel: { "report_to": "wm_nel", "max_age": 86400, "failure_fraction": 0.05, "success_fraction": 0.0}
permissions-policy: interest-cohort=()
set-cookie: WMF-Last-Access=03-Jun-2021;Path=/;HttpOnly;secure;Expires=Mon, 05 Jul 2021 12:00:00 GMT
set-cookie: WMF-Last-Access-Global=03-Jun-2021;Path=/;Domain=.wikidata.org;HttpOnly;secure;Expires=Mon, 05 Jul 2021 12:00:00 GMT
x-client-ip: 191.83.209.88
cache-control: private, s-maxage=0, max-age=0, must-revalidate
set-cookie: GeoIP=AR:B:La_Plata:-34.93:-57.95:v4; Path=/; secure; Domain=.wikidata.org
accept-ranges: bytes


como me retorna 200 entonces me fijo si ese contentido me retorna algo parecido a un turtle.
no no.
y chequeo lo que explicó casco en el foro, por ahí a veces está incluído en el html mismo un link especificando el contenido en rdf, como en foaf.
y veo que si. pero a una entidad en particular!!! no a la ontología de ese prefijo!!

https://www.wikidata.org/wiki/Special:EntityData/

contenido: 

Entity Data
Jump to navigationJump to search
This page provides a linked data interface to entity values. Please provide the entity ID in the URL, using subpage syntax.

Content negotiation applies based on you client's Accept header. This means that the entity data will be provided in the format preferred by your client. For a web browser, this will be HTML, causing your browser to be redirected to the regular entity page.
You can explicitly request a specific data format by adding the appropriate file extension to the entity ID: Q23.json will return data in the JSON format, Q23.ttl will return RDF/Turtle, and so on. Supported formats are: json, php, n3, ttl, nt, rdf, jsonld, html.
For more information about this special page, read Wikibase/EntityData.

Return to Wikidata:Main Page.

----

es como el huevo y la gallina. no quiero una entidad, quiero la ontología que define como definimos la entidad!!.
la class!!! el rdf:type.
jajaj 

un poco esto es a lo que me refería cuando pensaba de las diferencias entre dbpedia y wikidata.
wikidata es demasiado abstracto.
ya veo que investigando mas encuentro que la ontología de wikidata es una entidad en si!!!
mi cabeza se siente como cuando vió la película inceptión la primera vez.

----


arranco a hacer las consutlas y despues vuelvo sobre las preuntas de teoría.


7. Adapte las queries que construyo en los puntos c y d del ejercicio anterior en el endpoint de Wikidata. (https://query.wikidata.org). ¿Obtuvo resultados diferentes? Si la respuesta es si, ¿a que se deben?

c -  la consulta que tenía para dbpedia.

SELECT DISTINCT ?s
			WHERE
			{
                ?s dbo:birthPlace ?pais .
                ?s rdf:type dbo:Writer .
                filter (?pais = dbr:Argentina || ?pais = dbr:Uruguay) .
			}


la adapto textualmente pero usando las propieades de wikidata que me encuentro en el endpoint de sparql.

SELECT DISTINCT ?s
			WHERE
			{
                ?s wdt:P19 ?pais .
                ?s rdf:type wd:Q36180 .
                filter (?pais = wd:Q414 || ?pais = wd:Q77) .
			}


P19 = place of birth
Q36180 = person who uses written words ...
Q414 = Argentina
Q77 = Uruguay

claramente no me retorna nada de nada.
ya que la estructura de wikidata es distinta.
asumo que tiene algo onda una sentencia armada que dice 

persona --> statement ---> pais 
                           referencia
        --> statement ---> escritor 
                           referencia

voy a investigar como hice con schema:about para tratar de identificar la estructura que pararece en wikidata


SELECT DISTINCT ?s ?p
			WHERE
			{
                ?s wdt:P19 ?p .
			}
            limit 10

 wd:Q72871	 wd:Q16
 wd:Q92679	 wd:Q16
 wd:Q92996	 wd:Q16
 wd:Q186042	 wd:Q16
 wd:Q209102	 wd:Q15

mmmm efectivamente entonces Q16 = Canada y los demas son personas. asi que se puede obtener directamente
el problema entonces es el rdf:type que no debe estar asociado asi. no debe usar rdf:type. 
a ver obtengo todas las tripletas para wd:Q16 a ver como define el type.

SELECT DISTINCT ?p ?o
			WHERE
			{
                wd:Q16 ?p ?o .
			}
            limit 10

schema:description	дзяржава ў Паўночнай Амерыцы
schema:description	stat sovran fl-Amerka ta’ Fuq
rdf:type	wdno:P3075
schema:description	उत्तर अमेरिकाको देश    

si si usa rdf:type para definir el tipo de entidad.

analizo a ver si alguna tripleta tiene a escritor como sujeto primero.

SELECT DISTINCT ?p ?o
			WHERE
			{
                wd:Q36180 ?p ?o .
			}
            limit 10

schema:description	nebonan a skrif
schema:description	mestê
rdfs:label	escritor
rdfs:label	schrijver
rdfs:label	作家
rdfs:label	writer
rdfs:label	scríbhneoir
rdfs:label	scrittore
rdfs:label	escritor


copado. tengo si los datos de la entidad escritor. 
ahora veo lo inverso, que tipo de entidades relacionan escritores. chan chan. serán sentencias?

SELECT DISTINCT ?s ?p
			WHERE
			{
                ?s ?p wd:Q36180 .
			}
            limit 10

wds:q1868-33C22E5B-1754-45DB-86DD-A629690EF913	ps:P106
wds:Q42-663C6872-388F-4024-995A-BC3C1C0E57C6	ps:P106
wds:Q1058-b781d2ba-45b3-a044-ca7d-76df042e1869	ps:P106
wds:Q1151-44DC241E-A4B1-4B0C-A662-7FBC3BFD88E1	ps:P106
wds:Q501-A1597288-D3C5-4D56-B8E7-5924DEBBCB12	ps:P106
wds:Q747-705D3730-CFB1-4127-ADD1-98F58119734F	ps:P106

SIII. veo que mas o menos estoy entendiendo el equema de datos de wikidata.
wds = prefijo de sentencias.

ahora quiero analizar la estructura de una de las sentencias por ejemplo.

SELECT DISTINCT ?p ?o
			WHERE
			{
                wds:q1868-33C22E5B-1754-45DB-86DD-A629690EF913 ?p ?o .
			}
            limit 10


wikibase:rank	wikibase:NormalRank
rdf:type	wikibase:BestRank
ps:P106	 wd:Q36180            

ahora solo tendría que ver a ver si las personas se asocian directamente con sentencias.


SELECT DISTINCT ?s ?p
			WHERE
			{
                ?s ?p wds:q1868-33C22E5B-1754-45DB-86DD-A629690EF913 .
			}
            limit 10

wd:Q1868	p:P106

efectivamente.
wd:Q1868 ---> una persona
p:P106 -----> ocupación.


entonces en base a ese análisis, el grafo que estoy consultando seria algo así:

persona --> p:P106 (ocupación) --> sentencia (wds:q1868-33C22E5B-1754-45DB-86DD-A629690EF913) --> p:P106	(ocupación) -----> wd:Q36180 (escritor)


SELECT DISTINCT ?per
			WHERE
			{
                ?per p:P106 ?s .
                ?s p:P106 wd:Q36180 .
			}
            limit 10


mmmmmmmmmmmmmmmmm algo raro hay porque no me trae nada!!!.

despues de pruebas y pruebas me acabo de dar cuenta del prefix!!!!
P106 está en 2 prefijos.
 PREFIX ps: <http://www.wikidata.org/prop/statement/>
 PREFIX p: <http://www.wikidata.org/prop/>


ajusto la consulta.


SELECT DISTINCT ?per
			WHERE
			{
                ?per p:P106 ?s .
                ?s ps:P106 wd:Q36180 .
			}
            limit 10


siiii perfecto. eso me trae personas que tienen de ocupación escritores!!.

por lo que podría adaptar la consulta original a :


SELECT DISTINCT ?per
			WHERE
			{
               ?per p:P106 ?s .
               ?s ps:P106 wd:Q36180 .
               ?per wdt:P19 ?pais .
               filter (?pais = wd:Q414 || ?pais = wd:Q77) .
			}
            limit 10

yessss me trae escritores argentines y uruguayes.

ahora la consulta para ver con el pais sin el limit a ver cuantos son.

SELECT DISTINCT ?per ?pais
			WHERE
			{
               ?per p:P106 ?s .
               ?s ps:P106 wd:Q36180 .
               ?per wdt:P19 ?pais .
               filter (?pais = wd:Q414 || ?pais = wd:Q77) .
			}


me trajo 271 resultados.
la dejo para que responda a la adapación de la pregunta original.

SELECT DISTINCT ?per
	WHERE
			{
               ?per p:P106 ?s .
               ?s ps:P106 wd:Q36180 .
               ?per wdt:P19 ?pais .
               filter (?pais = wd:Q414 || ?pais = wd:Q77) .
			}


--
bien ahora analizo el tema de la misma consulta pero usando union

d) Utilizando el keyword union (vea sección 6.3.2.6 del libro), obtener a los escritores que hayan nacido en una ciudad de Argentina o de Uruguay


la consulta original que había realizado para dbpedia es :

SELECT DISTINCT ?s
			WHERE
			{
                ?s rdf:type dbo:Writer .
                { ?s dbo:birthPlace dbr:Argentina . }
                UNION 
                { ?s dbo:birthPlace dbr:Uruguay .}
			}


entonces la adaptación sería muy parecida.

SELECT DISTINCT ?per
	WHERE
	{
        ?per p:P106 ?s .
        ?s ps:P106 wd:Q36180 .
        { ?per wdt:P19 wd:WQ414 . }
        UNION
        { ?per wdt:P19 wd:Q77 . }
	}

me da 229 escritores???!!! menos resultados!!.

vuelvo a correr la consulta del filter para ver si me tira ok los resultados?.

SELECT DISTINCT ?per
	WHERE
			{
               ?per p:P106 ?s .
               ?s ps:P106 wd:Q36180 .
               ?per wdt:P19 ?pais .
               filter (?pais = wd:Q414 || ?pais = wd:Q77) .
			}

me responde 271 resultados.
ahora vuelvo a correr la consulta del union y no me responde nada de nada!! mmm.

---

espere unos minutos y volví a correr la consulta con union y veo que ahora si me tira resultados!!! 
pero solo 229


descargue los 2 resultados y los guarde como csv en la parte de respuestas.
igual quiero analizar por que este me trae 42 escritores menos!!!.


---

ME quiero matar de nuevo!!!.
tenía error de sintaxis. le puse una W en argentina.

SELECT DISTINCT ?per
	WHERE
	{
        ?per p:P106 ?s .
        ?s ps:P106 wd:Q36180 .
        { ?per wdt:P19 wd:Q414 . }
        UNION
        { ?per wdt:P19 wd:Q77 . }
	}

ahora si me da ok 271 escritores correctamente!!!




----


bueno ahora que tengo un poco mas analizado todo. voy a asumir que las conusltas de arriba están ok y voy a arrancar con la parde de desarrollo. 
así por lo menos me aseguro de que puedo llegar a la fecha de entrega con algo.
total ni bien tengo algo repaso todo y actualizo las consultas que tenga que actualizar.

y todavia me falta entender todo el proceso de scrapeo de la wikipedia hacia wikidata.
que no encontre nada de eso hasta ahora.

-----

idea inicial del código para hacer pruebas.
hacer un script que tome las personas de mi dataset y busque en dbpedia para aumentar los datos.
almacene esos datos en otro dataset. dbpedia_aumentado.ttl

personas_dbpedia.py

bien, era simple obtener las personas de mi dataset. ahora voy a agregarle para consultar el endpoint de dbpedia.

---

bien hago una prueba en el endpoint de dbpedia para ver si obtengo algo de aunque sea un nombre

select distinct ?s ?p 
where {
     ?s ?p "Paul Briggs"@eng .
}

nada de nada. algo me estoy confundiendo porque claramente tengo que 

https://dbpedia.org/page/Paul_Briggs_(animator)

dbp:name	Paul Briggs (en)

o sea que aunque sea esa tripleta tendría que retornar.

select distinct ?s
where {
     ?s dbp:name "Paul Briggs"@eng .
}

tampoco.
claramente estoy re boleado, voy a repasar lo que hice de la dbpedia para analizar un poco como eran las consultas.

select distinct ?s
where {
     ?s dbp:name "Paul Briggs"@en .
}

http://dbpedia.org/resource/Paul_Briggs_(American_football)
http://dbpedia.org/resource/Paul_Briggs_(animator)
http://dbpedia.org/resource/Paul_Briggs_(boxer)

que nabo. ahora si. 
veo que me tira 3 recursos. como se que tengo actores entonces voy a filtrar por ocupaciones.

pensamiento random.
veo que la ocupación del animador (el que estoy buscando yo) tiene un dbr:person_function_paul_briggs generado dentro de dbpedia.
o sea que es proceso de la imporación de wikipedia, y le genera la occupation como una entidad específica de ese actor.

dbo:occupation dbr:Paul_Briggs_(animator)__PersonFunction__1

los datos de esa entidad en dbpedia son:

dbo:title	                  Animator, voice actor (en)
rdf:type                    owl:Thing, dbo:PersonFunction 
is dbo:occupation of	      dbr:Paul_Briggs_(animator)

obviamente importada de : https://en.wikipedia.org/wiki/Paul_Briggs_(animator)
veo que el infobox de la wikipedia tiene:

Paul Briggs
Born	December 17, 1974 (age 46)
San Antonio, Texas
Nationality	American
Education	Kansas City Art Institute
Occupation	Animator, voice actor
Years active	1997–present
Employer	Walt Disney Animation Studios (1997-present)
Known for	Head of story on Frozen, Big Hero 6 and Zootopia

y mediante los mapeos y extracciones de las infobox de dbpedia va a parar al namespace "dbo"

otra cosa que veo asociado en dbpedia es que dbp:occupation = Animator, voice actor
claramente extraído mediante los extractores NLP y en vez de crearse una entidad se asocia al string.

ahora el tema es que se estaría creando entidades duplicadas para todos los animadores y voiceactors.
conclusión, al no estar asociado a una entidad de wikidata en el infobox, dbpedia no interpreta la ocupación como una entidad compartida si no que 
crea una entidade specifica para esa ocupación solamente de la persona.
sería lo mismo que crearle un blank node para la ocupación de la persona. algo como (en pseudocódigo):

Paul_Briggs dbo:occupation [
    rdf:type dbr:person_function;
    rdf:label "Animator, voice actor"
]

algo así es lo que almacena dbpedia.
en cambio si en el infobox de Paul_Briggs le asocio en la parte de ocupación una entidad Qnumber como 
https://www.wikidata.org/wiki/Q266569
el extractor de dbpedia asociaría dentro del mismo namespace ahora si una entidad específica compartida entre todos los recursos
que tengan esa ocupación asociada mediante las infoboxes.

solo cosas que voy notando para relacionar conceptos.

--

teniendo en cuenta esto me conviene filtrar los recursos por dbp:occupation inicialmente.
por algunos strings.
y depsues expandir ese filtro por entidades dbo para ocupaciones reales mapeadas, si no es suficiente lo anterior.

hago la prueba a ver si funciona.

--

bien haciendo una prueba funcionó bien.
ahora corrijo la consulta para que me traiga todas las tripletas de la persona con cierto nombre.

            PREFIX dbp: <http://dbpedia.org/property/>
            select distinct ?s ?p ?o
            where {
                ?s ?p ?o .
                ?s dbp:name \"""" + name + """\"@en .
            }        
        """)

encontro uno del dataset y encontro todas las tripletas asociadas correctamente.

---

ahora voy a obtener todas las ocupaciones de las personas que tengo en mi dataset.
para analizar los datos que tiene dbpedia y wikipedia.

pablo@xiaomi:/src/github/facu-infor/twss/entrega5$ python3 src/personas_dbpedia.py 
obteniendo Eiza González
{'s': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Eiza_González'}, 'oc': {'type': 'literal', 'xml:lang': 'en', 'value': ''}}

{'s': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Eiza_González'}, 'oc': {'type': 'literal', 'xml:lang': 'en', 'value': 'singer'}}
singer
{'s': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Eiza_González'}, 'oc': {'type': 'literal', 'xml:lang': 'en', 'value': 'Actress'}}
Actress

bueno de esta prueba obtengo que puede tener varios resultados (obviamente porque son varias tripletas) y tambien que algunos resultados 
son nulos o sea no tienen nada.

---
las organizo dentro de un set y corro de nuevo el script.

pablo@xiaomi:/src/github/facu-infor/twss/entrega5$ python3 src/personas_dbpedia.py 
obteniendo Josh Boone
{'Filmmaker'}

--

bien parece funcionar. descomento el break y corro para todos los nombres de mi dataset

funcionó.

pablo@xiaomi:/src/github/facu-infor/twss/entrega5$ python3 src/personas_dbpedia.py 
obteniendo Gabriella Wilde
obteniendo Christopher Nolan
obteniendo Satoshi Hino
obteniendo Chiara Mastroianni
obteniendo Mark Meszoros
obteniendo Carey Mulligan
obteniendo Haruo Sotozaki
obteniendo Valerie Kalfrin
obteniendo Michael Gwisdek
obteniendo Subrat Dutta
obteniendo Pablo Rago
obteniendo Stéphane Roger
obteniendo Paul Briggs
obteniendo Geoff Johns
obteniendo Camille Cottin
obteniendo Anthony Ramos
obteniendo Doug Liman
obteniendo Rick Kelvin Branch
obteniendo Rafer Guzman
obteniendo Lynda Carter
obteniendo Natalia Oreiro
obteniendo Christopher Mintz-Plasse
obteniendo Amy Nicholson
obteniendo Brian Tyree Henry
obteniendo Lolita Marie
obteniendo Stacha Hicks
obteniendo Carlos Lopez Estrada
obteniendo David Sims
obteniendo Tomokazu Seki
obteniendo Sam Richardson
obteniendo Frank Welker
obteniendo Henry Zaga
obteniendo Alison Brie
obteniendo David Bax
obteniendo Jennifer Coolidge
obteniendo June Foray
obteniendo Martin Schreier
obteniendo Natsuki Hanae
obteniendo Bobby Cannavale
obteniendo Jamal Alkhaldi
obteniendo Mercedes Burgos
obteniendo animación
obteniendo Brian Gill
obteniendo Kaila Ingram
obteniendo Happy Anderson
obteniendo Sia Alipour
obteniendo John McDonald
obteniendo Brittney Aleah
obteniendo Amr Waked
obteniendo Harrison Arevalo
obteniendo Lyon Beckwith
obteniendo Manoj Anand
obteniendo Bo Burnham
obteniendo Jeff Moore
obteniendo Emerald Fennell
obteniendo Andrew Ellis
obteniendo Anya Taylor-Joy
obteniendo Lilly Aspell
obteniendo Ella Walker
obteniendo Jeffrey Corazzini
obteniendo Tom Holland
obteniendo Dimple Kapadia
obteniendo Ian Kenny
obteniendo Bern Collaco
obteniendo Dan Tabor
obteniendo Ravi Patel
obteniendo Penelope Kapudija
obteniendo Chris Silcox
obteniendo Lil Rel Howery
obteniendo Millie Bobby Brown
obteniendo Nina Garin
obteniendo Daniel Dae Kim
obteniendo Saori Hayami
obteniendo Clancy Brown
obteniendo Sam Rockwell
obteniendo Sonia Goswami
obteniendo Jake Wilson
obteniendo Vincent Lacoste
obteniendo Pedro Pascal
obteniendo Michael Caine
obteniendo Chloë Grace Moretz
obteniendo Clemence Poésy
obteniendo Takahiro Sakurai
obteniendo Robin Wright
obteniendo Marisol Correa
obteniendo Elizabeth Debicki
obteniendo Wilfried Hochholdinger
obteniendo jayg_58
obteniendo Patty Jenkins
obteniendo Shun Oguri
obteniendo Laverne Cox
obteniendo Mickey Gilmore
obteniendo Joel Morris
obteniendo Mel Blanc
obteniendo Aaron Taylor-Johnson
obteniendo Andrew Galdi
obteniendo James Corden
obteniendo Rebecca Hall
obteniendo Stephen Romei
obteniendo Vanessa Curry
obteniendo Demian Bichir
obteniendo Natasha Rothwell
obteniendo Julian Ferro
obteniendo John David Washington Kenneth Branagh
obteniendo Robert Pattinson
obteniendo Laura López Moyano
obteniendo Amreen N. Yaqoob
obteniendo Ray McKinnon
obteniendo Molly Shannon
obteniendo Kunal Nayyar
obteniendo Kurt Sutter
obteniendo Spencer Trinwith
obteniendo Jake Curran
obteniendo Julian Dennison
obteniendo Marla Aaron Wapner
obteniendo William Moulton Marston
obteniendo Jonathan Ajayi
obteniendo Vikram Jayakumar
obteniendo Heiner Lauterbach
obteniendo Alexander Skarsgård
obteniendo Nick Jonas
obteniendo Óscar Jaenada
obteniendo Justin Sisk
obteniendo Mads Mikkelsen
obteniendo Dennis Mojen
obteniendo Rosario Dawson
obteniendo Hernán Jiménez
obteniendo Nicky Jam
obteniendo Himesh Patel
obteniendo Sarah Ward
obteniendo Ken Duken
obteniendo Christophe Honoré
obteniendo Jennifer Ouellette
obteniendo Eiza González
obteniendo Sahil Shroff
obteniendo Dave Callaham
obteniendo Julius Berg
obteniendo Adam Wingard
obteniendo John David Washington
obteniendo Kyle Chandler
obteniendo César Bordón
obteniendo Kenneth Branagh
obteniendo Marie-Christine Adam
obteniendo Alice Braga
obteniendo Matt Neal
obteniendo Nikolai Kinski
obteniendo Isabela Palópoli
obteniendo Carole Bouquet
obteniendo Chance the Rapper
obteniendo Monica Haynes
obteniendo Kenichi Suzumura
obteniendo Tony Medley
obteniendo Benjamin Biolay
obteniendo Josh Boone
obteniendo Gastón Portal
obteniendo Diego Peretti
obteniendo Chris Pine
obteniendo Jamaal Burcher
obteniendo Daisy Ridley
obteniendo Michael Peña
obteniendo Martín Desalvo
obteniendo Sheri Flanders
obteniendo Blu Hunt
obteniendo Oakley Bull
obteniendo Adam Brody
obteniendo Emilia Schüle
obteniendo Don Hall
obteniendo Pablo Echarri
obteniendo David Oyelowo
obteniendo Ian Simmons
obteniendo Alberto Ammann
obteniendo William Hanna
obteniendo Maisie Williams
obteniendo Rita Tushingham
obteniendo Gal Gadot
obteniendo Mark Jackson
obteniendo Connie Britton
obteniendo Esteban Bigliardi
obteniendo Akari Kito
obteniendo Alex Delescu
obteniendo Justin Timberlake
obteniendo Thomas Clay Strickland
obteniendo Tessa Bonham Jones
obteniendo Sylvester McCoy
obteniendo Connie Nielsen
obteniendo Max Greenfield
obteniendo Martin Donovan
obteniendo Cynthia Erivo
obteniendo Katsuyuki Konishi
obteniendo Tony Cervone
obteniendo Yoshitsugu Matsuoka
obteniendo Walt Dohrn
obteniendo Kevin Carr
obteniendo Thomas Kee
obteniendo Avaryl Halley
obteniendo Mora Recalde
obteniendo Karan Soni
obteniendo Demián Bichir
obteniendo Charlie Heaton
obteniendo David P Smith
obteniendo Paul Ron Cruz
obteniendo Kristoffer Polaha
obteniendo Eddie Harrison
obteniendo Eric Marchen
obteniendo Jamie Dornan
obteniendo Tim Story
obteniendo Antonio Banderas
obteniendo Hiro Shimono
obteniendo Jessica Henwick
obteniendo John Gettier
obteniendo T-Pain
obteniendo Awkwafina
obteniendo Kelly Marie Tran
obteniendo Flula Borg
obteniendo Anna Kendrick
obteniendo Kristen Wiig
{'actor', 'Actress, singer, songwriter, beauty pageant titleholder', 'Animator, voice actor', 'Actor, comedian', 'Photographer', 'impressionist', 'record producer', 'Animator', 'Author, Poet, Writer', 'Animator, producer, director, screenwriter', 'Inventor', 'Actress, model', 'Journalist', 'Actor, producer', 'American sportswriter', 'Actor and musician', 'internet entrepreneur', 'Rapper', 'Film director, screenwriter, producer, actor, model', 'screenwriter', 'Actress & Writer', 'television host', 'http://dbpedia.org/resource/Audio_engineer', 'Film director, producer, screenwriter, editor, cinematographer, composer', 'director', 'Actor, voice actor, producer', 'Screenwriter, director, producer, actor, script consultant', 'Animator, producer, director, screenwriter, voice actor', 'musician', 'Writer', 'Film director', 'writer', 'http://dbpedia.org/resource/Master_of_ceremonies', 'philanthropist', 'http://dbpedia.org/resource/Voice_acting_in_Japan', 'voice actor', 'http://dbpedia.org/resource/Rapping', 'Filmmaker', 'http://dbpedia.org/resource/Hype_man', 'http://dbpedia.org/resource/Actor', 'Producer', 'Actor, model, musician', 'DJ', 'rapper', 'cartoonist', 'Film director, writer, voice actor', 'Singer', 'Screenwriter, director, producer, actor', 'Singer, songwriter, actress, model', 'Screenwriter', 'Psychologist', 'Voice actress', 'Actress, director', 'Actor', 'Film director, producer', 'Voice actor', 'singer', 'http://dbpedia.org/resource/Theatre_director', 'Writer and editor', 'Actor, producer, director', 'stand-up comedian', 'producer', 'Actor, writer, comedian', 'voice actress', 'Voice actor, radio personality', 'Actress, singer, songwriter', 'Actor, film director', 'Director, producer', 'comedian', 'Actor, physical comedian', 'film producer', 'Actress', 'Film director, screenwriter, actor, comedian', 'Film director, writer', 'narrator', 'Actor, Model', 'http://dbpedia.org/resource/Author', 'http://dbpedia.org/resource/List_of_YouTubers', 'Poet, Farmer, Schoolteacher', 'Writer, director, animator, musician, voice actor', 'activist', 'Model, dancer, singer-songwriter', 'Director', 'songwriter', 'Rugby union referee'}

---

ahora entonces uso esa colección para filtrar las ocupaciones que realmente me interesan para mi dataset.
puedo tomar esta estrategia porque son pocas personas y realizar las consutlas para obtener todo el conjunto de datos existente
de dbpedia.
pero obviamente esta solución no es escalable!!! para nada.
debería poder obtener ocupaciones de los dataset de dbpedia o wikipedia que identifiquen actores o directores y 
usar esos datos para filtrar mis consultas.

lo hago bien casero!!.
filtro a mano los datos y corrijo el script para que las ocupaciones se encuentren dentro de ese conjunto.

primero almaceno las ocupaciones en un json para poder editarlas.
creo el script download_dbpedia_occupations.py

---

corri el script y descargó todas las ocupaciones de las personas que encontró con esos nombres.
la edité a mano y la deje en data/
el original descargado quedó con postfijo _original.json

---

hago una prueba con el script, primero obtengo todas las tripletas de la persona con cierto nombre y 
chequearía si la tripleta que define la ocupación se encuentra dentro de las ocupaciones editadas y almacenadas en el json.
pero al ejecutar el script tomo un nombre y tiró miles de tripletas!!.
así que claramente no es efectivo. voy a realizarlo en 2 partes al proceso.
primero filtrar por los subjects que tengan las ocupaciones correctas, y despues usando esos subjects obtener si las tripletas asociados a ellos.
asi por lo menos la cantidad de datos que traigo son de los subjects correctos.

---

bueno efectivamente funciona.

pablo@xiaomi:/src/github/facu-infor/twss/entrega5$ python3 src/personas_dbpedia.py 
obteniendo Kenichi Suzumura
obteniendo datos de http://dbpedia.org/resource/Kenichi_Suzumura
{'p': {'type': 'uri', 'value': 'http://www.w3.org/1999/02/22-rdf-syntax-ns#type'}, 'o': {'type': 'uri', 'value': 'http://www.w3.org/2002/07/owl#Thing'}}

puse un break asi no me tira todos los datos.

ahora modifico el script para poder relacionar mis subjects con los encontrados.

---


ejecuto la prueba y veo que funciona ok.
por ejmeplo tom Holland encontró las 3 entidades. una como director, otro como actor y otro como author.

pablo@xiaomi:/src/github/facu-infor/twss/entrega5$ python3 src/personas_dbpedia.py 
obteniendo Jonathan Ajayi
entidades externas encontradas set()
obteniendo Connie Britton
entidades externas encontradas {'http://dbpedia.org/resource/Connie_Britton'}
obteniendo Rebecca Hall
entidades externas encontradas {'http://dbpedia.org/resource/Rebecca_Hall'}
obteniendo Dimple Kapadia
entidades externas encontradas {'http://dbpedia.org/resource/Dimple_Kapadia'}
obteniendo Justin Timberlake
entidades externas encontradas set()
obteniendo Mads Mikkelsen
entidades externas encontradas {'http://dbpedia.org/resource/Mads_Mikkelsen'}
obteniendo Doug Liman
entidades externas encontradas {'http://dbpedia.org/resource/Doug_Liman'}
obteniendo Chloë Grace Moretz
entidades externas encontradas {'http://dbpedia.org/resource/Chloë_Grace_Moretz'}
obteniendo Ella Walker
entidades externas encontradas set()
obteniendo Michael Caine
entidades externas encontradas {'http://dbpedia.org/resource/Michael_Caine'}
obteniendo Patty Jenkins
entidades externas encontradas {'http://dbpedia.org/resource/Patty_Jenkins'}
obteniendo Stéphane Roger
entidades externas encontradas set()
obteniendo Brittney Aleah
entidades externas encontradas set()
obteniendo Himesh Patel
entidades externas encontradas {'http://dbpedia.org/resource/Himesh_Patel'}
obteniendo Diego Peretti
entidades externas encontradas set()
obteniendo Antonio Banderas
entidades externas encontradas {'http://dbpedia.org/resource/Antonio_Banderas'}
obteniendo Adam Wingard
entidades externas encontradas {'http://dbpedia.org/resource/Adam_Wingard'}
obteniendo Jeffrey Corazzini
entidades externas encontradas set()
obteniendo Elizabeth Debicki
entidades externas encontradas {'http://dbpedia.org/resource/Elizabeth_Debicki'}
obteniendo Vincent Lacoste
entidades externas encontradas {'http://dbpedia.org/resource/Vincent_Lacoste'}
obteniendo Brian Gill
entidades externas encontradas set()
obteniendo Monica Haynes
entidades externas encontradas set()
obteniendo Dennis Mojen
entidades externas encontradas set()
obteniendo Benjamin Biolay
entidades externas encontradas {'http://dbpedia.org/resource/Benjamin_Biolay'}
obteniendo Pablo Rago
entidades externas encontradas {'http://dbpedia.org/resource/Pablo_Rago'}
obteniendo John Gettier
entidades externas encontradas set()
obteniendo Nikolai Kinski
entidades externas encontradas {'http://dbpedia.org/resource/Nikolai_Kinski'}
obteniendo Thomas Clay Strickland
entidades externas encontradas set()
obteniendo Kaila Ingram
entidades externas encontradas set()
obteniendo Tom Holland
entidades externas encontradas {'http://dbpedia.org/resource/Tom_Holland_(director)', 'http://dbpedia.org/resource/Tom_Holland_(actor)', 'http://dbpedia.org/resource/Tom_Holland_(author)'}
obteniendo Rick Kelvin Branch
entidades externas encontradas set()
obteniendo Lynda Carter
entidades externas encontradas {'http://dbpedia.org/resource/Lynda_Carter'}
obteniendo Manoj Anand
entidades externas encontradas set()

---

ahora pongo un break para probar con una persona si el resto del código funciona.

pablo@xiaomi:/src/github/facu-infor/twss/entrega5$ python3 src/personas_dbpedia.py 
obteniendo Charlie Heaton
entidades externas encontradas {'http://dbpedia.org/resource/Charlie_Heaton'}
obteniendo datos de http://dbpedia.org/resource/Charlie_Heaton
my subject : https://raw.githubusercontent.com/pablodanielrey/twss/master/owl/data/Charlie_Heaton 
external subject : http://dbpedia.org/resource/Charlie_Heaton
{'p': {'type': 'uri', 'value': 'http://www.w3.org/1999/02/22-rdf-syntax-ns#type'}, 'o': {'type': 'uri', 'value': 'http://www.w3.org/2002/07/owl#Thing'}}


perfecto, corrió ok y trajo tripletas asociadas al subject externo a mi dataset.
creo que ya estaría todo el script de dbpedia.
el de wikipedia debería ser sencillo de obtener.

---

ahora voy a tratar de agregar todas esas tripletas en un grafo rdf para después poder serializarlo en turtle o el formato que elija.

analizando un poco lo que tengo que hacer acabo de leer en el libro que nos dió diego
CONSTRUCT!!!!.

6.2.3
CONSTRUCT Query
construct query is another query form provided by SPARQL which, instead of
returning a collection of query solutions, returns a new RDF graph. Let us take a
look at some examples.

In fact, a common use of the construct query form is to transform a given graph to
a new graph that uses a different ontology.

ahi está la respuesta!!!!
y la librería que estoy usando posiblemente me retorne un grafo rdflib!!. joooooo.
no tengo que hacer nada de procesamiento del tema.
voy a hacer una prueba a ver si funciona.

---

no me funciona con construct y quiero tener algo funcionando hoy que rindo el jueves ingeniería y tengo que estudiar.
asi que lo voy a hacer con select y después lo corrijo. de paso me sirve para comparar soluciones.

---


bueno la primer prueba que hago salió andando.
era mas simple de lo que pensaba.

pablo@xiaomi:/src/github/facu-infor/twss/entrega5$ python3 src/personas_dbpedia.py 
obteniendo Natasha Rothwell
entidades externas encontradas {'http://dbpedia.org/resource/Natasha_Rothwell'}
obteniendo datos de http://dbpedia.org/resource/Natasha_Rothwell
my subject : https://raw.githubusercontent.com/pablodanielrey/twss/master/owl/data/Natasha_Rothwell 
external subject : http://dbpedia.org/resource/Natasha_Rothwell
{'p': {'type': 'uri', 'value': 'http://www.w3.org/1999/02/22-rdf-syntax-ns#type'}, 'o': {'type': 'uri', 'value': 'http://www.w3.org/2002/07/owl#Thing'}}

<https://raw.githubusercontent.com/pablodanielrey/twss/master/owl/data/Natasha_Rothwell> a <http://www.w3.org/2002/07/owl#Thing> .


solo lo probé con una sola propiedad para ver si funcionaba.
le saco los break para ver si aunque sea me serializa una persona completa.


---

funciona perfectoooooo.


pablo@xiaomi:/src/github/facu-infor/twss/entrega5$ python3 src/personas_dbpedia.py 
obteniendo David Oyelowo
entidades externas encontradas {'http://dbpedia.org/resource/David_Oyelowo'}
obteniendo datos de http://dbpedia.org/resource/David_Oyelowo
@prefix ns1: <http://dbpedia.org/ontology/> .
@prefix ns2: <http://purl.org/dc/terms/> .
@prefix ns3: <http://dbpedia.org/property/> .
@prefix ns4: <http://schema.org/> .
@prefix ns5: <http://www.w3.org/2002/07/owl#> .
@prefix ns6: <http://xmlns.com/foaf/0.1/> .
@prefix ns7: <http://www.w3.org/ns/prov#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .

<https://raw.githubusercontent.com/pablodanielrey/twss/master/owl/data/David_Oyelowo> a <http://dbpedia.org/class/yago/Absentee109757653>,
        <http://dbpedia.org/class/yago/Actor109765278>,
        <http://dbpedia.org/class/yago/Alumnus109786338>,
        <http://dbpedia.org/class/yago/Articulator109811712>,
        <http://dbpedia.org/class/yago/CausalAgent100007347>,
        <http://dbpedia.org/class/yago/Communicator109610660>,
        <http://dbpedia.org/class/yago/Emigrant110051975>,
        <http://dbpedia.org/class/yago/Entertainer109616922>,
        <http://dbpedia.org/class/yago/Exile110071332>,
        <http://dbpedia.org/class/yago/Intellectual109621545>,
        <http://dbpedia.org/class/yago/LivingThing100004258>,
        <http://dbpedia.org/class/yago/Migrant110314952>,
        <http://dbpedia.org/class/yago/Narrator110345804>,
        <http://dbpedia.org/class/yago/Object100002684>,
        <http://dbpedia.org/class/yago/Organism100004475>,
        <http://dbpedia.org/class/yago/Performer110415638>,
        <http://dbpedia.org/class/yago/Person100007846>,
        <http://dbpedia.org/class/yago/PhysicalEntity100001930>,
        <http://dbpedia.org/class/yago/Scholar11055785

        .....



me quede pensando en construct, y como transformar grafos a otros namespaces.
es una herramineta muy poderosa esto de la web semántica!!.
a media que hago pruebas me estoy dando cuenta de que lo que quería obtener de la materia ya está mas que cumplido.
y me estoy convenciendo que el poder de estas herramientas que tienen para combinar datos de distintos datasets es IMPRESIONANTE!!.
además de poder hacer evolucionar una ontología inicial transformandola posteriormente e ir adaptandola es también una de las grandes ventajas por
sobre otras tecnologías.
inclusive sobre las bases de datos orientadas a documentos.

en este punto, sin haber probado mucho, teniendo muy poca experiencia ya estoy convencido de es la herramienta correcta 
para usar en mi laburo. como mínimo se merece un prototipo para evaluar la viabilidad de una solucion usando estas herramientas.
por sobre bases de datos relacionales.

----


acomodo el script para escribir las tripletas en un grafo serializado a turtle dentro de data.
le saco todos los breaks al código y lo disparo para todos las personas que tengo.

---

lo ejecuté y funcionó perfecto.
asi que veo a ver con protegé a ver si me genero algo válido.
editandolo con el editor de archivos lo veo válido.

lo veo bien. asi que perfecto.

---

agrego un bind_schemas par que quede un poquito mas legible el turtle de dbpedia.

--











