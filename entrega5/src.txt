
El proceso se divide en varias etapas.
un poco inspirado en el tp anterior, donde links.ttl identificaban las iris que debíamos desreferenciar.
ahora hago lo mismo pero para desreferenciar y obtener los datos uso sparql.
también pensando a que la etapa de identificación de: que entidad debería acceder en el datasource externo, y el proceso de 
obtener esos datos son distintos.


etapa1 :
se usa sparql dentro de cada uno de los servicios para filtrar e identificar las iris de los recursos asociados a las personas de mi dataset. 
en principio existen 2 scripts.
personas_dbpedia.py
personas_wikidata.py

en esta etapa se encuentran todas las entidades dentro de cada dataset, que correspondem a cada persona de mi dataset.
en el caso de dbpedia existe un filtrado manual de las ocupaciones posibles.
inclusive en el namespace mas formal y chequeado de dbpedia (dbo vs dbp), me encuentro con que no tengo las ocupaciones usando alguna clasificación.
mas bien dbpedia les asigna un PersonFunction. con lo cual es complejo analizar si es una ocupación de la industria cinematográfica.
asi que decidí tener un script intermedio inicial:
download_dbpedia_occupations.py
que genera un json con todas las ocupaciones que ve que son posibles dentro de las entidades posible de dbpedia.
ese json es filtrado a mano por las ocupaciones que creo que estarían realacionadas con la industria y sirve como fuente de datos adicional del script: 
personas_dbpedia.py
así filtro mejor a las entidades posibles. 
un enfoque similar tomo con wikidata, pero wikidata me permite analizar las ocupaciones por clase y tipo. así que no es necesario el filtrado manual y lo resuelvo todo con sparql.


etapa2 :
se usa un script 
augment_dataset.py 
para acceder a dbpedia y wikidata, descargando todas los datos que tienen sobre esos subjects y escribirlos
en un nuevo grafo aumentado con esos datos.


etapa3:
unificación de mi ontología con lo obtenido.
se usa el script:
unify_datasets.py

esta etapa solo es para permitir una corrección mas simple. ya que se tienen todas las etapas y sus datos generados en distintos archivos.
es trivial usar un solo grafo para ir generando todo dentro del mismo.


estapa 4:
implementa la parte de agregar a la ontología el : wasDirectedByOscarWinner
el script : add_oscar_winner.py

para resolver esto tomo 2 direcciones.
en el punto 1 de la solución uso mi base de conocimiento como la principal y solo obtengo de bases externas que personas tienen premios.
y en el punto 2 de la solución ignoro mi base de conocimiento y uso la base externa como principal, donde los actores, directores y premios salen de esa info. 
mi base solo la uso como fuente de personas.
la idea fue ver las diferencias entre los 2 enfoques.


la sintaxis para ejcutar los scripts:

cd entrega5
python3 src/personas_dbpedia.py
python3 src/personas_wikidata.py
python3 src/augment_dataset.py
python3 src/unify.py
python3 src/add_oscar_winner.py


archivos :
varios de los archivos generados son para hacer mas simple la corrección. en algunos casos se usan como archvios intermedios pero no serían necesarios. se pueden unificar todo y evitar etapas.
inclusive no importan la ontología, con lo cual protegé los muestra incompletos, o sea sin las clases, etc.
son pensados para visualizar e identificar información mas simple usando un editor de texto.


dataset-original.ttl                <-- dataset unificado de la ontología de los trabajos anteriores. con los individuals y datos.

db_pedia_occupations_original.json  <-- archivo de ocupaciones de las personas que tengo en mi base- segun dbpedia.
db_pedia_occupations.json           <-- archivo filtrado a mano de las ocupaciones que me parecen correctas.
dbpedia_subjects.ttl                <-- tripletas de las referenicias a las entidades de dbpedia que representan a mis personas.
external_dataset2.ttl               <-- tripletas de los datos obtenidos de las entidades de dbpedia. solo como referencia para la corrección. ya que fue unificado el script que las generaba.


wikidata_subjects.ttl               <-- tripletas de las referenicas a las entidades de wikidata que representan a mis personas.   
external_dataset.ttl                <-- tripletas de los datos obtendios de wikidata de esas entidades, solo como referencia para la corrección ya que fue unificado el script que las generaba.

external_dataset_final.ttl          <-- tripletas con los datos de las bases externas (dbpedia, wikidata) generado por augment_dataset.py (esto es para mas simple corrección)     
dataset_final.ttl                   <-- todos los datos con las bases externas (dbpedia, wikidata) generado por unify_datasets.py 

dataset_directed.ttl                <-- tripletas con los wasDirectedByOscarWinner, solo para la corrección sea mas simple. ya que estos datos se encuentran en el siguiente archivo.
dataset_final_directed.ttl          <-- tripletas de toda la base con todos los datos. inclusive con wasDirectedByOscarWinner.



todo lo que se me va ocurriendo y lo que voy probando lo trato de anotar en notes_random.txt.
es un archivo desordenado y algunos de los conceptos pueden estar mal. voy anotando para acordarme de por que hago cada cosa. y las cosas que se me van ocurriendo a lo largo del camino.
las conclusiones mas importantes filtradas están dentro de notes.txt

