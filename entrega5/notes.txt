
Algunas ideas y conclusiones que fuí anotando a lo largo del desarrollo.
un poco mas filtradas que notes_random.txt.

---

https://meta.wikimedia.org/wiki/Wikidata/Notes/URI_scheme
https://meta.wikimedia.org/wiki/Wikidata/Notes/DBpedia_and_Wikidata

http://www.semantic-web-journal.net/system/files/swj1462.pdf
https://blog.reputationx.com/wikidata


el editor de sparql de wikidata es muy superior al de dbpedia.
con ejemplos. mas interactivo.
hasta tiene una parte donde seleccionas y te da el ejemplo en código. varias alternativas de lenguajes. etc

---

se me ocurren similitudes con los scrapers del tp1 en el sentido de que 
es necesario aplicar cierto "tuneo" a las consutlas de cada uno de los datasets para encontrar la información que se requiere.
como en el tp1 era necesario adaptar nuestros scrapers a cada página consultada (fuente de datos) para obtener los datos que requeríamos, 
en el caso de sparql, transferimos estos "tuneos" al lenguaje de consultas, y no al código del scraper mismo. 
para unificar fuentes de datos es necesario adaptar las consultas en base a los datasets y la estructura de grafo que manejan.
en resumen el problema es el mismo. pero se maneja en otro nivel!.
muchisimo mas estructurado y mejor pensado para manejar este problema.


pensamiento random, como son bases de datos colaborativas y abiertas, 
se nos da la posibilidad de redireccionar el esfuerzo que aplicamos a mejorar los datasets en vez de en tunear los scripts para encontrar información ya existente.
o sea diseñar los scripts en base a la ontología existente "ideal", y si algún recurso no cumple con esa estructura o le falta información, en vez de tunear el script para busque distinto o mas, 
tunear el dataset para que este mas completo.
por ejemplo lo que encontré sobre las ocupaciones de las personas actores y directores. (notes_random.txt)

otro pensamiento,
posibilitamos la optimización de ese proceso ya que existen mas personas con conocimientos de incorportar datos (proveyendo herraminetas amigables) para tunear los datasets, 
a personas con conocimientos de desarrollo y consulta de los mismos. (python y sparql)
todo wikimedia por lo menos te da editores gráficos para tunear esos datasets, es la primer info que te dan. por sobre info técnica.

---

las consultas que diseñamos para resolver un problema podrían cambiar con el tiempo.
por ejemplo para seleccionar ciertas personas no tienen imdb id. pero a futuro podrían tenerlo si alguien se los carga.
https://www.wikidata.org/wiki/Q1142853
https://www.wikidata.org/wiki/Q11665869
entonces si filtramos por quien tenga imdb id para una consulta, hoy sería una solución ya que trae una sola persona, a futuro podría no serlo!!.
un poco me hace pensar de nuevo en los desafíos que tiene consultar bases con datos abiertos y colaborativos. están constantemente cambiando.
y es complejo detectar si estás dejando afuera resultados o no.
la complejidad la fueron moviendo mas y mas cerca a la estructura y base de conocimiento en vez de que sea un problema de desarrollo.
el script se hace mas simple, la estructura donde guardo mi conocimiento mas compleja.


---


obteniendo Dennis Mojen
tiene occupation = actor.
asumía que subclass of era true para la clase misma.

P279 <--- subclass of.
https://www.wikidata.org/wiki/Q33999


Al definir la ontología de wikidata con propiedades internas en vez de estandarizadas como
por ejemplo rdfs:subClassOf tenemos que algunas cosas no son inferidas!!. no tienen la misma semántica que las propiedades de RDF.
algo que no se si estoy interpretando bien. por ahi tendría que consultarlo en el foro.
lo anoto para tenerlo en la cabeza.

---

el proceso de desarrollo del tp, estoy analizando un poquito que hice en las respuestas inciales.
a media que el desarrollo avanza, veo cada vez mas que lo que hice está incompleto.
supociciones que tenía en ciertos niveles terminan siendo no ciertas a medida que avanzamos en el desarrollo y análisis del 
problema y de los datasets.
esto depende en gran parte al análisis de datos sobre las respuestas esperadas.
tener los actores para comparar lo que me debería responder las consultas, y el estar prácticamente seguro de que
casi todas esas personas están, de alguna forma, representadas dentro del dataset que estoy consultando, implica que puedo
analizar la calidad de mi consulta.

me imagino que en el mundo real nosotros hacemos consultas para obtener datos para cruzar. 
y de las cuales NO sabemos si nos está respondiendo todos los datos que necesitamos.
esto me genera un contraste completo entre los tipos de datasets consultados.

1 - datasets internos definidos por nosotros. donde tenemos mas o menos control de los datos que ingresan.
y podemos cruzar y comparar
2 - datasets abiertos como los que estamos procesando. donde los datos y la estructura de esos datos varían constantemente.

a la seguridad de cuan completo es la respuesta obtenida en base a la consulta generada depende mucho de eso.

---

casos ambiguos en los datasets (wikidata)

obteniendo Hernán Jiménez
entidades externas encontradas {'http://www.wikidata.org/entity/Q28070848', 'http://www.wikidata.org/entity/Q27832751'}

ese caso tenemos 2 recursos.
1 actor
1 director
los 2 con profesiones relacionados al cine!! 
ninguno de los cuales tiene asociada filmografía o trabajo.
por lo que no puedo decidir a nivel de conuslta cual es el correcto!!.
requieren curados manuales de la información.

---

consultas en los datasets de ontologías mas abstractas son mucho mas complejas que las consultas
sobre datasets de ontologías no tan abstractas!!
wikidata es mas complejo que dbpedia.
pero tengo mas poder sobre los datasets de wikidata que sobre los de dbpedia.

así y todo wikidata con el prefijo wdt: se acerca bastante a dbpedia en algunas cuestiones.

---

la ontologia de wikidata me resulta un poco confusa.
el tema de las ocupaciones de las personas.
son a la vez.

subclase de
instancia de

y las 2 clasificaciones manejan una jerarquía sobre las ocupaciones para films.
no entiendo por que no sería una sola clasificación. analizarlo mejor.

---


creo que me cambió completamente la cabeza fue el pensar la estructura de la información es parte de la misma información.
siempre vi la estructura como parte del diseño de relaciones entre pedazos de información.
lo que estoy pensando y evaluando es que dentro de estos grafos de conocimiento la estructura y relaciones son parte de la misma información!.
y si bien existe cierta definición de esas relaciones, puede variar y mutar a medida que tocamos ontologías, etc.
y el poder de consultar la "forma de esa estructura" (lo que explicó diego en la teoría) en consultas sparql es impresionante!!. 
los filtros que sean sobre la "forma" en vez de solamente sobre que datos tiene. totalmente nuevo para mi. y una forma de pensar los problemas desde otro lado.

---

pensando en las bases relacionales, y como han pasado desde ser motores para procesar consultas complejas, donde prácticamente todo el software estaba implementado en sql gigantes y usando triggers,
a bases practicamente bobas para almacenar los mapeos de los objetos usados por motores de ORM.
el porqué de esto, y las limitaciones de asignar a un motor lógica del problema.
no puedo dejar de preguntarme que analogía veo con sparql y que lo hace diferente. 
hoy generando sistemas complejos con mucha lógica implementada en las consultas sparql. y como impactará el problema de unir varios dominios distintos.
replicar información en distintos nodos, etc.
creo que una diferencia importantísima es justamente la estructura de la información basada en grafos y en tripletas, que hace trivial el unir datasets.
pero el procesamiento distribuído de esos datasets es lo que me queda pendiende de entender si es posble. como lo logran, etc.
las bases de wikidata están solamente en servidores grosos? o usan algún tipo de arquitectura distribuída?. como la implementan como transfieren parte de lógica sparql entre los nodos y la dividen para procesarla, etc.

---

lo que me llevo del trabajo,
la idea original que yo tenía de analizar otras formas para relacionar información, motivada por necesidades en mi trabajo, creo que está por lo menos mas dirigida.
veo como una opción super válida comenzar a explorar por este lado alternativas de solución a lo que tenemos en el laburo implementado o que reimplementar.
inclusive para mi sorpresa es tan simple armar un motor de sparql usando fuseki junto con un dataset de ejemplo que armar lo mismo usando un motor de bases relacionales.

creo que hoy con todo lo que ha surgido en pocos años en referencia a esta forma de representar información, no debería ser una materia opcional. 
debería ser troncal a la par de base de datos. y poder contrastar alternativas de representación de los datos mas allá de los formatos relacionales tradicionales.
tengo igual una sensación de que lo vi muy por arriba. que son conceptos que son necesarios explorar muchísimo mas a fondo. 
asi como objetos y bases de datos relacionales, la web semántica es otra forma de pensar y estructurar soluciones.
a lo largo del trabajo fui cambiando consultas, me surgieron dudas, volvi a cambiarlas, descubrí otras cosas a medida que leía documentación, volví a cambialas, 
al final me quedaron tan simples que no se si son correctas!! jajjajaj.

siento que cumplió el objetivo de introducirnos en este mundo, pero me quedan mas dudas que certezas jajja.
me siento como la primera vez que aprendí objetos y las miles de posibilidades que se abren para explorar. 

---

y como última nota.
usando el google tv que tengo en el living, viendo los ratings que saca de rotten tomatoes, y las sugerencias que da de pelis, etc. 
jaa ahora se como funcionas!!!.

---

